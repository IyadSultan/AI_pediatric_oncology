{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IyadSultan/AI_pediatric_oncology/blob/main/03_training_a_transformer_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN23ix2lCjYn"
      },
      "source": [
        "# Training a Transformer for Medical Problem NER in Clinical Text\n",
        "\n",
        "**Named Entity Recognition (NER)** in healthcare can automatically highlight medical problems (like diseases or symptoms) in clinical notes, helping clinicians quickly identify key conditions. In this hands-on tutorial, we will fine-tune a Transformer model to recognize medical problems in text using Hugging Face ü§ó Transformers.\n",
        "\n",
        "We'll use a domain-specific BERT model (BioClinicalBERT) and an open medical NER dataset, walking through the entire process from data loading to evaluation. What you will learn:\n",
        "\n",
        "1- Setting up the environment and installing necessary libraries.\n",
        "\n",
        "2- Loading an open-access medical NER dataset from the Hugging Face Hub.\n",
        "\n",
        "3- Preprocessing clinical text data and aligning entity labels with subword tokens.\n",
        "\n",
        "4- Choosing a suitable pre-trained Transformer model for clinical text.\n",
        "\n",
        "5- Fine-tuning the model for NER using the Hugging Face Trainer API.\n",
        "\n",
        "6- Evaluating model performance (precision, recall, F1-score).\n",
        "\n",
        "7- Testing the model on example clinical text.\n",
        "\n",
        "\n",
        "*Prerequisites:* Basic Python coding and familiarity with general concepts of machine learning. We will explain NLP and Transformer concepts in simple terms, so healthcare professionals with beginner coding experience should be able to follow along."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W-fA_LhCjKh"
      },
      "source": [
        "# 1. Environment Setup\n",
        "First, we need to set up our environment. This tutorial is designed for Google Colab ‚Äì make sure you've selected a GPU runtime for faster training (go to Runtime > Change runtime type > Hardware accelerator > GPU). We‚Äôll install Hugging Face‚Äôs Transformers, Datasets, and evaluation libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUTIyna1Ca77"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate seqeval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwL6x3-_CkLr"
      },
      "source": [
        "*Explanation* : The transformers library provides the model and training tools, datasets will let us easily load the dataset, and seqeval (via evaluate) is a library for computing NER metrics like precision and F1. After running this, you should see the libraries installing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEkY9pHCCkSR"
      },
      "outputs": [],
      "source": [
        "# Let‚Äôs also verify that a GPU is available:\n",
        "import torch\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWNpboIVCkXf"
      },
      "source": [
        "If the output says GPU available: True, we‚Äôre ready to go (Colab Pro‚Äôs GPU will help speed up training). If it‚Äôs False, double-check the runtime settings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW0b9XexEEhF"
      },
      "source": [
        "# 2. Loading the Dataset\n",
        "We will use an open-access dataset from the Hugging Face Hub. One good option is the NCBI Disease corpus, which contains biomedical text (PubMed abstracts) annotated with disease names‚Äã. This **English-language dataset** is open-access and suitable for our task of extracting medical problems. Let's load the dataset using ü§ó Datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIhbGXY8CkcE"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the NCBI Disease dataset (with train/val/test splits)\n",
        "raw_datasets = load_dataset(\"ncbi_disease\")\n",
        "print(raw_datasets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj8s_jSeCkjs"
      },
      "source": [
        "This should download the dataset and show the splits (train/validation/test). The NCBI Disease corpus has 793 abstracts with disease annotations‚Äã, already split into about 5,433 training sentences, 924 validation sentences, and 941 test sentences. Each data sample in this dataset is a sentence with two fields:\n",
        "\n",
        "- \"tokens\": the list of word tokens in the sentence.\n",
        "- \"ner_tags\": the list of numeric labels for each token (0 = outside any entity, 1 = beginning of a disease entity, 2 = inside a disease entity)‚Äã.\n",
        "\n",
        "Let‚Äôs inspect an example from the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUxYCnRbCkpV"
      },
      "outputs": [],
      "source": [
        "# Peek at one training example\n",
        "example = raw_datasets[\"train\"][0]\n",
        "print(example[\"tokens\"])\n",
        "print(example[\"ner_tags\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI_XVzZJCkvY"
      },
      "source": [
        "In this example, the tokens ['adenomatous', 'polyposis', 'coli', 'tumour'] have tags [1, 2, 2, 2], meaning \"adenomatous\" is tagged as B-disease (beginning of a disease name) and the next three tokens are I-disease (continuation of the disease name), while all other tokens are 0 (not an entity). The disease mentioned here is ‚Äúadenomatous polyposis coli tumour.‚Äù"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjaQgmyyE50_"
      },
      "source": [
        "**Understanding the task**: Our goal is to train a model that takes a sequence of tokens (a clinical sentence) and outputs the correct tag (O, B-problem, I-problem) for each token, thereby identifying spans of text that are medical problems (diseases, in this dataset)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfZzAmHmE-dg"
      },
      "source": [
        "# 3. Data Preprocessing and Tokenization\n",
        "Before training, we need to preprocess the data for our Transformer model. Transformers like BERT cannot take raw text strings directly for NER; we must:\n",
        "Tokenize the text with the model‚Äôs tokenizer, and Align the labels to the tokenizer‚Äôs output tokens.\n",
        "\n",
        "*Why align labels?*\n",
        "\n",
        "Models like BERT use subword tokenization. A single word (e.g., \"diabetes\") might be broken into multiple subtokens (e.g., \"dia\", \"##betes\"). Our dataset labels are at the word level, so we need to create a label for each subtoken as well. Typically, we give the same label to all subtokens of a word, or label only the first subtoken and mark the rest as ‚Äúignore‚Äù in the loss calculation. We will use the Hugging Face AutoTokenizer to handle tokenization. Let‚Äôs choose our model‚Äôs tokenizer (we will use BioClinicalBERT in the next section, which has a fast tokenizer available):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKDOUmHGCk03"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UQHH7GxCk8h"
      },
      "source": [
        "Now we define a function to tokenize the inputs and align the NER tags. We'll use is_split_into_words=True because our dataset already provides a list of tokens for each example (so we don't want the tokenizer to re-split the sentence into words, but rather tokenize each given token).\n",
        "\n",
        "**For each tokenized example:**\n",
        "- We get a mapping of token indices to original word indices using tokenized_inputs.word_ids(batch_index).\n",
        "= We assign each subtoken the label of its originating word. We will mark only the first subtoken of each word with the original label and use -100 for the remaining subtokens. The value -100 is a special label that tells the loss function to ignore those positions (so we don't double-count a multi-subtoken word)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d88VzGphClCL"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    # Use padding and truncation to ensure consistent sequence lengths\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        is_split_into_words=True,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128  # Choose an appropriate max length\n",
        "    )\n",
        "\n",
        "    all_labels = examples[\"ner_tags\"]\n",
        "    aligned_labels = []\n",
        "\n",
        "    for i, labels in enumerate(all_labels):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        label_ids = []\n",
        "        previous_word_idx = None\n",
        "\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                # Special token like [CLS] or [SEP]\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                # Start of a new word, use that word's label\n",
        "                label_ids.append(labels[word_idx])\n",
        "            else:\n",
        "                # Continuation of the same word, mark for ignoring\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        aligned_labels.append(label_ids)\n",
        "\n",
        "    # Include the aligned labels in the tokenized input\n",
        "    tokenized_inputs[\"labels\"] = aligned_labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Apply the tokenization and alignment to the entire dataset\n",
        "tokenized_datasets = raw_datasets.map(tokenize_and_align_labels, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKdO04cdClH_"
      },
      "source": [
        "*Explanation:* We iterate through each example‚Äôs word indices from the tokenizer. When word_idx is None, it corresponds to special tokens (like [CLS] start or [SEP] end tokens) ‚Äì we assign -100 so that they are ignored in training. When we see a new word (word index not equal to the previous one), we take that word‚Äôs original label (labels[word_idx]). If the tokenizer output is still on the same word as the previous token (meaning the word was split into multiple subtokens), we also assign -100 to those subtokens. This way, only the first subtoken of a word carries the label, and the loss will be calculated only once per original word.\n",
        "\n",
        "**Note: An alternative would be to label subsequent subtokens as \"I-\" (inside) tags of the same entity. In our simple approach, we ignore them for loss calculation to avoid overweighting long words. This is a common technique to handle subword tokenization in NER‚Äã**.\n",
        "\n",
        "Now that we have tokenized_datasets, each entry has not only \"input_ids\" and \"attention_mask\" (from tokenization) but also an aligned \"labels\" sequence for training. We can verify the tokenization on our earlier example to see how the labels align:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GrMovTtClSF"
      },
      "outputs": [],
      "source": [
        "# Check tokenization and label alignment on the example\n",
        "tokens = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "print(\"Original tokens:\", example[\"tokens\"])\n",
        "print(\"Original labels:\", example[\"ner_tags\"])\n",
        "print(\"Subword tokens:\", tokens.tokens())\n",
        "print(\"Aligned labels:\", tokenize_and_align_labels({\"tokens\": [example[\"tokens\"]], \"ner_tags\": [example[\"ner_tags\"]]})[\"labels\"][0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbwGa2jTClYm"
      },
      "source": [
        "This will show the original tokens and their labels, the subword tokens (including special [CLS] and [SEP]), and the aligned labels for each subword token. You should see -100 for the special tokens and second part of any split word. For instance, if \"polyposis\" was split into [\"poly\", \"##posis\"], the first subtoken might get label 2 (I-disease) and the second subtoken -100."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7344wQ75du8P"
      },
      "source": [
        "Let's also visualize an example from the training set in a more readable format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsrOnEPYUL0i"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "# Define the label mapping\n",
        "id2label = {0: \"O\", 1: \"B-Disease\", 2: \"I-Disease\"}\n",
        "\n",
        "# 1. Display a raw example from the training set\n",
        "example_idx = 10  # You can change this index to view different examples\n",
        "example = raw_datasets[\"train\"][example_idx]\n",
        "\n",
        "print(\"=== RAW TRAINING EXAMPLE ===\")\n",
        "df_raw = pd.DataFrame({\n",
        "    \"Token\": example[\"tokens\"],\n",
        "    \"NER Tag ID\": example[\"ner_tags\"],\n",
        "    \"NER Tag\": [id2label[tag] for tag in example[\"ner_tags\"]]\n",
        "})\n",
        "display(df_raw)\n",
        "\n",
        "# Extract entities for easier viewing\n",
        "entities = []\n",
        "current_entity = []\n",
        "for token, tag_id in zip(example[\"tokens\"], example[\"ner_tags\"]):\n",
        "    if tag_id == 1:  # B-Disease\n",
        "        if current_entity:\n",
        "            entities.append(\" \".join(current_entity))\n",
        "            current_entity = []\n",
        "        current_entity.append(token)\n",
        "    elif tag_id == 2:  # I-Disease\n",
        "        current_entity.append(token)\n",
        "    else:  # O\n",
        "        if current_entity:\n",
        "            entities.append(\" \".join(current_entity))\n",
        "            current_entity = []\n",
        "\n",
        "if current_entity:\n",
        "    entities.append(\" \".join(current_entity))\n",
        "\n",
        "print(\"\\nEntities found:\")\n",
        "for i, entity in enumerate(entities):\n",
        "    print(f\"{i+1}. {entity}\")\n",
        "\n",
        "print(\"\\nOriginal sentence:\")\n",
        "print(\" \".join(example[\"tokens\"]))\n",
        "\n",
        "# Display the tokenized example\n",
        "print(\"\\n=== TOKENIZED EXAMPLE ===\")\n",
        "# Show how tokens are aligned with labels after tokenization\n",
        "tokenized_inputs = tokenizer(\n",
        "    example[\"tokens\"],\n",
        "    is_split_into_words=True,\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    max_length=128\n",
        ")\n",
        "\n",
        "# Manual label alignment for display\n",
        "word_ids = tokenized_inputs.word_ids()\n",
        "labels = example[\"ner_tags\"]\n",
        "aligned_labels = []\n",
        "previous_word_idx = None\n",
        "\n",
        "for word_idx in word_ids:\n",
        "    if word_idx is None:\n",
        "        # Special token\n",
        "        aligned_labels.append(-100)\n",
        "    elif word_idx != previous_word_idx:\n",
        "        # Start of a new word\n",
        "        aligned_labels.append(labels[word_idx])\n",
        "    else:\n",
        "        # Continuation of same word\n",
        "        aligned_labels.append(-100)\n",
        "    previous_word_idx = word_idx\n",
        "\n",
        "# Display in DataFrame for clarity\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"])\n",
        "df_tokenized = pd.DataFrame({\n",
        "    \"Index\": range(len(tokens)),\n",
        "    \"Token\": tokens,\n",
        "    \"Word ID\": word_ids,\n",
        "    \"Label ID\": aligned_labels,\n",
        "    \"Label\": [id2label[l] if l != -100 else \"IGNORE\" for l in aligned_labels]\n",
        "})\n",
        "display(df_tokenized)\n",
        "\n",
        "# Show some statistics\n",
        "special_tokens = sum(1 for l in aligned_labels if l == -100)\n",
        "real_labels = sum(1 for l in aligned_labels if l != -100)\n",
        "\n",
        "print(f\"\\nTotal tokens after tokenization: {len(tokens)}\")\n",
        "print(f\"Special tokens or subword continuations (IGNORE): {special_tokens}\")\n",
        "print(f\"Tokens with real labels: {real_labels}\")\n",
        "print(f\"Original tokens: {len(example['tokens'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0zPu9isGBad"
      },
      "source": [
        "# 4. Choosing a Pretrained Model (BioClinicalBERT)\n",
        "Selecting a good pretrained model as a starting point is crucial. We need a model that understands clinical language. BioClinicalBERT is an excellent choice: it‚Äôs a BERT-based model that was pretrained on biomedical literature and clinical notes (MIMIC-III ICU records), giving it a strong grasp of medical terminology and context. This domain-specific pretraining should help it recognize disease names and other medical entities better than a generic BERT.\n",
        "\n",
        "Other options could be BioBERT (pretrained on biomedical articles) or even a smaller model like DistilBERT fine-tuned on a medical NER dataset. However, BioClinicalBERT is well-suited for clinical text, and since we have a GPU, its size (base BERT, ~110M parameters) is manageable.\n",
        "\n",
        "Let's load the model with a classification head for token classification (NER). We will use AutoModelForTokenClassification which adds a token-level classification layer on top of the transformer. We need to specify the number of labels our model will predict:\n",
        "\n",
        "- Label 0: \"O\" (no entity)\n",
        "- Label 1: \"B-Disease\" (beginning of a disease mention)\n",
        "- Label 2: \"I-Disease\" (inside a disease mention)\n",
        "\n",
        "We'll also pass **id2label and label2id** mappings for better clarity (so the model knows which label index corresponds to \"B-Disease\", etc., which will be useful for inference)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZHmzPMLClep"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Extract all labels from training data to compute class weights\n",
        "all_labels = []\n",
        "for example in tokenized_datasets[\"train\"]:\n",
        "    all_labels.extend([l for l in example[\"labels\"] if l != -100])\n",
        "\n",
        "# Compute class weights to handle imbalanced data\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.array([0, 1, 2]),  # O, B-Disease, I-Disease\n",
        "    y=np.array(all_labels)\n",
        ")\n",
        "\n",
        "print(f\"Computed class weights: {class_weights}\")\n",
        "\n",
        "# Define our label mappings\n",
        "num_labels = 3  # O, B-Disease, I-Disease\n",
        "id2label = {0: \"O\", 1: \"B-Disease\", 2: \"I-Disease\"}\n",
        "label2id = {\"O\": 0, \"B-Disease\": 1, \"I-Disease\": 2}\n",
        "\n",
        "# Create the model\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Htvc9vceCljk"
      },
      "source": [
        "When this runs, it will download the BioClinicalBERT weights. You might see a warning that some weights are not used (those are for the original masked language modeling head) and some new weights are initialized (the classification head). That‚Äôs expected because we are repurposing the base model for NER.\n",
        "\n",
        "**Note: BioClinicalBERT was initialized from BioBERT and trained on. It‚Äôs a domain-specific model ideal for clinical text. If you were dealing with general text, you might use a generic BERT base model, but for our medical task, leveraging domain knowledge should give us a boost.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFqxnBKkGvxQ"
      },
      "source": [
        "# 5. Fine-Tuning the Model with Trainer API\n",
        "Now we have our prepared dataset (tokenized_datasets) and our model. The next step is to fine-tune the model on the training data. We'll use Hugging Face‚Äôs Trainer API, which simplifies the training loop and handles things like gradient accumulation, evaluation, and more. Setup Training Arguments: We need to specify how we train, e.g., number of epochs, batch size, learning rate, etc. For NER with a base BERT model:\n",
        "- A few epochs (2-4) are usually sufficient for a dataset of this size.\n",
        "- A learning rate around 2e-5 to 5e-5 works well for fine-tuning BERT.\n",
        "- We‚Äôll use the validation set to evaluate after each epoch.\n",
        "\n",
        "We also define a compute_metrics function to compute precision, recall, and F1 using the seqeval metric. We will focus on the ‚ÄúDisease‚Äù entity class performance and overall metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkzGYP9oClsd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "# Load the seqeval metric for NER evaluation\n",
        "seqeval = evaluate.load(\"seqeval\")\n",
        "\n",
        "# Define compute_metrics to use during evaluation\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    true_labels = []\n",
        "    true_predictions = []\n",
        "    for pred, lab in zip(predictions, labels):\n",
        "        # Remove ignored index (subword pieces)\n",
        "        lab = [l for l in lab if l != -100]\n",
        "        pred = pred[:len(lab)]  # truncate prediction to same length as lab\n",
        "        true_labels.append([id2label[l] for l in lab])\n",
        "        true_predictions.append([id2label[p] for p in pred])\n",
        "    # Compute metrics using seqeval\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "    # Return precision, recall, F1, and accuracy\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chhBg2OBCl19"
      },
      "source": [
        "*A quick explanation:* We take the logits (model outputs before softmax) and labels from the evaluation batch. We argmax the logits to get the predicted label indices. We then filter out any -100 in the labels (those correspond to subword pieces we chose to ignore), and make sure to truncate the predictions to the same length (since we don't want to count predictions for subword tokens that were ignored in labels). We map the numeric labels back to their string names (using id2label) and feed the lists to seqeval.compute(). Seqeval will calculate precision, recall, F1, etc., treating the labels as sequences of entity tags. We return the overall metrics. (Seqeval also provides class-specific metrics, but ‚Äúoverall‚Äù here mostly reflects the single entity type we care about, Disease.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAF6kztkCmBs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Create a custom trainer with weighted loss to handle class imbalance\n",
        "class WeightedLossTrainer(Trainer):\n",
        "    def __init__(self, class_weights=None, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        # Convert to the same data type as the model (float32)\n",
        "        if class_weights is not None:\n",
        "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
        "        else:\n",
        "            self.class_weights = None\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Use class weights in loss calculation\n",
        "        if self.class_weights is not None:\n",
        "            loss_fct = torch.nn.CrossEntropyLoss(\n",
        "                weight=self.class_weights.to(model.device),\n",
        "                ignore_index=-100\n",
        "            )\n",
        "        else:\n",
        "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output_model_dir\",\n",
        "    num_train_epochs=8,\n",
        "    per_device_train_batch_size=16,\n",
        "    learning_rate=1e-4,  # Higher learning rate for faster convergence\n",
        "    weight_decay=0.01,   # Regularization to prevent overfitting\n",
        "    logging_steps=len(tokenized_datasets[\"train\"]) // 16,  # Log once per epoch\n",
        "    fp16=True            # Mixed precision training for better performance\n",
        ")\n",
        "\n",
        "# Create the weighted loss trainer\n",
        "trainer = WeightedLossTrainer(\n",
        "    class_weights=class_weights,\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpD-3e55HsXt"
      },
      "source": [
        "Key points in these settings:\n",
        "We set evaluation_strategy=\"epoch\" to evaluate on the validation set at the end of each epoch.\n",
        "We chose 3 epochs. You can adjust this; often 2-4 is enough for convergence on a small dataset, and more can cause overfitting.\n",
        "per_device_train_batch_size=16 is reasonable for a base model on a GPU with enough memory (Colab Pro GPUs can handle this). If you get OOM errors, you might reduce the batch size.\n",
        "We disable saving checkpoints (save_strategy=\"no\") and pushing to hub for simplicity, but in a real scenario, you might want to save the fine-tuned model.\n",
        "Before training, let's optionally evaluate the untrained model on the validation set to get a baseline. (Since the classification head is random initially, we expect it to perform very poorly ‚Äì likely predicting no entities correctly.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkxnSi49HtzF"
      },
      "outputs": [],
      "source": [
        "# Before training, let's evaluate the untrained model to get a baseline:\n",
        "# Evaluate the model *before* training (baseline performance)\n",
        "baseline_metrics = trainer.evaluate()\n",
        "print(\"Baseline performance:\", baseline_metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUhWvRC1I68I"
      },
      "source": [
        "This will run the model on the validation set and print metrics. Likely, you will see precision, recall, F1 near 0. The model probably labels everything as \"O\" (no entity) by default, resulting in 0 recall for the disease class. The accuracy might be high (because most tokens are non-entity and those were \"predicted\" correctly by always outputting O), but accuracy is misleading here since our interest is in the rare entity class. This highlights why we use precision/recall/F1 for NER evaluation ‚Äì a model that just predicts no entities will have high token-level accuracy but F1 of 0 for the actual task. Now, let's fine-tune the model on the training data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7pN2obfCmI0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2_ZcOSgXCmPt"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "train_results = trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5ESwX1Dg21R"
      },
      "source": [
        "\n",
        "\n",
        "{'loss': 0.2119, 'grad_norm': 10.693309783935547, 'learning_rate': 8.764705882352942e-05, 'epoch': 0.9970588235294118}\n",
        "\n",
        "{'loss': 0.0817, 'grad_norm': 0.4537445306777954, 'learning_rate': 7.518382352941177e-05, 'epoch': 1.9941176470588236}\n",
        "\n",
        "{'loss': 0.044, 'grad_norm': 9.398859977722168, 'learning_rate': 6.275735294117647e-05, 'epoch': 2.9911764705882353}\n",
        "\n",
        "{'loss': 0.0216, 'grad_norm': 0.3768038749694824, 'learning_rate': 5.0294117647058826e-05, 'epoch': 3.988235294117647}\n",
        "\n",
        "{'loss': 0.0141, 'grad_norm': 0.07289780676364899, 'learning_rate': 3.7830882352941175e-05, 'epoch': 4.985294117647059}\n",
        "\n",
        "{'loss': 0.0088, 'grad_norm': 13.817891120910645, 'learning_rate': 2.536764705882353e-05, 'epoch': 5.982352941176471}\n",
        "\n",
        "{'loss': 0.0038, 'grad_norm': 0.37299567461013794, 'learning_rate': 1.2904411764705885e-05, 'epoch': 6.979411764705882}\n",
        "\n",
        "{'loss': 0.0022, 'grad_norm': 0.5072056651115417, 'learning_rate': 4.411764705882353e-07, 'epoch': 7.976470588235294}\n",
        "\n",
        "{'train_runtime': 423.4081, 'train_samples_per_second': 102.653, 'train_steps_per_second': 6.424, 'train_loss': 0.048368592780795604, 'epoch': 8.0}\n",
        "\n",
        "These metrics provide valuable insights into how our model is learning:\n",
        "\n",
        "Loss: We can see a dramatic decrease in training loss from 0.2119 in the first epoch to 0.0022 by the end of training - a 99% reduction. This indicates that our model is effectively learning to identify disease entities in the text.\n",
        "Learning Rate: We're using a linear learning rate scheduler that gradually decreases from 8.76e-05 to nearly zero by the end of training. This helps the model converge to an optimal solution by taking smaller steps as it gets closer to the minimum.\n",
        "Gradient Norm: The gradient norm fluctuates throughout training, with some spikes (like in epochs 1 and 6). These spikes represent moments when the model encounters batches of data that cause larger gradient updates. The overall trend shows the model is stabilizing, especially in the later epochs.\n",
        "Training Efficiency: The final line shows our training took about 423 seconds (7 minutes) to complete all 8 epochs, processing approximately 102 examples per second. This is quite efficient for a GPU-based training run.\n",
        "Convergence Pattern: The most important observation is how quickly the loss decreases in early epochs (from 0.21 to 0.08 in just one epoch) and then continues to improve more gradually. This is a healthy learning curve that suggests:\n",
        "\n",
        "Our learning rate was well-chosen\n",
        "The pre-trained BioClinicalBERT model provides an excellent starting point\n",
        "The class weighting is effectively handling the imbalanced dataset\n",
        "\n",
        "\n",
        "\n",
        "The final training loss of 0.048 averaged across all epochs (with the last epoch having a loss of just 0.0022) indicates that our model has successfully learned to identify disease entities in medical text.\n",
        "This pattern of rapidly decreasing loss followed by more gradual improvements is characteristic of transfer learning with pre-trained models - we're seeing the benefit of starting with a model that already understands biomedical language, then fine-tuning it for our specific NER task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swmZ-C8URYXN"
      },
      "source": [
        "This will start the training loop. You should see output for each epoch, including the evaluation metrics on the validation set at the end of each epoch.\n",
        "\n",
        "For example, after each epoch you might see lines showing the eval precision, recall, f1, etc., gradually improving. Training may take a few minutes per epoch on Colab GPU. Once training is complete, we can evaluate the model on the test set (which the model has never seen, to get an unbiased performance estimate):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LGSUVmg6ecHm"
      },
      "outputs": [],
      "source": [
        "# Fix the compute_metrics function to properly handle token-level predictions\n",
        "def compute_metrics_fixed(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    true_labels = []\n",
        "    true_predictions = []\n",
        "\n",
        "    for pred, lab in zip(predictions, labels):\n",
        "        # Filter out ignored indexes (-100)\n",
        "        valid_indices = [i for i, l in enumerate(lab) if l != -100]\n",
        "        pred_filtered = [pred[i] for i in valid_indices]\n",
        "        lab_filtered = [lab[i] for i in valid_indices]\n",
        "\n",
        "        # Convert to string labels\n",
        "        true_labels.append([id2label[l] for l in lab_filtered])\n",
        "        true_predictions.append([id2label[p] for p in pred_filtered])\n",
        "\n",
        "    # Compute metrics\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n",
        "\n",
        "# Re-evaluate with the fixed function\n",
        "trainer.compute_metrics = compute_metrics_fixed\n",
        "test_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
        "print(\"Fixed test results:\", test_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imWIJxSwiYr4"
      },
      "source": [
        "With the fixed metrics function, you should see much better results. Let's examine these final evaluation metrics:\n",
        "\n",
        "| Metric | Value | Explanation |\n",
        "|--------|-------|-------------|\n",
        "| eval_loss | 0.2913 | The final loss value on the test dataset. Lower values indicate better model fit. |\n",
        "| eval_precision | 0.8254 | The proportion of predicted disease entities that were actually correct. Our model is right about 82.5% of the time when it identifies a disease. |\n",
        "| eval_recall | 0.9010 | The proportion of actual disease entities that were correctly identified. Our model finds about 90% of all diseases in the text. |\n",
        "| eval_f1 | 0.8616 | The harmonic mean of precision and recall, providing a single metric to judge overall performance. This strong F1 score of 0.86 indicates excellent performance for a biomedical NER task. |\n",
        "| eval_accuracy | 0.9842 | The proportion of all tokens (including non-entities) that were correctly classified. The high value reflects both good entity detection and correct identification of non-entities. |\n",
        "| eval_runtime | 2.3923 | Time in seconds to evaluate the model on the test set. |\n",
        "| eval_samples_per_second | 393.34 | The number of test examples processed per second. |\n",
        "\n",
        "These results demonstrate that our model has successfully learned to identify disease mentions in medical text with high accuracy. The high recall (90%) is particularly valuable in clinical applications where missing a disease mention could have more serious consequences than occasionally mislabeling a non-disease term."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffUAAP3WRx3G"
      },
      "source": [
        "# 6. Qualitative Examples: Before vs. After Fine-Tuning\n",
        "Numbers are important, but it's also useful to see the model‚Äôs predictions on actual text to understand what it‚Äôs doing. We‚Äôll take a synthetic clinical note snippet and run the NER model on it. We'll compare the output before and after fine-tuning to appreciate the improvement.\n",
        "\n",
        "Let's create a pipeline for NER using our fine-tuned model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ek0SZlFnRZh6"
      },
      "outputs": [],
      "source": [
        "# Run predictions on a few examples from the test set\n",
        "from transformers import pipeline\n",
        "\n",
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
        "\n",
        "# Test on a few examples from the test set\n",
        "for i in range(3):  # Check first 3 examples\n",
        "    example = raw_datasets[\"test\"][i]\n",
        "    text = \" \".join(example[\"tokens\"])\n",
        "    print(f\"\\nExample {i+1} text: {text}\")\n",
        "    print(\"True entities:\", [(example[\"tokens\"][j], id2label[tag]) for j, tag in enumerate(example[\"ner_tags\"]) if tag != 0])\n",
        "\n",
        "    # Get model predictions\n",
        "    entities = ner_pipeline(text)\n",
        "    print(\"Predicted entities:\", [(ent['word'], ent['entity_group']) for ent in entities])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLQdG1ipRZoB"
      },
      "source": [
        "By setting aggregation_strategy=\"simple\", the pipeline will group contiguous tokens with the same entity tag, so we get whole entity mentions rather than token-by-token output. Now, consider this sample text (simulating a line from a clinical note):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a0QjZZ-RaBh"
      },
      "source": [
        "The fine-tuned model correctly detects ‚Äúdiabetes mellitus‚Äù and ‚Äúhypertension‚Äù as medical problems (diseases), with high confidence scores. This is a big improvement from before training, when those terms would not be identified at all.\n",
        "\n",
        "*Discussion*: Our model treats both diabetes and hypertension as Disease entities because the dataset we trained on is focused on disease names‚Äã. In a real clinical NER scenario, \"medical problems\" could include symptoms and other conditions as well. If we had a dataset that labeled symptoms, we could fine-tune in a similar way. The key takeaway is that with domain-specific data and a pretrained model, we can adapt a Transformer to accurately tag clinically relevant information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMmaB15SSURq"
      },
      "source": [
        "# 7. Conclusion and Next Steps\n",
        "In this tutorial, we covered the end-to-end process of fine-tuning a Transformer model (BioClinicalBERT) to perform NER on clinical text. Starting from an open dataset‚Äã\n",
        "huggingface.co\n",
        ", we preprocessed the data, loaded a domain-specific model‚Äã\n",
        "huggingface.co\n",
        ", and trained it to recognize disease entities. We evaluated the model‚Äôs performance, seeing a drastic improvement in precision/recall/F1 from the untrained baseline to the fine-tuned model. We also tested the model on example text to see how it identifies medical problems in context.\n",
        "\n",
        "**Key learnings and tips:**\n",
        "- Data preprocessing: Aligning labels with tokenized inputs is essential for token classification tasks. Libraries like ü§ó Datasets and fast tokenizers make this easier.\n",
        "- Choosing a model: A model pretrained on biomedical and clinical text can significantly boost performance for medical NER compared to a general-language model‚Äã.\n",
        "- Training: Even a few epochs of fine-tuning with a reasonably low learning rate (2e-5) can achieve high accuracy for NER when starting from a good pretrained base.\n",
        "- Evaluation: Always look at precision, recall, and F1 in NER tasks ‚Äì token-level accuracy can be misleading when the dataset has many non-entity tokens.\n",
        "- Qualitative checks: It‚Äôs helpful to try the model on realistic examples to verify it‚Äôs picking up the entities of interest (and to ensure it‚Äôs not flagging spurious ones).\n",
        "\n",
        "\n",
        "With this foundation, you can experiment further. For example, you could try a different dataset (e.g., one that includes symptoms or other entity types), or use a larger model like BioMegatron or a Clinical-XLM-R for multilingual clinical notes. You could also incorporate more entity categories (like medications, tests, treatments) if your dataset provides them.\n",
        "\n",
        "By understanding and following these steps, healthcare professionals and researchers can train custom NER models to extract valuable information from clinical narratives, which can be a stepping stone to building clinical NLP applications such as automated problem list generation, clinical decision support, or research data mining."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMpt0H0VHsPoXPQvO1vHMpC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}