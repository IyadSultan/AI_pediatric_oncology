{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMn5nNP65QAQCVFETNCQK/R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IyadSultan/AI_pediatric_oncology/blob/main/05_Healthcare_Triage_Bot_with_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Healthcare Triage Bot with LangChain and LangGraph"
      ],
      "metadata": {
        "id": "LBtemPQCJ5Ob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial focuses on on using LangChain and LangGraph for healthcare applications. We’ll build a mini smart triage chatbot that leverages OpenAI’s API (e.g., GPT-4) to assess patient symptoms and guide care. By the end, you’ll know how to set up LangChain, perform retrieval-augmented generation (RAG) over clinical notes, use simple agents with tools, and create branching logic with LangGraph."
      ],
      "metadata": {
        "id": "7hW8v2RFJ--5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why LangChain & LangGraph for Healthcare AI?\n",
        "\n",
        "In healthcare, AI assistants can improve triage accuracy and efficiency, aiding clinicians and patients alike. Large language models like GPT-4 have shown diagnostic and triage performance comparable to physicians. However, raw LLMs can hallucinate or give inconsistent answers, which is risky in medicine. LangChain helps by providing tools and abstractions to integrate LLMs with knowledge bases and reasoning steps, making chatbots and retrieval systems easier to build. LangGraph, on the other hand, is a framework for controlling and orchestrating agent workflows, allowing complex decision trees (like triage flows) with reliability (e.g., human-in-the-loop and moderation).\n",
        "\n",
        "Use cases: Imagine a chatbot that can answer patient FAQs by pulling info from medical documents (RAG), or a symptom checker that asks questions and decides “ER vs home care.” These require the LLM to use external knowledge and follow a decision logic:\n",
        "- Clinical Triage: Identify emergency symptoms vs. routine issues.\n",
        "- Symptom Checking: Ask follow-up questions and narrow down possibilities.\n",
        "- Patient FAQ: Retrieve answers from hospital policy documents or medical literature.\n",
        "\n",
        "LangChain + LangGraph give us the tools to build these safely and effectively by grounding outputs in data and steering the conversation flow. Exercise:\n",
        "\n",
        "Think of a scenario where an AI assistant could help in healthcare (e.g., medication information, scheduling). How might an LLM plus external data improve the outcome? Jot down one idea before moving on."
      ],
      "metadata": {
        "id": "GBTN2URIKTBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Hello World (Colab Friendly)\n",
        "We’ll use Google Colab for this workshop so everyone can code along easily. Before coding, ensure you have an OpenAI API key ready (from your OpenAI account).\n",
        "\n"
      ],
      "metadata": {
        "id": "rmc6huy7KlQT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1:\n",
        "Pip installs. In a Colab cell, install the required libraries:"
      ],
      "metadata": {
        "id": "0cSepOInVvpi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dBWNnesJ2a-"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain langgraph openai chromadb sentence-transformers\n",
        "# !pip install langchain_openai\n",
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This installs:**\n",
        "\n",
        "- LangChain (LLM framework)\n",
        "- LangGraph (agent orchestration)\n",
        "- OpenAI (to call GPT models)\n",
        "- ChromaDB and sentence-transformers (for RAG vector database & embeddings)\n"
      ],
      "metadata": {
        "id": "DDdLzq74J6Hq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2\n",
        "Setup API Key. Set your OpenAI API key so LangChain can use it. In Colab, you can use getpass to input securely:"
      ],
      "metadata": {
        "id": "7FVAzC7gK-eH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ],
      "metadata": {
        "id": "6p0WwyhMJ6Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, you could directly assign os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" but avoid hardcoding keys in shared notebooks.\n",
        "\n",
        "## Step 3\n",
        "Quick “Hello World”. Let’s verify everything works by making a simple completion call using LangChain’s OpenAI wrapper:"
      ],
      "metadata": {
        "id": "TzXJtliJJ6UR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", temperature=0.7)\n",
        "response = llm.invoke(\"Hello, how can AI assist in healthcare?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "CUX6x9UEJ6aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the cell. You should see a polite response from the model about AI in healthcare. Congratulations! You’ve made your first LangChain LLM call. Exercise: Try changing the prompt to ask a medical question (e.g., “What are symptoms of flu?”) and observe the answer. Also, experiment with temperature=0.7 to see how the reply varies (higher temperature yields more varied, creative outputs)."
      ],
      "metadata": {
        "id": "SvbXZHYDJ6gv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. LangChain Basics: Prompt Templates & Simple LLM Calls\n",
        "Now that we have a basic LLM call working, let’s explore LangChain’s features for structuring prompts and managing outputs. Prompt Templates: In LangChain, you can create templates with placeholders. This is useful in healthcare – for example, to enforce a format or inject context."
      ],
      "metadata": {
        "id": "pHUihRIVMp50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "template = \"You are a helpful medical assistant. Answer briefly:\\nQuestion: {question}\\nAnswer:\"\n",
        "prompt = PromptTemplate(input_variables=[\"question\"], template=template)\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "query = \"What is the normal range for adult body temperature?\"\n",
        "print(chain.run(question=query))\n"
      ],
      "metadata": {
        "id": "21aTDiE_J6mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we defined a prompt that always prefixes the role (“helpful medical assistant”) and instructs to answer briefly. The {question} gets filled with our query. The LLMChain sends this formatted prompt to the LLM. LangChain handles the prompt injection, so you focus on content. You could easily swap in another question or change the template to adjust style (e.g., more formal, or providing additional context like patient age if needed). Controlling Outputs: We already used temperature to reduce randomness. You can also set max_tokens to limit response length and top_p to control diversity. In critical apps like triage, deterministic and concise outputs are often preferable (e.g. temperature near 0 for consistency). Also consider tokens and cost: Each token (word piece) used costs money and context. Monitoring token usage ensures the bot stays within limits (like OpenAI’s 4k/8k token context windows). LangChain’s LLMResult often includes token usage info, or you could wrap the LLM in a callback to log usage. We won’t deep-dive here, but keep an eye out for .usage in responses or use LangChain’s debugging tools later. Exercise: Modify the prompt template to include a patient’s name (e.g., “Patient: John Doe. Symptom: headache...”). Run the chain again. See how the answer changes when more context is given. This practice of prompt engineering helps tailor outputs."
      ],
      "metadata": {
        "id": "goo2E7E2J6sf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Retrieval-Augmented Generation (RAG) with Clinical Notes\n",
        "One powerful feature for healthcare apps is Retrieval-Augmented Generation (RAG). This means the LLM can pull in information from a knowledge base (e.g., clinical guidelines, patient records) before answering. RAG helps ensure factual, up-to-date answers and reduces hallucination. Scenario: Let’s say we have a short clinical note or a medical guideline that the bot should reference. We’ll simulate a small knowledge base and ask questions.#"
      ],
      "metadata": {
        "id": "l-tYjgJpM_yN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1\n",
        "\n",
        "Prepare Documents. For simplicity, we’ll create a small text about triage guidelines:"
      ],
      "metadata": {
        "id": "FyfSbf8SNEkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document\n",
        "\n",
        "text = \"\"\"Chest Pain Triage Guidelines:\n",
        "- If chest pain is severe, sudden, or accompanied by shortness of breath or dizziness, consider it an emergency and seek immediate care.\n",
        "- If chest pain is mild and brief, and especially if related to breathing or cough, it might be musculoskeletal. Advise routine check-up.\n",
        "\"\"\"\n",
        "docs = [Document(page_content=text)]\n"
      ],
      "metadata": {
        "id": "PvzQaKwsJ6y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a real case, you might load PDFs or multiple files using DirectoryLoader or TextLoader. Here we directly create one Document."
      ],
      "metadata": {
        "id": "ojTwQQfHJ65z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2\n",
        "Create Vector Store (Embedding + Index). We will embed the document text into a vector space so we can retrieve it by similarity to a query. We use a lightweight embedding model and ChromaDB as vector store:"
      ],
      "metadata": {
        "id": "MbJha_AoNVlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Use a small, fast embedding model\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = Chroma.from_documents(docs, embedding)\n"
      ],
      "metadata": {
        "id": "wRS_fhXdJ7A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This turns our docs into a searchable vector index. In practice, you could use OpenAI embeddings or a larger model for better accuracy, but this is fine for demonstration."
      ],
      "metadata": {
        "id": "ToFpPf0zJ7HX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3\n",
        "RAG Chain (RetrievalQA). LangChain provides high-level chains to do retrieval + LLM Q&A. We’ll use RetrievalQA to connect our vector store with an LLM:"
      ],
      "metadata": {
        "id": "876s9-y3NrKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Use a slightly higher temperature for variety in explanation\n",
        "qa_llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", temperature=0.7)\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=qa_llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())\n",
        "\n",
        "question = \"I have mild chest pain when I cough. Should I go to the ER?\"\n",
        "result = qa_chain.run(question)\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "DgPliYmVJ7Nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s what happens under the hood:\n",
        "\n",
        "- vectorstore.as_retriever() takes our Chroma DB and allows the chain to query it.\n",
        "- The question is used to fetch relevant doc chunks (our guidelines).\n",
        "- The LLM sees the retrieved text + question, then generates an answer grounded in the text.\n",
        "\n",
        "If all went well, the answer should mention that mild pain with cough is likely not emergency and a routine check might suffice, referencing the guideline. Notice we used chain_type=\"stuff\" (meaning it will just “stuff” the retrieved text into the prompt). For larger docs, LangChain can do more sophisticated combine strategies (map-reduce, refine), but that’s beyond our scope today. Why RAG? In healthcare especially, grounding answers in a trusted text is crucial.\n",
        "\n",
        "RAG ensures the LLM’s output is factual and traceable. It’s great for patient FAQs (e.g., “What’s the hospital’s visiting policy?”) where answers must quote official info.\n",
        "\n",
        "*Exercise*: Try adding another bullet in the text (e.g., advice about chest pain that worsens with exercise). Then ask a question related to that. See if the chain retrieves the new info. This shows how updating the knowledge base can instantly change the bot’s answers – a big win for maintainability."
      ],
      "metadata": {
        "id": "v4JfUNNTJ7UD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Simple Agents and Tools (e.g., Medical Calculator)\n",
        "\n",
        "LangChain Agents allow an LLM to decide if and when to use a tool to help answer a query. Tools can be things like search engines, calculators, or custom functions. For a healthcare bot, tools might include:\n",
        "A calculator for medical scores (BMI, medication dose, etc.).\n",
        "A database lookup for clinic hours or doctor information.\n",
        "Calling an API (e.g., for drug interaction info).\n",
        "\n",
        "In our workshop, we’ll implement a small custom tool and use an agent to invoke it. Example tool: BMI calculator. If a user asks “What is my BMI if I weigh 70kg and am 1.75m tall?”, the bot could calculate that rather than guess.\n",
        "\n"
      ],
      "metadata": {
        "id": "CsRby8z4OiD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1\n",
        "\n",
        "Define the tool function. A tool is essentially a Python function we expose to the agent:"
      ],
      "metadata": {
        "id": "J_DIfC8sV0dJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_bmi(query: str) -> str:\n",
        "  \"\"\"Calculate BMI given input \"weight,height\" in kg and meters.\"\"\"\n",
        "  try:\n",
        "      weight, height = query.split(',')\n",
        "      bmi = float(weight) / (float(height) ** 2)\n",
        "      return f\"Your BMI is {bmi:.1f}\"\n",
        "  except Exception as e:\n",
        "      return \"Please provide input as 'weight,height'.\"\n",
        "\n",
        "# This function expects a string like \"70,1.75\" and returns the BMI."
      ],
      "metadata": {
        "id": "XFAO3TEEJ7Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2\n",
        "Create the Tool object and initialize agent. LangChain has a Tool class to wrap the function with a name and description:"
      ],
      "metadata": {
        "id": "Kr4BoCjaJ7f6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import Tool, initialize_agent, AgentType\n",
        "\n",
        "bmi_tool = Tool(\n",
        "    name=\"BMI Calculator\",\n",
        "    func=calc_bmi,\n",
        "    description=\"Calculates BMI from input 'weight,height'. Example input: '70,1.75'.\"\n",
        ")\n",
        "tools = [bmi_tool]\n",
        "\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n"
      ],
      "metadata": {
        "id": "vYsMTH0-J7o7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set AgentType.ZERO_SHOT_REACT_DESCRIPTION, which means the agent will use the ReAct framework (Reason + Act) with the tool’s description to decide when to use it. The verbose=True will show the agent’s thought process (which is super insightful during development!).\n",
        "\n",
        "## Step 3\n",
        "\n",
        "Query the agent. Now ask a question that requires the tool:"
      ],
      "metadata": {
        "id": "ZRZ5fV04J7wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run(\"If I weigh 70 kg and my height is 1.75 m, what is my BMI?\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "ZqLKPxdhJ76-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Watch the logs (thanks to verbose=True). You’ll see something like the agent thinking “The user is asking for BMI, I have a tool for that. Let me parse inputs and call the tool…”. It should then output the numeric BMI.\n",
        "\n",
        "*How it works*: The LLM, guided by LangChain’s agent prompt, will decide to use BMI Calculator tool by outputting an “action” (which LangChain then executes by calling calc_bmi), then resume answering with the tool result. This showcases an AI augmented with a capability – vital for healthcare where calculations and database lookups are common.\n",
        "\n",
        "*Exercise*: Try tricking the agent: ask it a medical question that doesn’t need the BMI tool (like “What’s the normal heart rate for adults?”). See that it will not use the tool and just answer from its own knowledge. Then ask for BMI as above – it should use the tool. This flexibility is what makes agents powerful."
      ],
      "metadata": {
        "id": "0e4ELr-0PWsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. LangGraph Basics: Creating Branching Logic\n",
        "\n",
        "We’ve seen how **LangChain** handles single-turn or sequential tool use, but what if we want a structured multi-step conversation with branching paths?  \n",
        "This is where **LangGraph** comes in.\n",
        "\n",
        "LangGraph allows you to define a graph of nodes (each node can be an LLM call or function) and edges (transitions) explicitly, giving you fine control over dialogues and workflows.\n",
        "\n",
        "[Read more at langchain-ai.github.io](https://langchain-ai.github.io)\n",
        "\n",
        "## Key Concepts in LangGraph:\n",
        "\n",
        "- **State**: Carries the conversation or data. Often a list of messages or a custom object.\n",
        "- **Node**: A step in the workflow (could prompt the LLM, call a tool, etc.).\n",
        "- **Edge**: A transition from one node to the next. Edges can be conditional (like `if severity == \"ER\"` go to ER node).\n",
        "- **Graph**: The overall flowchart connecting nodes.\n",
        "\n",
        "LangChain integrates well with LangGraph – you can still use LangChain LLMs or tools inside LangGraph nodes.\n",
        "\n",
        "---\n",
        "\n",
        "## Example: Simple Graph for Triage\n",
        "\n",
        "1. **Ask Symptoms Node**:  \n",
        "   Bot asks, “What symptoms are you experiencing?”\n",
        "\n",
        "2. **Assess Severity Node (LLM)**:  \n",
        "   Takes the user’s symptom description and uses GPT-4 to classify severity (e.g., “ER” vs “ROUTINE”).\n",
        "\n",
        "3. **Branch**:  \n",
        "   - If \"ER\", go to **Emergency Advise Node**.\n",
        "   - If \"ROUTINE\", go to **Routine Care Advise Node**.\n",
        "\n",
        "4. **ER Advise Node**:  \n",
        "   Responds with:  \n",
        "   _\"Your symptoms may be serious. Please visit the ER...\"_\n",
        "\n",
        "5. **Routine Advise Node**:  \n",
        "   Responds with:  \n",
        "   _\"It looks like this can be managed with a regular doctor visit...\"_\n",
        "\n",
        "---\n",
        "\n",
        "We will not fully implement the graph in code (which would take more than a few minutes), but here’s how one could do it conceptually in **LangGraph’s Python API**.\n"
      ],
      "metadata": {
        "id": "yELwN-kzPW-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from typing import List, Union, TypedDict, Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "# Define our state structure\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[List[Union[HumanMessage, AIMessage, SystemMessage]], add_messages]\n",
        "\n",
        "# 1. Initialize graph\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# 2. Define nodes:\n",
        "def ask_symptoms(state):\n",
        "    return {\"messages\": [AIMessage(content=\"What symptoms are you experiencing?\")]}\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "\n",
        "def assess_severity(state):\n",
        "    # Get the last user message\n",
        "    user_msg = next((msg.content for msg in reversed(state[\"messages\"])\n",
        "                     if isinstance(msg, HumanMessage)), \"\")\n",
        "\n",
        "    prompt = f\"Classify the severity as 'ER' or 'ROUTINE' given the patient's symptoms: {user_msg}\"\n",
        "    response = llm.invoke([HumanMessage(content=prompt)])\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "def advise_er(state):\n",
        "    return {\"messages\": [AIMessage(content=\"Your symptoms might be serious. Please seek emergency care.\")]}\n",
        "\n",
        "def advise_routine(state):\n",
        "    return {\"messages\": [AIMessage(content=\"Your symptoms seem manageable. Schedule a routine appointment.\")]}\n",
        "\n",
        "# 3. Add nodes to the graph\n",
        "graph_builder.add_node(\"ask_symptoms\", ask_symptoms)\n",
        "graph_builder.add_node(\"assess_severity\", assess_severity)\n",
        "graph_builder.add_node(\"advise_er\", advise_er)\n",
        "graph_builder.add_node(\"advise_routine\", advise_routine)\n",
        "\n",
        "# 4. Define edges (the flow)\n",
        "graph_builder.add_edge(START, \"ask_symptoms\")\n",
        "graph_builder.add_edge(\"ask_symptoms\", \"assess_severity\")\n",
        "\n",
        "# Conditional branch based on LLM output in state\n",
        "def route_severity(state):\n",
        "    # Get the last assistant message\n",
        "    ai_message = next((msg.content for msg in reversed(state[\"messages\"])\n",
        "                      if isinstance(msg, AIMessage)), \"\")\n",
        "\n",
        "    if \"ER\" in ai_message.upper():\n",
        "        return \"advise_er\"\n",
        "    else:\n",
        "        return \"advise_routine\"\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"assess_severity\",\n",
        "    route_severity,\n",
        "    {\n",
        "        \"advise_er\": \"advise_er\",\n",
        "        \"advise_routine\": \"advise_routine\"\n",
        "    }\n",
        ")\n",
        "\n",
        "graph_builder.add_edge(\"advise_er\", END)\n",
        "graph_builder.add_edge(\"advise_routine\", END)\n",
        "\n",
        "# 5. Compile the graph\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "# Create initial state with user's message\n",
        "initial_state = {\"messages\": [HumanMessage(content=\"I have sudden chest pain and trouble breathing.\")]}\n",
        "\n",
        "# Run the graph\n",
        "final_state = graph.invoke(initial_state)\n",
        "\n",
        "# Print all messages in the conversation\n",
        "for msg in final_state[\"messages\"]:\n",
        "    if isinstance(msg, HumanMessage):\n",
        "        print(f\"User: {msg.content}\")\n",
        "    elif isinstance(msg, AIMessage):\n",
        "        print(f\"Assistant: {msg.content}\")"
      ],
      "metadata": {
        "id": "cQKc-PiaPXRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s your text converted into **Markdown** format:\n",
        "\n",
        "---\n",
        "\n",
        "# Conversation Structure and LangGraph Overview\n",
        "\n",
        "Don’t worry if not every line is clear – the idea is to see how we lay out the conversation structure explicitly.\n",
        "\n",
        "In **Step 2**, we created an LLM node that will receive the user’s symptoms and produce a severity classification. We then branch to different advice nodes depending on that result.\n",
        "\n",
        "LangGraph’s power is in this orchestration:  \n",
        "It ensures the flow is followed reliably (and can even enforce human review at certain nodes if needed).\n",
        "\n",
        "In fact, LangGraph is designed for **controllability** and **extensibility** in complex agent systems ([langchain-ai.github.io](https://langchain-ai.github.io)), which is perfect for scenarios like healthcare where you need predictable behavior and sometimes oversight.\n",
        "\n",
        "---\n",
        "\n",
        "### Example\n",
        "\n",
        "If we ran the above graph with the input symptoms, **GPT-4** might classify it as “ER” (since chest pain + shortness of breath is serious) and the user would get the ER advice.\n",
        "\n",
        "For a milder symptom input, it might go to **routine advice**.\n",
        "\n",
        "---\n",
        "\n",
        "### Visualization\n",
        "\n",
        "LangGraph can also visualize graphs (with **Mermaid charts**) – handy for understanding or debugging complex flows.\n",
        "\n",
        "In a Jupyter environment, one could do:\n",
        "\n",
        "```python\n",
        "graph.get_graph().draw_mermaid()\n",
        "```\n",
        "\n",
        "to see the flowchart.\n",
        "\n",
        "In Colab, you might need extra steps to display it, so we skip it here.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise\n",
        "\n",
        "Consider extending this flow:\n",
        "- What if the bot should ask one more question for moderate cases?\n",
        "- How would you insert another node and adjust conditions?\n",
        "\n",
        "Sketch a quick extension on paper or pseudocode.  \n",
        "This practice helps solidify how branching logic can be built.\n",
        "\n",
        "---\n",
        "\n",
        "Would you also like me to make an even **more polished** version with callouts, quotes, or tables? 🚀"
      ],
      "metadata": {
        "id": "FFXjCn1DPXXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Mini-Project: Smart Healthcare Triage Bot (LangGraph + LangChain)"
      ],
      "metadata": {
        "id": "omYoKQitPXhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Mini-Project: Smart Healthcare Triage Bot\n",
        "\n",
        "Now for the fun part – bringing it all together.  \n",
        "In this mini-project, we’ll outline how to build a **\"smart healthcare triage bot\"** using everything we’ve learned:\n",
        "\n",
        "- **LangChain** for LLM and tools.\n",
        "- **LangGraph** for conversation flow.\n",
        "- **OpenAI’s GPT-4** as our medical brain (you can use **3.5-turbo** if GPT-4 access is limited, though results might be less accurate).\n",
        "\n",
        "---\n",
        "\n",
        "## Workflow Recap\n",
        "\n",
        "The bot will:\n",
        "\n",
        "1. **Ask** the patient to describe their symptoms.\n",
        "2. **Use the LLM** to evaluate severity (maybe even list possible conditions, but primarily decide urgency).\n",
        "3. **Branch into one of two paths**:\n",
        "    - **Emergency Path**: If severe, advise ER or urgent care.\n",
        "    - **Routine Care Path**: If not severe, give home care tips or suggest a normal doctor visit.\n",
        "4. **(Optionally)** Follow up or ask if they need anything else.\n",
        "\n",
        "---\n",
        "\n",
        "## Good Practices to Implement\n",
        "\n",
        "- **Deterministic Output for Decision**:  \n",
        "  Prompt GPT-4 to respond with a **structured output** (like exactly the word `ER` or `ROUTINE`) to reliably trigger the correct branch.  \n",
        "  > This is a form of **prompt-based guardrail**.\n",
        "\n",
        "- **Temperature Control**:  \n",
        "  Keep **temperature low** for the decision node to avoid randomness.  \n",
        "  For explanatory nodes, you could allow a bit more creativity — but still stay cautious.\n",
        "\n",
        "- **Token Management**:  \n",
        "  Our prompts are short and we have few turns, so we’re safe.  \n",
        "  In longer chats, use **LangChain’s memory** or **truncation** to stay within limits.\n",
        "\n",
        "- **Basic Guardrails**:  \n",
        "  Instruct the LLM clearly:\n",
        "  - Only make the **ER vs ROUTINE** call.\n",
        "  - If unsure or unsafe content is detected, **default to advising an actual doctor** (simple safety net).\n",
        "\n",
        "---\n",
        "\n",
        "## Coding the Bot\n",
        "\n",
        "We essentially covered this in **Section 4**.  \n",
        "In practice, you would refine the `severity_node` prompt to ensure clarity.\n",
        "\n",
        "For example:\n",
        "\n",
        "```plaintext\n",
        "If chest pain, difficulty breathing, serious injury, or other major symptoms are described, classify as \"ER\".\n",
        "Otherwise, classify as \"ROUTINE\".\n",
        "If unsure, classify as \"ER\".\n",
        "Respond ONLY with \"ER\" or \"ROUTINE\" — no extra text.\n",
        "```\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "NeIPDJF5PXyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Next Step After Classification\n",
        "\n",
        "After getting `\"ER\"` or `\"ROUTINE\"`, the next node might actually be **another LLM call** to explain the advice **more verbosely** to the user.\n",
        "\n",
        "For example:\n",
        "- An `er_advice_node` could be another `LLMNode` with a prompt like:\n",
        "\n",
        "```plaintext\n",
        "Explain to the patient kindly that their symptoms need emergency care and what they should do now.\n",
        "```\n",
        "\n",
        "This way:\n",
        "- **GPT-4** can provide a **detailed** and **kind** response.\n",
        "- And because we only enter this node after a known \"ER\" classification, it stays **on track**.\n",
        "\n",
        "---\n",
        "\n",
        "## Running the Bot\n",
        "\n",
        "In a real app:\n",
        "- You would **run the compiled graph in a loop** interacting with a **user interface** (or chat interface).\n",
        "\n",
        "For this workshop:\n",
        "- We **simulate** by providing the user’s input directly.\n",
        "\n",
        "You can try different symptom inputs to see how it routes:\n",
        "\n",
        "### Test 1\n",
        "\n",
        "```python\n",
        "symptoms = \"I have a mild headache and a runny nose.\"\n",
        "```\n",
        "- Likely **ROUTINE** path with gentle home care advice.\n",
        "\n",
        "### Test 2\n",
        "\n",
        "```python\n",
        "symptoms = \"Sudden numbness on one side of face and confusion.\"\n",
        "```\n",
        "- Likely **ER** path because those sound like **stroke symptoms**.\n",
        "\n",
        "---\n",
        "\n",
        "## Exercise\n",
        "\n",
        "Discuss with a partner or reflect:\n",
        "\n",
        "- What other **branches** could be useful in a triage bot?\n",
        "- Maybe a **third branch** for:\n",
        "  - **Advice to call a nurse line**, or\n",
        "  - **Integrating a FAQ retrieval** if the user asks an unrelated question.\n",
        "\n",
        "**Consider:**  \n",
        "How might you integrate a **retrieval tool** into the graph?  \n",
        "*(Hint: LangChain’s vector search as a node.)*\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "G6FEHX8IPX6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 6. Best Practices & Next Steps\n",
        "\n",
        "Before we conclude, let’s summarize some best practices when building LLM-powered healthcare apps:\n",
        "\n",
        "---\n",
        "\n",
        "# Best Practices\n",
        "\n",
        "- **Prompt Engineering for Clarity**:  \n",
        "  Be explicit in prompts when you need structured outputs or certain behaviors.  \n",
        "  Our severity prompt is an example of guiding the model firmly.\n",
        "\n",
        "- **Keep Temperature Low for Critical Decisions**:  \n",
        "  A **temperature of 0** (or very low) is ideal when the bot is making a **classification** or any **important determination** — it makes the output **more predictable**.  \n",
        "  Save creativity for less critical parts (even then, healthcare usually needs a **factual tone**).\n",
        "\n",
        "- **Monitor Tokens and Responses**:  \n",
        "  Always track:\n",
        "  - Length of user input\n",
        "  - Length of LLM output  \n",
        "  In **LangChain**, you might use:\n",
        "  - `ConversationBufferMemory` with a **max token limit**\n",
        "  - Manually trimming old messages\n",
        "  Also, leverage OpenAI’s token usage info to log and monitor conversation costs.\n",
        "\n",
        "- **Add Guardrails**:  \n",
        "  We added simple guardrails (like **default to ER if uncertain**).  \n",
        "  In production, add more:\n",
        "  - Use OpenAI’s **content moderation API** or simple keyword checks.\n",
        "  - Possibly insert **human review** for edge cases.  \n",
        "  > *LangGraph is built with human handoff in mind, allowing a human node or approval step* ([source](https://langchain-ai.github.io)).\n",
        "\n",
        "- **Continuous Evaluation**:  \n",
        "  - Test your bot with **many scenarios**, across patient demographics, to ensure **fairness**.\n",
        "  \n",
        "- **Iterate and Update**:  \n",
        "  - **Medical guidelines** evolve.\n",
        "  - **LLMs** evolve.  \n",
        "  Using **RAG**, updating info is easy — just update your documents.\n",
        "  If needed, retrain or prompt-tune the model.\n",
        "\n",
        "> *Example: A UCLA study found GPT-4 had no triage bias by race — but we must stay vigilant.*\n",
        "\n",
        "---\n",
        "\n",
        "# Exercise\n",
        "\n",
        "As a closing thought experiment:\n",
        "\n",
        "- **Design a small improvement** to the bot.\n",
        "- Example:  \n",
        "  - If **ER**, after giving advice, the bot asks:  \n",
        "    *“Do you want me to call 911 for you?”*\n",
        "  - How might you implement this?\n",
        "    - Add another node\n",
        "    - Use a **Twilio API** tool call\n",
        "\n",
        "> This is just brainstorming how you can **extend the graph** further — possibilities are endless!\n",
        "\n",
        "---\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "In this hands-on session, we:\n",
        "\n",
        "- Motivated why **frameworks** like **LangChain** and **LangGraph** are valuable for healthcare AI (combining LLM smarts with tools and control).\n",
        "- Set up a **Colab environment** for rapid development.\n",
        "- Explored **LangChain basics** with prompt templates and LLM calls.\n",
        "- Implemented **Retrieval-Augmented Generation (RAG)** to ground answers in clinical documents.\n",
        "- Used **LangChain Agents** to give our bot **tool-using abilities** (e.g., BMI calculator).\n",
        "- Learned the fundamentals of **LangGraph** to create **branching conversational flows**.\n",
        "- Built a **mini triage bot** that guides patients to the right care, showing how **GPT-4 + careful design** can triage accurately.\n",
        "\n",
        "---\n",
        "\n",
        "By pacing these steps over an hour with interactive coding and exercises, you should now have a **solid foundation** to start building your own **healthcare AI applications**.\n",
        "\n",
        "**The combo of LangChain + LangGraph** gives you both:\n",
        "\n",
        "- **High-level convenience**\n",
        "- **Low-level control**\n",
        "\n",
        "A **powerful mix** for sensitive domains like healthcare.\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- Continue experimenting!\n",
        "- Ideas:\n",
        "  - Hook up a **larger medical knowledge base**.\n",
        "  - Integrate **speech-to-text** for **voice-based triage**.\n",
        "  - Deploy your bot on a **simple web app**.\n",
        "\n",
        "The skills you learned — **prompt design**, **RAG**, **agents**, **graphs** — will apply to **many projects beyond healthcare** too.\n",
        "\n",
        "---\n",
        "\n",
        "# Thank You! 🎉\n",
        "\n",
        "Feel free to share your mini-projects or ask questions.  \n",
        "Together, let’s build **AI that truly helps people**.\n",
        "\n",
        "---\n",
        "\n",
        "## Sources\n",
        "\n",
        "- **LangChain**: Open-source framework for integrating LLMs with external data, memory, and tools.\n",
        "- **GPT-4**: Demonstrated triage and diagnostic accuracy comparable to physicians.\n",
        "- **Retrieval-Augmented Generation (RAG)**: Grounds outputs in factual, domain-specific knowledge.\n",
        "- **LangGraph**: Low-level orchestration library for building reliable, controllable agents with custom workflows.\n",
        "- **OpenAI Temperature Parameter**: Controls randomness; lower = more deterministic (ideal for critical apps).\n",
        "\n",
        "---\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "T74UNDhoVZ6M"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kGYdcu70PYCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BKgBnns3PYJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pzJ41pKqPYWF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p7OJ6wRpPYe4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}