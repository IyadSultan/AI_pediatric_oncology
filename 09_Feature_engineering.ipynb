{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMharUnR82hHNvZgCAblPrL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IyadSultan/AI_pediatric_oncology/blob/main/09_Feature_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering for Tabular & Time-Series Data  \n",
        "**Level:** Beginner → Intermediate  **Duration:** ≈ 2 hours  \n",
        "\n",
        "Feature engineering transforms raw data into informative features that boost model performance.  \n",
        "This notebook covers both **tabular** and **time-series** techniques:\n",
        "\n",
        "* Handling missing values  \n",
        "* Encoding categorical variables  \n",
        "* Binning & discretization  \n",
        "* Feature scaling & transformation  \n",
        "* Feature extraction (datetime parts, polynomial terms)  \n",
        "* Interaction features  \n",
        "* Feature-selection methods  \n",
        "* Time-series specifics (lags, rollings, seasonal features)  \n",
        "* Automated FE with **tsfresh** & **Featuretools**\n",
        "\n",
        "> **How to use this notebook**  \n",
        "> 1. Run the cells in order.  \n",
        "> 2. Tweak code or plug in your own data.  \n",
        "> 3. Install extra libraries when prompted.\n"
      ],
      "metadata": {
        "id": "-ROxNvadHhui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setup & Sample Data -----------------------------------\n",
        "!pip install seaborn tsfresh featuretools --quiet\n",
        "\n",
        "import numpy as np, pandas as pd, seaborn as sns\n",
        "df = sns.load_dataset(\"titanic\")\n",
        "print(\"Titanic shape:\", df.shape)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "nBqPmSQ8HiPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1  Handling Missing Values  \n",
        "\n",
        "Real-world data nearly always contain **NaNs**.  \n",
        "Problems: biased stats, models that crash, lost information.\n",
        "\n",
        "**Common strategies**\n",
        "\n",
        "| Strategy | When to use | Caveats |\n",
        "|----------|-------------|---------|\n",
        "| **Drop rows/cols** | few NaNs or column nearly empty | data loss |\n",
        "| **Impute constant** | categorical “Unknown”, numeric 0 | may hide signal |\n",
        "| **Statistical impute** | mean/median/mode | assumes missing at random |\n",
        "| **Model-based impute** | KNN / Iterative | heavier, possible bias |\n",
        "| **Missing flag** | when “missingness” is informative | add extra column |\n",
        "\n",
        "We’ll inspect Titanic, drop the hopeless **deck** column, then impute Age (median) & Embarked (mode).\n"
      ],
      "metadata": {
        "id": "MvwiWN-_HiXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Missing-value inspection ------------------------------\n",
        "df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "hVbWPS15Hieu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Drop 'deck' and view size impact ----------------------\n",
        "df = df.drop(columns=[\"deck\"])\n",
        "print(\"Cols after drop:\", df.columns.tolist())\n",
        "print(\"Rows after dropping any-NaN rows:\",\n",
        "      len(df.dropna()), \"of\", len(df))\n"
      ],
      "metadata": {
        "id": "Ut5d2lztHwSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Median/Mode imputation --------------------------------\n",
        "age_med  = df[\"age\"].median()\n",
        "emb_mode = df[\"embarked\"].mode()[0]\n",
        "\n",
        "df[\"age\"]      = df[\"age\"].fillna(age_med)\n",
        "df[\"embarked\"] = df[\"embarked\"].fillna(emb_mode)\n",
        "df[[\"age\",\"embarked\"]].isna().sum()\n"
      ],
      "metadata": {
        "id": "t20Ja8uxHxmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Pipeline-friendly imputation demo ---------------------\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "sample = sns.load_dataset(\"titanic\").drop(columns=[\"deck\"])\n",
        "imp_med  = SimpleImputer(strategy=\"median\")\n",
        "imp_freq = SimpleImputer(strategy=\"most_frequent\")\n",
        "\n",
        "sample[\"age\"]      = imp_med .fit_transform(sample[[\"age\"]])\n",
        "sample[\"embarked\"] = imp_freq.fit_transform(sample[[\"embarked\"]])\n",
        "sample[[\"age\",\"embarked\"]].isna().sum()\n"
      ],
      "metadata": {
        "id": "ElGz5i3EH1oB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2  Encoding Categorical Variables  \n",
        "\n",
        "* **One-Hot** for nominal (sex, embarked)  \n",
        "* **Ordinal** for ordered (First > Second > Third)  \n",
        "* Avoid plain label-encoding on nominal features.\n",
        "\n",
        "We’ll one-hot `sex` & `embarked`, then ordinal-encode `class`.\n"
      ],
      "metadata": {
        "id": "rIwKdEjtHimK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- One-Hot encode ----------------------------------------\n",
        "dummies = pd.get_dummies(df[[\"sex\",\"embarked\"]], drop_first=False)\n",
        "df      = pd.concat([df, dummies], axis=1).drop(columns=[\"sex\",\"embarked\"])\n",
        "df.head(3)\n"
      ],
      "metadata": {
        "id": "g-RME0NrHitX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Ordinal encode 'class' --------------------------------\n",
        "class_map = {\"Third\":1,\"Second\":2,\"First\":3}\n",
        "df[\"class_encoded\"] = df[\"class\"].map(class_map)\n",
        "df[[\"class\",\"class_encoded\"]].head()\n"
      ],
      "metadata": {
        "id": "16mY1BNwH6yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3  Binning & Discretization  \n",
        "\n",
        "Why bin? Robust to outliers, capture step-wise effects, simplify models.\n",
        "\n",
        "* **Domain bins** – age groups  \n",
        "* **Quantile bins** – fare quartiles  \n",
        "* **KBinsDiscretizer** – automated (uniform / quantile)\n",
        "\n",
        "We’ll create `AgeGroup` (Child/Teen/Adult/Senior) and fare quartiles.\n"
      ],
      "metadata": {
        "id": "NNyt4bBtHiz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Age domain bins ---------------------------------------\n",
        "bins   = [0,12,19,59,np.inf]\n",
        "labels = [\"Child\",\"Teenager\",\"Adult\",\"Senior\"]\n",
        "df[\"AgeGroup\"] = pd.cut(df[\"age\"], bins=bins, labels=labels)\n",
        "df[[\"age\",\"AgeGroup\"]].head()\n"
      ],
      "metadata": {
        "id": "WeKvLyXAHi7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Fare quartile bins ------------------------------------\n",
        "df[\"Fare_bin\"] = pd.qcut(df[\"fare\"], q=4, labels=[\"Q1\",\"Q2\",\"Q3\",\"Q4\"])\n",
        "df[\"Fare_bin\"].value_counts()\n"
      ],
      "metadata": {
        "id": "oN1dvkE0IAWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- KBinsDiscretizer demo ---------------------------------\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "kb = KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\")\n",
        "df[\"age_bin3\"] = kb.fit_transform(df[[\"age\"]])\n",
        "df[[\"age\",\"age_bin3\"]].head()\n"
      ],
      "metadata": {
        "id": "ckL55YqlICrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4  Feature Scaling & Transformation  \n",
        "\n",
        "Models like k-NN, SVM, neural nets need comparable scales.\n",
        "\n",
        "* **StandardScaler** – mean 0, std 1  \n",
        "* **MinMaxScaler** – 0 → 1  \n",
        "* **Log / Box-Cox** – fix skew\n",
        "\n",
        "We’ll scale `age`, `fare`, `sibsp`, `parch`, and add a log-fare.\n"
      ],
      "metadata": {
        "id": "NMhwhvRYHjBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "num_cols = [\"age\",\"fare\",\"sibsp\",\"parch\"]\n",
        "orig     = df[num_cols].copy()\n",
        "\n",
        "std = StandardScaler().fit_transform(orig)\n",
        "mm  = MinMaxScaler().fit_transform(orig)\n",
        "\n",
        "print(\"Std-scaled sample:\\n\", pd.DataFrame(std, columns=num_cols).head())\n",
        "\n",
        "df[\"fare_log10\"] = np.log10(df[\"fare\"] + 1e-5)\n",
        "df[[\"fare\",\"fare_log10\"]].head()\n"
      ],
      "metadata": {
        "id": "A5BCrwEhHjIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5  Feature Extraction  \n",
        "\n",
        "### 5.1 Datetime Parts  \n",
        "Break timestamps into year, month, dow, hour, etc.\n",
        "\n",
        "### 5.2 Polynomial Features  \n",
        "`PolynomialFeatures(degree=2)` adds squares & interactions.\n"
      ],
      "metadata": {
        "id": "RmFYi4p8HjPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Datetime parts demo -----------------------------------\n",
        "dates = pd.DataFrame({\"purchase_date\": pd.to_datetime([\n",
        "    \"2021-01-01 14:23\", \"2021-07-15 09:00\",\n",
        "    \"2022-03-05 20:45\", \"2022-03-06 12:00\",\n",
        "    \"2022-12-25 00:00\"])})\n",
        "\n",
        "dates[\"year\"]  = dates[\"purchase_date\"].dt.year\n",
        "dates[\"month\"] = dates[\"purchase_date\"].dt.month\n",
        "dates[\"dow\"]   = dates[\"purchase_date\"].dt.day_name()\n",
        "dates[\"hour\"]  = dates[\"purchase_date\"].dt.hour\n",
        "dates\n"
      ],
      "metadata": {
        "id": "BCE9Zh-tHjWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PolynomialFeatures demo -------------------------------\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "\n",
        "af_poly = poly.fit_transform(df[[\"age\",\"fare\"]].fillna(0))\n",
        "pd.DataFrame(af_poly, columns=poly.get_feature_names_out([\"age\",\"fare\"])).head()\n"
      ],
      "metadata": {
        "id": "26yVLkDkIKbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6  Interaction Features  \n",
        "\n",
        "Domain-driven combos often matter:\n",
        "\n",
        "* `family_size = sibsp + parch + 1`  \n",
        "* `is_alone` flag  \n",
        "* `fare_per_person` ratio  \n",
        "* Categorical combos like `sex_pclass`\n"
      ],
      "metadata": {
        "id": "sJRWQQzWHjc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Family size & friends ---------------------------------\n",
        "df[\"family_size\"] = df[\"sibsp\"] + df[\"parch\"] + 1\n",
        "df[\"is_alone\"]    = (df[\"family_size\"] == 1).astype(int)\n",
        "df[\"fare_per_person\"] = df[\"fare\"] / df[\"family_size\"]\n",
        "df[[\"sibsp\",\"parch\",\"family_size\",\"is_alone\",\"fare_per_person\"]].head()\n"
      ],
      "metadata": {
        "id": "xNqkXoQuHjj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- sex_pclass combo (fresh dataset for demo) -------------\n",
        "raw = sns.load_dataset(\"titanic\")\n",
        "raw[\"sex_pclass\"] = raw[\"sex\"] + \"_\" + raw[\"pclass\"].astype(str)\n",
        "raw[\"sex_pclass\"].value_counts().head()\n"
      ],
      "metadata": {
        "id": "TyhVlKcQIQAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7  Feature Selection  \n",
        "\n",
        "* **Filter** – SelectKBest(ANOVA)  \n",
        "* **Embedded** – tree importances  \n",
        "* **Wrapper** – RFE\n",
        "\n",
        "We’ll build a synthetic dataset and try each.\n"
      ],
      "metadata": {
        "id": "xq4wiWWsHjra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "from sklearn.ensemble  import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20,\n",
        "                           n_informative=5, n_redundant=5,\n",
        "                           random_state=42, shuffle=False)\n",
        "\n",
        "# Filter\n",
        "flt = SelectKBest(f_classif, k=5).fit(X, y)\n",
        "print(\"Filter selected:\", flt.get_support(indices=True))\n",
        "\n",
        "# Embedded\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=42).fit(X, y)\n",
        "print(\"Top RF features:\", np.argsort(rf.feature_importances_)[::-1][:5])\n",
        "\n",
        "# Wrapper\n",
        "rfe = RFE(LogisticRegression(max_iter=1000, solver=\"liblinear\"), n_features_to_select=5)\n",
        "rfe.fit(X, y)\n",
        "print(\"RFE selected:\", np.where(rfe.support_)[0])\n"
      ],
      "metadata": {
        "id": "E1FXOK2UHjwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8  Time-Series Feature Engineering  \n",
        "\n",
        "Key ideas: **lags, rolling stats, diffs, seasonal parts**.\n"
      ],
      "metadata": {
        "id": "MUqPPQ34Hj2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Simulate monthly series -------------------------------\n",
        "idx   = pd.date_range(\"2019-01\", periods=36, freq=\"M\")\n",
        "t     = np.arange(36)\n",
        "np.random.seed(0)\n",
        "vals  = 10 + 0.5*t + 5*np.sin(2*np.pi*t/12) + np.random.normal(0,2,36)\n",
        "ts    = pd.DataFrame({\"Value\": vals}, index=idx)\n",
        "\n",
        "# Lag & rolling\n",
        "ts[\"lag1\"]   = ts[\"Value\"].shift(1)\n",
        "ts[\"lag12\"]  = ts[\"Value\"].shift(12)\n",
        "ts[\"roll3\"]  = ts[\"Value\"].rolling(3).mean()\n",
        "ts[\"diff1\"]  = ts[\"Value\"].diff()\n",
        "ts.tail()\n"
      ],
      "metadata": {
        "id": "OCXhuqpVHj9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9  Automated Feature Engineering  \n",
        "\n",
        "### 9.1 tsfresh – exhaustive stats for each time-series  \n",
        "### 9.2 Featuretools – Deep Feature Synthesis for relational data\n"
      ],
      "metadata": {
        "id": "vSi2qAAPHkDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- tsfresh minimal example -------------------------------\n",
        "from tsfresh.feature_extraction import extract_features\n",
        "mini = ts.reset_index().rename(columns={\"index\":\"time\"})\n",
        "mini[\"id\"] = 1\n",
        "features = extract_features(mini, column_id=\"id\", column_sort=\"time\",\n",
        "                            default_fc_parameters={\"mean\":None,\"median\":None})\n",
        "features\n"
      ],
      "metadata": {
        "id": "wMTxypxgHkJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Featuretools mock-customer demo -----------------------\n",
        "import featuretools as ft\n",
        "data = ft.demo.load_mock_customer()\n",
        "es = ft.EntitySet(id=\"cust\")\n",
        "es = es.add_dataframe(\"customers\",   data[\"customers\"],   index=\"customer_id\")\n",
        "es = es.add_dataframe(\"sessions\",    data[\"sessions\"],    index=\"session_id\",    time_index=\"session_start\")\n",
        "es = es.add_dataframe(\"transactions\",data[\"transactions\"],index=\"transaction_id\",time_index=\"transaction_time\")\n",
        "es = es.add_relationship(parent_dataframe_name=\"customers\",  parent_column_name=\"customer_id\",\n",
        "                         child_dataframe_name=\"sessions\",    child_column_name=\"customer_id\")\n",
        "es = es.add_relationship(parent_dataframe_name=\"sessions\",   parent_column_name=\"session_id\",\n",
        "                         child_dataframe_name=\"transactions\",child_column_name=\"session_id\")\n",
        "fm, defs = ft.dfs(entityset=es, target_dataframe_name=\"customers\", max_depth=2)\n",
        "fm.head()\n"
      ],
      "metadata": {
        "id": "fKDxNh5rIaMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aT_AoBnCHmbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA1ia8bLHgdw"
      },
      "outputs": [],
      "source": []
    }
  ]
}