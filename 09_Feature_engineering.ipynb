{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3L6GpGkJW91HehKIF0HUm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IyadSultan/AI_pediatric_oncology/blob/main/09_Feature_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering for Tabular & Time-Series Data  \n",
        "**Level:** Beginner → Intermediate  **Duration:** ≈ 2 hours  \n",
        "\n",
        "Feature engineering transforms raw data into informative features that boost model performance.  \n",
        "This notebook covers both **tabular** and **time-series** techniques:\n",
        "\n",
        "* Handling missing values  \n",
        "* Encoding categorical variables  \n",
        "* Binning & discretization  \n",
        "* Feature scaling & transformation  \n",
        "* Feature extraction (datetime parts, polynomial terms)  \n",
        "* Interaction features  \n",
        "* Feature-selection methods  \n",
        "* Time-series specifics (lags, rollings, seasonal features)  \n",
        "* Automated FE with **tsfresh** & **Featuretools**\n",
        "\n",
        "> **How to use this notebook**  \n",
        "> 1. Run the cells in order.  \n",
        "> 2. Tweak code or plug in your own data.  \n",
        "> 3. Install extra libraries when prompted.\n"
      ],
      "metadata": {
        "id": "-ROxNvadHhui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setup & Sample Data -----------------------------------\n",
        "!pip install seaborn tsfresh featuretools --quiet"
      ],
      "metadata": {
        "id": "nBqPmSQ8HiPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, pandas as pd, seaborn as sns\n",
        "df = sns.load_dataset(\"titanic\")\n",
        "print(\"Titanic shape:\", df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "1FxZkcCHS2pC",
        "outputId": "4da0fc47-8ea5-4e24-f3b6-52a0b8c9e08d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Titanic shape: (891, 15)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
              "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
              "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
              "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
              "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
              "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
              "\n",
              "     who  adult_male deck  embark_town alive  alone  \n",
              "0    man        True  NaN  Southampton    no  False  \n",
              "1  woman       False    C    Cherbourg   yes  False  \n",
              "2  woman       False  NaN  Southampton   yes   True  \n",
              "3  woman       False    C  Southampton   yes  False  \n",
              "4    man        True  NaN  Southampton    no   True  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d9589813-9f3b-4014-a280-d53070023b38\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survived</th>\n",
              "      <th>pclass</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>fare</th>\n",
              "      <th>embarked</th>\n",
              "      <th>class</th>\n",
              "      <th>who</th>\n",
              "      <th>adult_male</th>\n",
              "      <th>deck</th>\n",
              "      <th>embark_town</th>\n",
              "      <th>alive</th>\n",
              "      <th>alone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>S</td>\n",
              "      <td>Third</td>\n",
              "      <td>man</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>no</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C</td>\n",
              "      <td>First</td>\n",
              "      <td>woman</td>\n",
              "      <td>False</td>\n",
              "      <td>C</td>\n",
              "      <td>Cherbourg</td>\n",
              "      <td>yes</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>S</td>\n",
              "      <td>Third</td>\n",
              "      <td>woman</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>yes</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>S</td>\n",
              "      <td>First</td>\n",
              "      <td>woman</td>\n",
              "      <td>False</td>\n",
              "      <td>C</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>yes</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>S</td>\n",
              "      <td>Third</td>\n",
              "      <td>man</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>no</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d9589813-9f3b-4014-a280-d53070023b38')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d9589813-9f3b-4014-a280-d53070023b38 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d9589813-9f3b-4014-a280-d53070023b38');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-90def0f6-93d2-421e-9bf9-1a495df7a844\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-90def0f6-93d2-421e-9bf9-1a495df7a844')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-90def0f6-93d2-421e-9bf9-1a495df7a844 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 891,\n  \"fields\": [\n    {\n      \"column\": \"survived\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pclass\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"female\",\n          \"male\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.526497332334044,\n        \"min\": 0.42,\n        \"max\": 80.0,\n        \"num_unique_values\": 88,\n        \"samples\": [\n          0.75,\n          22.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sibsp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 8,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"parch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fare\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 49.693428597180905,\n        \"min\": 0.0,\n        \"max\": 512.3292,\n        \"num_unique_values\": 248,\n        \"samples\": [\n          11.2417,\n          51.8625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embarked\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"S\",\n          \"C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Third\",\n          \"First\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"who\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"man\",\n          \"woman\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"adult_male\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"deck\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"C\",\n          \"E\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embark_town\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Southampton\",\n          \"Cherbourg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"alive\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"yes\",\n          \"no\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"alone\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1  Handling Missing Values  \n",
        "\n",
        "From the output above, we can observe the count of missing values per column. In the Titanic dataset, the age and embarked columns have a few missing values, and the deck column has a lot of missing values. (The deck feature indicates passenger deck levels on the ship, and many entries are missing since not all passengers have a recorded deck.)\n",
        "**Common strategies**\n",
        "\n",
        "| Strategy | When to use | Caveats |\n",
        "|----------|-------------|---------|\n",
        "| **Drop rows/cols** | few NaNs or column nearly empty | data loss |\n",
        "| **Impute constant** | categorical “Unknown”, numeric 0 | may hide signal |\n",
        "| **Statistical impute** | mean/median/mode | assumes missing at random |\n",
        "| **Model-based impute** | KNN / Iterative | heavier, possible bias |\n",
        "| **Missing flag** | when “missingness” is informative | add extra column |\n",
        "\n",
        "\n",
        "**Strategy 1:** Removing missing data\n",
        "If a column is mostly missing (for example, deck is missing for the majority of passengers), it might be prudent to drop that column entirely, as it may not be very useful. Similarly, if only a few rows have missing data but in critical columns, and if dropping them doesn't lose too much data, we might drop those rows. Let's drop the deck column and see how many rows remain if we drop any rows with any missing values:\n",
        "\n"
      ],
      "metadata": {
        "id": "MvwiWN-_HiXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Missing-value inspection ------------------------------\n",
        "df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "hVbWPS15Hieu",
        "outputId": "9ba1e1ea-7e89-4d96-8227-4ef398ae68a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "survived         0\n",
              "pclass           0\n",
              "sex              0\n",
              "age            177\n",
              "sibsp            0\n",
              "parch            0\n",
              "fare             0\n",
              "embarked         2\n",
              "class            0\n",
              "who              0\n",
              "adult_male       0\n",
              "deck           688\n",
              "embark_town      2\n",
              "alive            0\n",
              "alone            0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>survived</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pclass</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sex</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sibsp</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>parch</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fare</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>embarked</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>class</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>who</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>adult_male</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>deck</th>\n",
              "      <td>688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>embark_town</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>alive</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>alone</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Drop 'deck' and view size impact ----------------------\n",
        "df = df.drop(columns=[\"deck\"])\n",
        "print(\"Cols after drop:\", df.columns.tolist())\n",
        "print(\"Rows after dropping any-NaN rows:\",\n",
        "      len(df.dropna()), \"of\", len(df))\n"
      ],
      "metadata": {
        "id": "Ut5d2lztHwSx",
        "outputId": "ac1e3002-6fdb-4236-c5ef-35e2581fee04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cols after drop: ['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'class', 'who', 'adult_male', 'embark_town', 'alive', 'alone']\n",
            "Rows after dropping any-NaN rows: 712 of 891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the output, notice how many rows would be left after dropping all rows with any missing value. We do this check to illustrate the impact: if a lot of rows are dropped, we might prefer imputation instead. In this case, dropping all missing data might remove a significant number of passengers, which could throw away useful information. Since we want to keep as much data as possible, let's opt for imputation for the remaining missing values (Age and Embarked):\n",
        "**Strategy 2:** Imputing missing values\n",
        "- For the numeric age feature, a common choice is to fill missing ages with the median age (median is used instead of mean if the distribution is skewed or has outliers).\n",
        "- For the categorical embarked feature, we can fill missing entries with the mode (the most common port of embarkation).\n",
        "We'll perform these imputations using pandas. (Alternatively, one can use scikit-learn's SimpleImputer – we'll show that as well.)"
      ],
      "metadata": {
        "id": "Eww-hOVqTcHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute missing Age with median, and missing Embarked with mode\n",
        "median_age = df['age'].median()\n",
        "mode_embarked = df['embarked'].mode()[0]  # mode() returns a Series\n",
        "print(\"Imputing missing ages with median:\", median_age)\n",
        "print(\"Imputing missing embarked with mode:\", mode_embarked)\n",
        "\n",
        "df['age'] = df['age'].fillna(median_age)\n",
        "df['embarked'] = df['embarked'].fillna(mode_embarked)\n",
        "\n",
        "# Verify no missing values remain in age and embarked\n",
        "print(\"Remaining missing values:\", df[['age','embarked']].isnull().sum().to_dict())\n"
      ],
      "metadata": {
        "id": "t20Ja8uxHxmf",
        "outputId": "3b339fa7-2483-41ff-ea19-e46665780507",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imputing missing ages with median: 28.0\n",
            "Imputing missing embarked with mode: S\n",
            "Remaining missing values: {'age': 0, 'embarked': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After imputation, the age and embarked columns should have no missing values. We filled age with the median (~28 years old, for example) and embarked with the most common port (likely \"S\" for Southampton in this dataset).\n",
        "\n",
        "**Using scikit-learn's Imputer:**\n",
        "For completeness, let's also demonstrate using scikit-learn's SimpleImputer to fill missing values. This is useful when building machine learning pipelines, so that imputation is combined with modeling steps and can be applied consistently to training and test data."
      ],
      "metadata": {
        "id": "essn8FvqUI8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Pipeline-friendly imputation demo ---------------------\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "sample = sns.load_dataset(\"titanic\").drop(columns=[\"deck\"])\n",
        "imp_med  = SimpleImputer(strategy=\"median\")\n",
        "imp_freq = SimpleImputer(strategy=\"most_frequent\")\n",
        "\n",
        "sample[\"age\"]      = imp_med .fit_transform(sample[[\"age\"]])\n",
        "sample[\"embarked\"] = imp_freq.fit_transform(sample[[\"embarked\"]])\n",
        "sample[[\"age\",\"embarked\"]].isna().sum()\n"
      ],
      "metadata": {
        "id": "ElGz5i3EH1oB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2  Encoding Categorical Variables  \n",
        "\n",
        "* **One-Hot** for nominal (sex, embarked)  \n",
        "* **Ordinal** for ordered (First > Second > Third)  \n",
        "* Avoid plain label-encoding on nominal features.\n",
        "\n",
        "\n",
        "\n",
        "Many machine learning algorithms require numeric input and cannot directly handle categorical strings. Encoding categorical features means converting category labels into numerical values. There are different encoding techniques depending on the nature of the categorical data:\n",
        "One-Hot Encoding (Dummy Variables): Create a new binary column for each category value, indicating presence (1) or absence (0) of that category for each observation. This is suitable for nominal categories (no natural order), e.g., embarked (C/Q/S) or sex (male/female). One-hot encoding avoids implying any ordinal relationship.\n",
        "\n",
        "- Ordinal Encoding (Label Encoding with order): Map each category to an integer value (e.g., 1, 2, 3) based on some order. This is suitable for ordinal categories where the categories have an inherent rank. For example, pclass (passenger class) is 1, 2, 3 for first, second, third class – here 1st > 2nd > 3rd in terms of luxury, so we could encode 1st=3, 2nd=2, 3rd=1 or similar to preserve that order.\n",
        "- Label Encoding (arbitrary integers): Assign an arbitrary numeric code to each category (e.g., red=0, green=1, blue=2). This is quick, but not recommended for nominal categories, because the model may interpret 2 > 1 > 0 as implying an order or magnitude. Use it only for ordinal data or when using certain models (like tree-based) that can treat the numeric codes as just categories internally.\n",
        "- Frequency or Target Encoding (advanced): Replace categories with their frequency or with target variable statistics (like mean target value per category). These are more advanced techniques often used in certain competitions, but they require caution to avoid overfitting (and usually using cross-validation schemes).\n",
        "\n",
        "Let's demonstrate encoding on the Titanic dataset for the sex (binary nominal) and embarked (nominal with 3 values) columns. We will use one-hot encoding for these. Pandas provides a convenient method pd.get_dummies for one-hot encoding.\n"
      ],
      "metadata": {
        "id": "rIwKdEjtHimK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode 'sex' and 'embarked' features\n",
        "print(\"Unique values in 'sex':\", df['sex'].unique())\n",
        "print(\"Unique values in 'embarked':\", df['embarked'].unique())\n",
        "\n",
        "encoded_df = pd.get_dummies(df[['sex', 'embarked']], drop_first=False)\n",
        "encoded_df.head()\n"
      ],
      "metadata": {
        "id": "g-RME0NrHitX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "b9b5d09d-8f58-4bc2-8bd9-25943f7cf999"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a97bf023ab32>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# One-hot encode 'sex' and 'embarked' features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unique values in 'sex':\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sex'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unique values in 'embarked':\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embarked'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mencoded_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sex'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'embarked'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, get_dummies creates a column for each category. We see new columns like sex_female, sex_male, embarked_C, embarked_Q, embarked_S with 0/1 values. We could drop one dummy column per feature (using drop_first=True) to avoid redundancy (for example, if we know a passenger is not male, they must be female, so one of the two is redundant). In practice, dropping the first dummy is often done to avoid multicollinearity issues in linear models, but for tree-based models it's not necessary. We'll keep all dummies here for clarity.\n",
        "\n",
        "We can concatenate these new dummy columns back to our dataframe (or directly integrate this step in a pipeline)."
      ],
      "metadata": {
        "id": "Uv-agJfLkAWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ordinal encode the 'class' column (First, Second, Third)\n",
        "class_mapping = {\"Third\": 1, \"Second\": 2, \"First\": 3}\n",
        "df['class_encoded'] = df['class'].map(class_mapping)\n",
        "print(\"Mapping 'class' -> numeric:\", class_mapping)\n",
        "df[['class', 'class_encoded']].head(5)\n"
      ],
      "metadata": {
        "id": "16mY1BNwH6yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the original sex and embarked columns are still present. We might drop them after encoding if we are going to use the dummy columns instead, to avoid duplication. Let's do that cleanup:"
      ],
      "metadata": {
        "id": "PvkyDUnTkJt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove original categorical columns after encoding\n",
        "df = df.drop(columns=['sex', 'embarked'])\n",
        "df.head(5)\n"
      ],
      "metadata": {
        "id": "-LUT5OQPkLjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a new column class_encoded where Third=1, Second=2, First=3. This numeric representation implies an order (higher is better class). We still keep the original class string for reference; in modeling we would typically use the numeric version. Important: If using ordinal encoding, ensure that the order you impose makes sense for the problem. If not, it's safer to one-hot encode even ordinal features because a model can learn an order if it exists, but if you impose a wrong order you might mislead the model.\n",
        "\n",
        "**Scikit-Learn approach:**\n",
        "We could also use sklearn.preprocessing.OneHotEncoder or OrdinalEncoder for these tasks, which is beneficial when building pipelines. For brevity, we won't show those here, but they offer more control (like handling unknown categories in test data). At this point, our dataset's categorical features are encoded into numeric form, which means we can feed them into models. Next, let's look at transforming continuous features through binning and scaling."
      ],
      "metadata": {
        "id": "_YFWLsDTkOv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3  Binning & Discretization  \n",
        "\n",
        "Why bin? Robust to outliers, capture step-wise effects, simplify models.\n",
        "\n",
        "* **Domain bins** – age groups  \n",
        "* **Quantile bins** – fare quartiles  \n",
        "* **KBinsDiscretizer** – automated (uniform / quantile)\n",
        "\n",
        "Binning (discretization) is the process of converting a continuous feature into multiple bins or ranges, effectively turning it into a categorical or ordinal feature. This can be useful for several reasons:\n",
        "\n",
        "- It can make the model more robust to outliers (since outlier values get put into a bin with a range).\n",
        "- It can capture non-linear relationships by grouping values. For example, perhaps age has a non-linear effect on an outcome: very young and very old might behave similarly (as groups) compared to middle-aged. Binning can sometimes capture this pattern.\n",
        "- It reduces the granularity of data, which can help some models (and also reduce overfitting in high-noise data).\n",
        "\n",
        "However, binning also loses information (exact values are coarsened), so it should be used judiciously. It is more common in some contexts (like scoring systems) or when using certain algorithms that prefer categorical inputs (like some rule-based models).\n",
        "\n",
        "**Techniques for binning:**\n",
        "- Fixed-width binning: Divide the range of the feature into equal-width intervals. For example, ages 0-10, 10-20, 20-30, ... etc.\n",
        "- Quantile binning (equal-frequency): Divide the data such that each bin has (approximately) equal number of observations. For example, quartiles (4 bins each containing 25% of data).\n",
        "- Domain-specific binning: Define bins based on domain knowledge (e.g., age: infant, child, teen, adult, senior).\n",
        "- Clustering-based binning: Using methods like k-means to find clusters and then assign bins (less common).\n",
        "\n",
        "Let's apply binning to the age feature in the Titanic data as an example. We'll create an AgeGroup feature:\n",
        "- Children: 0-12 years\n",
        "- Teenagers: 13-19 years\n",
        "- Adults: 20-59 years\n",
        "- Seniors: 60+ years\n",
        "\n",
        "This is a domain-driven choice of bins.\n"
      ],
      "metadata": {
        "id": "NNyt4bBtHiz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define age bins and labels\n",
        "bins = [0, 12, 19, 59, np.inf]  # np.inf for any age above 59\n",
        "labels = ['Child', 'Teenager', 'Adult', 'Senior']\n",
        "df['AgeGroup'] = pd.cut(df['age'], bins=bins, labels=labels)\n",
        "df[['age', 'AgeGroup']].head(10)\n"
      ],
      "metadata": {
        "id": "WeKvLyXAHi7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used pd.cut to bin ages into the specified intervals. The new column AgeGroup is categorical with the labels we provided. We can inspect the distribution of these groups:"
      ],
      "metadata": {
        "id": "XSikiotxk9NC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['AgeGroup'].value_counts()\n"
      ],
      "metadata": {
        "id": "Hj0XECdnk_ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows how many passengers fall into each age group. Now, AgeGroup could be one-hot encoded or ordinal-encoded (there is an implied order: Child < Teenager < Adult < Senior) depending on how we want to use it. For some models like decision trees, you could even keep it as a categorical type directly.\n",
        "\n",
        "**Equal-frequency binning example:**\n",
        "Maybe we want exactly 4 bins each containing 25% of the observations (quartiles). We can use pd.qcut for quantile-based cuts:"
      ],
      "metadata": {
        "id": "bh1Jjh3OlBge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantile-based binning of Fare into 4 buckets (quartiles)\n",
        "df['Fare_bin'] = pd.qcut(df['fare'], q=4, labels=['Q1','Q2','Q3','Q4'])\n",
        "print(df[['fare', 'Fare_bin']].head(10))\n",
        "print(\"\\nFare_bin distribution:\\n\", df['Fare_bin'].value_counts())\n"
      ],
      "metadata": {
        "id": "oN1dvkE0IAWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output shows each fare assigned a quartile label Q1-Q4. Each bin has (roughly) equal count of observations. This can be useful if the distribution of fare is highly skewed (which it is—most people paid low fare, a few paid very high fares). Binning can spread these out.\n",
        "\n",
        "**KBinsDiscretizer (sklearn):**\n",
        "Scikit-learn offers KBinsDiscretizer which can automate binning as part of a pipeline, with options for uniform or quantile strategy, and output as one-hot or ordinal. Here's a quick demonstration using KBinsDiscretizer to bin the age into 3 bins of equal width:"
      ],
      "metadata": {
        "id": "fQyukAEIlH52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "X_age = df[['age']]  # need 2D array\n",
        "kbins = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
        "age_binned = kbins.fit_transform(X_age)\n",
        "print(\"Age -> Binned (uniform width, 3 bins):\\n\", age_binned[:10].ravel())\n"
      ],
      "metadata": {
        "id": "ckL55YqlICrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above will divide the age range into 3 equal-width bins and give ordinal labels 0,1,2 for those bins. The output array shows the bin index for first 10 ages. (Note: KBinsDiscretizer might warn about the subsample vs full data – that's fine for our small demonstration.)\n",
        "\n",
        "In summary, binning is a way to simplify a continuous feature. It can be powerful when used appropriately (especially with certain models or when you suspect non-linear step changes in effect), but it also can throw away information, so consider it based on your analysis."
      ],
      "metadata": {
        "id": "L7Gq8pUKlO4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4  Feature Scaling & Transformation  \n",
        "\n",
        "Models like k-NN, SVM, neural nets need comparable scales.\n",
        "\n",
        "* **StandardScaler** – mean 0, std 1  \n",
        "* **MinMaxScaler** – 0 → 1  \n",
        "* **Log / Box-Cox** – fix skew\n",
        "\n",
        "Feature scaling refers to methods used to normalize the range or distribution of features. Many machine learning algorithms perform better or converge faster during training when features are on similar scales:\n",
        "Distance-based models (k-NN, K-Means, SVM with RBF kernel, etc.) are sensitive to the scale of features because they rely on distances.\n",
        "Gradient descent based models (linear regression, logistic regression, neural networks) converge faster when features are scaled, to avoid some features dominating the gradient due to larger scale.\n",
        "Regularized models (like Lasso or Ridge regression) impose penalties that assume features are centered or scaled similarly.\n",
        "Tree-based models (decision trees, random forests, XGBoost) generally do not require feature scaling, as they split based on thresholds and are scale-invariant for splits. But scaling does not hurt them either.\n",
        "\n",
        "\n",
        "We’ll scale `age`, `fare`, `sibsp`, `parch`, and add a log-fare.\n"
      ],
      "metadata": {
        "id": "NMhwhvRYHjBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Take a subset of numeric features for demonstration\n",
        "numeric_features = ['age', 'fare', 'sibsp', 'parch']  # sibsp = #siblings/spouses, parch = #parents/children\n",
        "subset = df[numeric_features].copy()\n",
        "print(\"Original scales:\")\n",
        "print(subset.describe().loc[['min','max','mean','std']].T)  # summary before scaling\n",
        "\n",
        "# Standardization (Z-score)\n",
        "scaler = StandardScaler()\n",
        "subset_std = pd.DataFrame(scaler.fit_transform(subset), columns=numeric_features)\n",
        "\n",
        "# Min-Max Scaling\n",
        "minmax = MinMaxScaler()\n",
        "subset_mm = pd.DataFrame(minmax.fit_transform(subset), columns=numeric_features)\n",
        "\n",
        "print(\"\\nAfter Standardization (mean ~0, std ~1):\")\n",
        "print(subset_std.describe().loc[['min','max','mean','std']].T)\n",
        "\n",
        "print(\"\\nAfter Min-Max Scaling (range 0 to 1):\")\n",
        "print(subset_mm.describe().loc[['min','max','mean','std']].T)\n"
      ],
      "metadata": {
        "id": "A5BCrwEhHjIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the output:\n",
        "- After standardization, each feature has mean very close to 0 and standard deviation 1. The min and max are not bounded to [0,1], but typically within a few multiples of the std.\n",
        "0 After min-max scaling, each feature's min is exactly 0 and max is 1, by construction. The mean and std will vary.\n",
        "\n",
        "We can also directly compare a few rows before and after scaling to see how individual values change:"
      ],
      "metadata": {
        "id": "nzNAT1paldeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First 5 rows - original age and fare:\")\n",
        "print(subset[['age','fare']].head())\n",
        "\n",
        "print(\"\\nFirst 5 rows - after StandardScaler:\")\n",
        "print(subset_std[['age','fare']].head())\n",
        "\n",
        "print(\"\\nFirst 5 rows - after MinMaxScaler:\")\n",
        "print(subset_mm[['age','fare']].head())\n"
      ],
      "metadata": {
        "id": "qpras9FTlgKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how an age of 22 years (for example) becomes around -0.6 after standardization (meaning 0.6 std below the mean age), and a fare of 7.25 becomes ~0.014 after min-max (very low in the 0-1 scale since 7.25 is near the minimum fare). Log transform example:\n",
        "\n",
        "The fare distribution is highly skewed (a few very high fares). We can apply a log10 transform to compress the high end:"
      ],
      "metadata": {
        "id": "c9pTiUKNlirB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['fare_log10'] = np.log10(df['fare'] + 1e-5)  # adding a tiny constant to avoid log(0)\n",
        "print(\"Fare vs log10(Fare) for first 5 entries:\")\n",
        "print(df[['fare','fare_log10']].head())\n",
        "\n",
        "print(\"\\nFare distribution stats:\\n\", df['fare'].describe())\n",
        "print(\"\\nlog10(Fare) distribution stats:\\n\", df['fare_log10'].describe())\n"
      ],
      "metadata": {
        "id": "M8H69niMlklI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After log transform, the distribution of fare should be less skewed (the difference between min and max in log scale is much smaller). We added a small constant to avoid taking log of 0 for any zero fares. When to scale:\n",
        "\n",
        "- Before using algorithms like k-NN, SVM, neural networks, or any model that uses gradient descent or distance, it's generally a good idea to scale features.\n",
        "\n",
        "- Tree-based models (Decision Tree, Random Forest, Gradient Boosting) typically don't need scaling.\n",
        "\n",
        "- Always apply the same scaling to training and test data (fit on train, apply to test) to avoid data leakage.\n",
        "\n",
        "\n",
        "We have scaled some features for demonstration, but for the next sections we'll often focus on the raw or minimally processed values, as feature engineering steps can be demonstrated without scaling in each case. Just keep in mind scaling is an important step in a modeling pipeline."
      ],
      "metadata": {
        "id": "zbOuCQSVlmqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5  Feature Extraction\n",
        "\n",
        "Feature extraction is about deriving new features from existing data. We create new representations that might be more informative for the model. This can involve:\n",
        "- Breaking down a feature into components (e.g., date -> year, month, day).\n",
        "- Combining features (e.g., area = length * width).\n",
        "- Mathematical transformations (e.g., polynomial terms)\n",
        "- Aggregations or statistical summaries (especially in grouped data or time series).\n",
        "\n",
        "We'll explore a few common scenarios:\n",
        "\n",
        "### 5.1 Datetime Parts  \n",
        "Break timestamps into year, month, dow, hour, etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "RmFYi4p8HjPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a small DataFrame with some dates\n",
        "date_df = pd.DataFrame({\n",
        "    'purchase_date': pd.to_datetime([\n",
        "        \"2021-01-01 14:23:00\",\n",
        "        \"2021-07-15 09:00:00\",\n",
        "        \"2022-03-05 20:45:00\",\n",
        "        \"2022-03-06 12:00:00\",\n",
        "        \"2022-12-25 00:00:00\"\n",
        "    ])\n",
        "})\n",
        "# Extract various components\n",
        "date_df['year'] = date_df['purchase_date'].dt.year\n",
        "date_df['month'] = date_df['purchase_date'].dt.month\n",
        "date_df['day'] = date_df['purchase_date'].dt.day\n",
        "date_df['day_of_week'] = date_df['purchase_date'].dt.day_name()\n",
        "date_df['hour'] = date_df['purchase_date'].dt.hour\n",
        "date_df['weekofyear'] = date_df['purchase_date'].dt.isocalendar().week  # ISO week number\n",
        "date_df\n"
      ],
      "metadata": {
        "id": "1HbYwHQ-mDKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above, we extracted several features:\n",
        "- year: 2021 or 2022.\n",
        "- month: 1-12.\n",
        "- day_of_week: Monday, Tuesday, etc. (This is categorical, could be encoded as numbers 0-6 or one-hot).\n",
        "- hour: 0-23.\n",
        "- weekofyear: week number within the year (1-52).\n",
        "\n",
        "We could further extract a boolean like is_weekend by checking if day_of_week is Saturday/Sunday, or is_holiday if we have a list of holidays."
      ],
      "metadata": {
        "id": "GHNlGTatmFqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cyclical encoding:**\n",
        "One thing to note: some of these features are cyclical (after December comes January, after Sunday comes Monday, hours wrap around after 23 to 0). If using them in a linear model, it can be beneficial to encode such features using sine/cosine transforms to capture the cyclic nature (e.g., \\text{month}_\\sin = \\sin(2\\pi \\cdot \\text{month}/12), \\text{month}_\\cos = \\cos(2\\pi \\cdot \\text{month}/12)). This ensures December (12) and January (1) are considered close in the encoded space. For simplicity, we won't do that math here, but it's a useful trick for cyclical features.\n",
        "\n",
        "The Titanic dataset does not have an obvious datetime feature (like a travel date). But if it did, we could apply similar extraction. However, when we get to the time series section, we'll see more about dealing with time indices."
      ],
      "metadata": {
        "id": "wezjAo5bmPmQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 5.2 Polynomial Features  \n",
        "`PolynomialFeatures(degree=2)` adds squares & interactions.\n",
        "\n",
        "Polynomial feature generation is a systematic way of creating interaction and power terms from numeric features.  \n",
        "\n",
        "This allows models like linear regression to fit nonlinear relationships by considering these higher-order terms. However, polynomial expansion increases the number of features rapidly and can lead to overfitting if the degree is high or if features are many. Scikit-learn provides PolynomialFeatures to automate this. Let's demonstrate on a very small example to see what it does:"
      ],
      "metadata": {
        "id": "Vm-A0_eJmXxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Small example dataset (2 samples, 2 features)\n",
        "X_example = np.array([[2, 3],\n",
        "                      [3, 4]])\n",
        "print(\"Original X:\\n\", X_example)\n",
        "\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly.fit_transform(X_example)\n",
        "print(\"\\nPolynomial features (degree 2) of X:\\n\", X_poly)\n",
        "print(\"Output feature names:\", poly.get_feature_names_out())\n"
      ],
      "metadata": {
        "id": "BCE9Zh-tHjWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```markdown\n",
        "In the output, `X_poly` has 5 columns derived from the original 2:\n",
        "\n",
        "* The first two columns are the original x₁, x₂.\n",
        "* The third column is x₁².\n",
        "* The fourth column is x₁ × x₂.\n",
        "* The fifth column is x₂².\n",
        "\n",
        "The feature names confirm this: e.g., `x0` = x₁, `x1` = x₂, `x0^2` = x₁², `x0 x1` = x₁x₂, `x1^2` = x₂².\n",
        "\n",
        "We can apply this to a real dataset too. For instance, suppose in Titanic we want to add a polynomial feature for `age` and `fare` to allow a model to capture interactions between age and fare (though it's not obvious there is one, but for demonstration):\n",
        "```"
      ],
      "metadata": {
        "id": "naeSOQ9ompSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PolynomialFeatures demo -------------------------------\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "\n",
        "af_poly = poly.fit_transform(df[[\"age\",\"fare\"]].fillna(0))\n",
        "pd.DataFrame(af_poly, columns=poly.get_feature_names_out([\"age\",\"fare\"])).head()\n"
      ],
      "metadata": {
        "id": "26yVLkDkIKbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the DataFrame `age_fare_poly` contains: `age`, `fare`, `age^2`, `age fare` (interaction), `fare^2`. We could join these back to `df` if needed.\n",
        "\n",
        "However, one must consider if these new features make sense and add value. For example, fare² might not be very meaningful, but an interaction age × fare might capture that perhaps the effect of fare on survival could depend on age, etc. (This is hypothetical in this context.)\n",
        "\n",
        "Polynomial features are particularly useful when you have continuous features and you suspect non-linear relationships but want to use a linear model, or when you want to capture interactions between features explicitly.\n",
        "\n",
        "**Note:** Higher degree polynomials (3, 4, ...) will generate a *lot* of features and can overfit. Use with caution, and consider applying feature selection or regularization to such expansions."
      ],
      "metadata": {
        "id": "Q0EVLf2pm8N4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6  Interaction Features  \n",
        "\n",
        "Interaction features are products or combinations of two or more original features. They capture relationships that are not evident when considering features individually. We already saw that polynomial features can create interaction terms automatically (e.g., x₁ × x₂). But often, domain knowledge might suggest specific interactions to create:\n",
        "\n",
        "* Sums or differences: e.g., `family_size = sibsp + parch + 1` (number of family members traveling, +1 for self).\n",
        "* Products: e.g., if modeling area from width and height, area = width × height.\n",
        "* Ratios: e.g., `fare_per_person = fare / (family_size)` could be meaningful (cost per individual).\n",
        "* Combinations of categorical features: e.g., combining two categorical into one (like `sex_class = sex + \"_\" + pclass` to capture that perhaps being female in 1st class differs from female in 3rd class).\n",
        "\n",
        "Let's do a couple of these on the Titanic data:\n",
        "\n",
        "**Family size interaction:** Titanic has `sibsp` (siblings/spouses aboard) and `parch` (parents/children aboard). A known useful feature is `family_size = sibsp + parch + 1` (the +1 is to count the person themselves). This tells how large the traveling group/family was.\n",
        "\n",
        "\n",
        "**Summary:**\n",
        "Domain-driven combos often matter:\n",
        "\n",
        "* `family_size = sibsp + parch + 1`  \n",
        "* `is_alone` flag  \n",
        "* `fare_per_person` ratio  \n",
        "* Categorical combos like `sex_pclass`\n"
      ],
      "metadata": {
        "id": "sJRWQQzWHjc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['family_size'] = df['sibsp'] + df['parch'] + 1\n",
        "print(df[['sibsp','parch','family_size']].head(10))\n",
        "print(\"\\nFamily size distribution:\\n\", df['family_size'].value_counts())\n"
      ],
      "metadata": {
        "id": "xNqkXoQuHjj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a new feature family_size. We see common values might be 1 (alone), 2, 3, etc. This feature could be further used to create, say, a boolean is_alone = (family_size == 1) which Titanic analyses often use (since solo travelers had different survival odds than those with family)."
      ],
      "metadata": {
        "id": "-urjo_fAnfnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['is_alone'] = (df['family_size'] == 1).astype(int)\n",
        "print(\"\\n'is_alone' (1 if family_size==1) distribution:\\n\", df['is_alone'].value_counts())\n"
      ],
      "metadata": {
        "id": "TyhVlKcQIQAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qf4dShQFnl5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fare per person:\n",
        "We can create fare_per_person = fare / family_size. This might normalize the fare by how many people shared that fare (siblings/parents often paid together). It could differentiate those who paid a high fare but for many people versus someone who paid a high fare just for themselves in first class."
      ],
      "metadata": {
        "id": "4fqp26GznjZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Avoid division by zero (shouldn't happen since family_size min is 1)\n",
        "df['fare_per_person'] = df['fare'] / df['family_size']\n",
        "df[['fare', 'family_size', 'fare_per_person']].head(10)\n"
      ],
      "metadata": {
        "id": "vG7kVRshnqnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We created a ratio feature fare_per_person. If someone has family_size 1, this is just their fare; if family_size > 1, this reduces the value.\n",
        "\n",
        "**Categorical interaction example:**\n",
        "Combine sex and pclass into a single category feature. (Remember, we dropped sex from df after encoding, but let's get it from original again for this concept.)"
      ],
      "metadata": {
        "id": "H3tM9fkjntVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using original Titanic load to get sex and pclass quickly\n",
        "titanic_raw = sns.load_dataset('titanic')\n",
        "titanic_raw['sex_pclass'] = titanic_raw['sex'] + \"_\" + titanic_raw['pclass'].astype(str)\n",
        "print(titanic_raw[['sex','pclass','sex_pclass']].head(6))\n",
        "print(\"\\nUnique sex_pclass combos:\", titanic_raw['sex_pclass'].unique())\n"
      ],
      "metadata": {
        "id": "_shNRw6mnxyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This new feature sex_pclass has values like \"male_3\", \"female_1\", etc., representing a passenger's sex and class together. If one were to one-hot encode this, it effectively is capturing an interaction between sex and class. If the effect of class on survival is different for males vs females, this kind of feature might help a model pick that up more easily. In general, you create interaction features when you have reason to believe two features combined have an important effect that is not simply linear/additive. Many algorithms can capture interactions on their own (e.g., decision trees naturally do, and neural networks can), but for linear models or just to introduce specific hypothesis-driven signals, manually creating interactions can be very useful. Keep in mind that adding many interaction features increases dimensionality and risk of overfitting, so again, prefer those that make logical sense or test them with feature importance/selection methods."
      ],
      "metadata": {
        "id": "Grozb5ehn0o8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  7. Feature Selection Methods\n",
        "\n",
        "After creating many features, we often face the question: **which features are actually helpful?** Feature selection is the process of reducing the number of input features to those that are most useful to the model. This can help in:\n",
        "\n",
        "* Simplifying the model (making it faster and more interpretable).\n",
        "* Reducing overfitting by removing noisy or irrelevant features.\n",
        "* Avoiding the curse of dimensionality (which can hurt performance when we have too many features and not enough data).\n",
        "\n",
        "There are several approaches to feature selection:\n",
        "\n",
        "* **Filter methods:** Select features based on statistical properties *without* involving any specific model. Examples:\n",
        "   * Remove features with very low variance (near-constant features).\n",
        "   * Select features most correlated with the target (for regression) or most associated via chi-square or mutual information (for classification).\n",
        "   * Use statistical tests (ANOVA F-test, chi-squared test) between each feature and the target, and keep top N features.\n",
        "\n",
        "* **Wrapper methods:** Use a predictive model to score feature subsets and select the best combination. This includes techniques like:\n",
        "   * **Forward selection:** start with no feature, add features one by one that improve the model until no improvement.\n",
        "   * **Backward elimination:** start with all features, remove least useful one by one until performance drops.\n",
        "   * **Recursive Feature Elimination (RFE):** iteratively train the model and remove the weakest feature(s) until reaching desired number of features.\n",
        "\n",
        "* **Embedded methods:** Feature selection occurs as part of the model training. For example:\n",
        "   * **Lasso (L1 regularization)** tends to shrink irrelevant feature coefficients to zero, effectively selecting features.\n",
        "   * **Tree-based feature importance:** Decision trees or ensemble (Random Forest, XGBoost) naturally compute an importance score for features; one can select top features based on that.\n",
        "   * **Feature importances from any model:** Train a model and rank features by absolute coefficient or importance.\n",
        "\n",
        "Let's demonstrate a couple of these methods\n",
        "\n",
        "\n",
        "\n",
        "* **Filter** – SelectKBest(ANOVA)  \n",
        "* **Embedded** – tree importances  \n",
        "* **Wrapper** – RFE\n",
        "\n",
        "We’ll build a synthetic dataset and try each.\n"
      ],
      "metadata": {
        "id": "xq4wiWWsHjra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "# Create a synthetic classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=5, n_redundant=5,\n",
        "                           n_repeated=0, n_classes=2, random_state=42, shuffle=False)\n",
        "print(\"Shape of X:\", X.shape)\n"
      ],
      "metadata": {
        "id": "E1FXOK2UHjwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above, we created 20 features where:\n",
        "\n",
        "* 5 are informative (actually affect the target).\n",
        "* 5 are redundant (random combinations of the informative ones).\n",
        "* 10 are noise.\n",
        "\n",
        "We set `shuffle=False` so that the first 5 might be informative, next 5 redundant, etc., just for ease of understanding (in real, it might be mixed).\n",
        "\n",
        "**Filter method: SelectKBest (ANOVA F-test)**\n",
        "\n",
        "We'll use `SelectKBest` to pick the top 5 features by univariate F-test (which measures linear correlation with the target for continuous features or ANOVA for categorical target). This is a filter method done independently for each feature."
      ],
      "metadata": {
        "id": "MS_S15enoL22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "selector = SelectKBest(score_func=f_classif, k=5)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "selected_indices = selector.get_support(indices=True)\n",
        "print(\"Indices of selected features (0-based):\", selected_indices)\n",
        "print(\"ANOVA F-test scores for first 10 features:\\n\", selector.scores_[:10])\n"
      ],
      "metadata": {
        "id": "lYzR8PKWoNOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output will list which feature indices were selected. Ideally, we hope it picks most of the truly informative ones (which we suspect might be features 0-4 if our generation placed them first). The F-test scores for features give an idea of how strongly each feature is correlated with the class label.\n",
        "\n",
        "**Embedded method:** Feature importance from RandomForest\n",
        "We can train a RandomForest classifier on all features and then look at its feature_importances_. Random forest is generally robust and can indicate which features it found useful."
      ],
      "metadata": {
        "id": "JHL2r3l9oPYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "importances = rf.feature_importances_\n",
        "important_idx = np.argsort(importances)[::-1]  # indices sorted by importance, descending\n",
        "print(\"Feature importances (top 5):\")\n",
        "for idx in important_idx[:5]:\n",
        "    print(f\"Feature {idx}: Importance {importances[idx]:.4f}\")\n"
      ],
      "metadata": {
        "id": "ArVIg1oEoTJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This prints the top 5 features according to the random forest. We can compare if those indices overlap with our earlier selection. Wrapper method: Recursive Feature Elimination (RFE)\n",
        "\n",
        "We'll do a quick RFE with a logistic regression to select, say, 5 features. This will repeatedly eliminate the least important feature as determined by the model coefficients."
      ],
      "metadata": {
        "id": "_0a-E4z7oVqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "logreg = LogisticRegression(max_iter=1000, solver='liblinear')\n",
        "rfe = RFE(estimator=logreg, n_features_to_select=5)\n",
        "rfe.fit(X, y)\n",
        "print(\"Features selected by RFE (LogisticRegression):\", np.where(rfe.support_)[0])\n"
      ],
      "metadata": {
        "id": "iBV2iiFaoXm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RFE selects a subset of features (the positions where `rfe.support_` is True). LogisticRegression with L1 penalty could also be used to do selection by shrinking coefficients to zero, but that would require adjusting the regularization strength.\n",
        "\n",
        "Each method might select slightly different features, especially since some features were redundant combinations of others.\n",
        "\n",
        "**Which method to use?**\n",
        "\n",
        "* **Filter methods** are fast and don't overfit to a model, but they consider features individually, ignoring interactions.\n",
        "* **Wrapper methods** can consider feature combinations and typically yield better subsets, but they are computationally expensive (training many models) and can overfit if not careful (so use cross-validation).\n",
        "* **Embedded methods** are a good balance: using the model to guide selection with usually one training. Tree importances or Lasso are common choices.\n",
        "\n",
        "In practice, a combination is often used. For example, you might first filter out features with zero or near-zero variance, then use a tree model to get importances, and perhaps perform RFE on a smaller set.\n",
        "\n",
        "For our tutorial purposes, understanding that these tools exist and seeing a simple usage is key. In your workflow, after engineering features, you can use these methods to whittle down to a set that makes your model perform best (using validation to ensure you're not just overfitting to training with selection).\n"
      ],
      "metadata": {
        "id": "9DFqxV9pogYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8  Time-Series Feature Engineering  \n",
        "\n",
        "Time series data brings additional challenges and opportunities for feature engineering. A time series is data recorded over time (usually at regular intervals like daily, monthly, etc.). Besides the kinds of features we can extract from each timestamp (as we saw in datetime extraction), there are unique features derived from the sequence of values itself.\n",
        "\n",
        "When preparing time series data for machine learning (especially for forecasting or supervised learning approaches), common techniques include:\n",
        "\n",
        "* **Lag features:** Using previous time steps' values as features for the current time. For example, if you want to predict today's value, you might use yesterday's value (lag 1) or last week's value (lag 7 for daily data) as features.\n",
        "* **Rolling statistics (window features):** Features like moving average, moving standard deviation, or moving sum over a window of past observations. These capture trends or volatility over time.\n",
        "* **Differences:** The change from previous period (first difference) or percentage change. Differencing can help stationarize a series (remove trend).\n",
        "* **Datetime components:** As discussed, features like day of week, month, hour, etc., which for time series can capture seasonality or periodic effects.\n",
        "* **Fourier or seasonal decomposition features:** (Intermediate/Advanced) Representing seasonal patterns via sine/cosine features or using seasonal decomposition to extract trend/season components.\n",
        "\n",
        "Let's create a simple time series and demonstrate creating some of these features. We'll simulate a monthly time series with a seasonal pattern.\n",
        "\n",
        "Key ideas: **lags, rolling stats, diffs, seasonal parts**.\n"
      ],
      "metadata": {
        "id": "MUqPPQ34Hj2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate a time series (monthly data for 3 years)\n",
        "date_index = pd.date_range(start='2019-01-01', periods=36, freq='M')\n",
        "time = np.arange(36)\n",
        "# Create a pattern: trend + seasonal + noise\n",
        "np.random.seed(0)\n",
        "values = 10 + 0.5*time + 5*np.sin(2*np.pi*time/12) + np.random.normal(0, 2, size=36)\n",
        "ts_df = pd.DataFrame({'Date': date_index, 'Value': values})\n",
        "ts_df = ts_df.set_index('Date')\n",
        "ts_df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "OCXhuqpVHj9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have ts_df with a Date index (monthly from Jan 2019 to Dec 2021) and a Value (some synthetic measurement). This series has a linear upward trend (0.5 per month) plus a yearly seasonality (sinusoidal) plus some noise.\n",
        "\n",
        "**Lag Features:**\n",
        "Let's create a new column for a 1-month lag. This means the value of the previous month as a feature to predict current month. We'll use shift(1) for that. We can similarly create a 12-month lag (for yearly seasonality, though with only 3 years data maybe not needed here)."
      ],
      "metadata": {
        "id": "oLduRcpcouqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ts_df['Value_lag1'] = ts_df['Value'].shift(1)\n",
        "ts_df['Value_lag12'] = ts_df['Value'].shift(12)\n",
        "ts_df[['Value', 'Value_lag1', 'Value_lag12']].head(15)\n"
      ],
      "metadata": {
        "id": "7X2fXqLgozD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice for January 2019, Value_lag1 is NaN (since there's no December 2018 in data), and for Jan 2020, Value_lag12 is the value from Jan 2019, etc. These NaNs at the start of the series for lags are expected; we can decide to drop them or fill them (often drop if predicting, since we can't have a feature when it didn't exist).\n",
        "\n",
        "**Rolling Window Features:**\n",
        "\n",
        "Let's compute a 3-month rolling mean and a 3-month rolling standard deviation of the value. We use .rolling(window=3):"
      ],
      "metadata": {
        "id": "rWAcUavgo0kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ts_df['rolling_mean_3'] = ts_df['Value'].rolling(window=3).mean()\n",
        "ts_df['rolling_std_3'] = ts_df['Value'].rolling(window=3).std()\n",
        "ts_df[['Value', 'rolling_mean_3', 'rolling_std_3']].head(10)\n"
      ],
      "metadata": {
        "id": "kfYCwTL3o5D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the first two months, the 3-month window isn't full, so the default behavior gives NaN (since it requires 3 points to compute). We could specify min_periods=1 in rolling to have it start earlier, but typically for modeling we'd start using those features only once windows are full, or we fill initial ones in some way if needed.\n",
        "\n",
        "The rolling mean smooths the series, capturing local trend, and rolling std captures the local volatility.\n",
        "\n",
        "**Differences:**\n",
        "We can add a feature that is the difference between the current value and previous month (lag1 difference). This can highlight short-term changes."
      ],
      "metadata": {
        "id": "Gs0EvQGeo7aV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ts_df['Diff_1'] = ts_df['Value'] - ts_df['Value_lag1']\n",
        "ts_df[['Value', 'Diff_1']].head(5)\n"
      ],
      "metadata": {
        "id": "vpmEGPa8pAJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or simply ts_df['Diff_1'] = ts_df['Value'].diff(1) which does the same. A positive difference means an increase from last month.\n",
        "\n",
        "**Datetime components for time series:**\n",
        "\n",
        "We already have Date index. We might extract month and year as features as well (though in a purely time series forecasting setting, one might incorporate them differently or use seasonal dummies). For demonstration:"
      ],
      "metadata": {
        "id": "w9NHqNh0pCIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ts_df['Month'] = ts_df.index.month\n",
        "ts_df['Year'] = ts_df.index.year\n",
        "ts_df[['Value','Month','Year']].head(12)\n"
      ],
      "metadata": {
        "id": "M82p1MNxpKP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Month` will be 1-12 corresponding to Jan-Dec. As mentioned earlier, we could one-hot encode month or use sin/cos encoding to account for the cyclic nature of month in a year.\n",
        "\n",
        "By now, our `ts_df` contains the original series and several new features derived from it:\n",
        "\n",
        "* Value_lag1 (previous month value)\n",
        "* Value_lag12 (value a year ago, capturing annual seasonality)\n",
        "* rolling_mean_3, rolling_std_3 (short-term trend and volatility)\n",
        "* Diff_1 (monthly change)\n",
        "* Month, Year (time index info)\n",
        "\n",
        "Let's see the tail of the DataFrame to observe these:"
      ],
      "metadata": {
        "id": "Cg8c8eq9pPac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ts_df.tail(5)\n"
      ],
      "metadata": {
        "id": "j87Tk6s9pQf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will see the values for late 2021, and the features computed. Note how `Value_lag12` for e.g. Dec 2021 equals the value from Dec 2020, etc.\n",
        "\n",
        "These features can be fed into a regression model to predict future values. For instance, to predict Value of Feb 2022, you'd use Jan 2022 Value (lag1) and Feb 2021 Value (lag12), etc. Proper setup would ensure training on known history and predicting forward.\n",
        "\n",
        "**Caution:** When creating lag/rolling features, be mindful of the context:\n",
        "\n",
        "* If doing forecasting, ensure you only use past data to predict future (avoid lookahead bias).\n",
        "* If you split train/test by time, you would compute these features on the training set and ensure they are available for test (sometimes needing to carry the last known values forward).\n",
        "* Rolling features near the edges have missing data; usually we drop the first few records or fill them in a minimal way if needed.\n",
        "\n",
        "For a concrete example: If we were predicting monthly values, we might drop the first 12 months after creating a 12-month lag, since we can't use those for modeling (no lag12 available for them).\n",
        "\n",
        "Time series feature engineering is a big topic, but these basic techniques (lags, rollings, time-based features) are the core. Depending on the domain, you might add specific ones (e.g., if forecasting sales, you might include features like \"7-day moving average of sales\", or \"days since last promotion event\", etc., which are domain-specific signals)."
      ],
      "metadata": {
        "id": "RTPbV3hXpRn0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9  Automated Feature Engineering  \n",
        "\n",
        "Building features manually as we did above is powerful because you can incorporate domain knowledge and intuition. However, it can be time-consuming and you might miss important patterns. **Automated feature engineering tools** aim to generate many potential features automatically, which you can then use or filter.\n",
        "\n",
        "We will introduce two popular Python libraries:\n",
        "\n",
        "* **tsfresh:** stands for *Time Series Feature Extraction based on Randomized Testing*. It automatically calculates a huge number of time series characteristics (mean, median, Fourier coefficients, autocorrelation, etc.) for time series data.\n",
        "* **Featuretools:** a general library for automated feature engineering, particularly useful for relational (multi-table) data or transactional data, using a method called *Deep Feature Synthesis*. It can create features like aggregations and transformations across tables.\n",
        "\n",
        "These tools can save a lot of effort, though they may produce more features than you need, so typically you'd pair them with feature selection.\n",
        "\n",
        "## 9.1 tsfresh (Automatic Time Series Features)\n",
        "\n",
        "If you have time series data (especially multiple time series or a panel of time series grouped by an ID), `tsfresh` can compute a wide array of features for each.\n",
        "\n",
        "**Scenario:** Suppose we have multiple sensors each producing a time series, and we want summary features of each sensor's series to feed into a classifier. tsfresh would create features for each sensor series such as mean, standard deviation, max, min, number of peaks, etc.\n",
        "\n",
        "Let's do a simple example. We will create a small dataset of two time series (as if from two entities). For simplicity, we'll use our earlier synthetic series and duplicate it with some variation for a second series.\n"
      ],
      "metadata": {
        "id": "vSi2qAAPHkDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tsfresh.feature_extraction import extract_features\n",
        "\n",
        "# Prepare a small dataset for tsfresh\n",
        "# Two IDs, each with a time series of 'Value'\n",
        "ts_df = ts_df.reset_index()  # make Date a column for ease\n",
        "# Create a second series as a variation (e.g., a phase shift or different noise)\n",
        "ts_df2 = ts_df.copy()\n",
        "ts_df2['Value'] = ts_df2['Value'] * 0.8 + np.random.normal(0, 2, len(ts_df2))  # slightly different series\n",
        "ts_df2['id'] = 2\n",
        "ts_df['id'] = 1\n",
        "combined_ts = pd.concat([ts_df[['id','Date','Value']], ts_df2[['id','Date','Value']]])\n",
        "combined_ts = combined_ts.rename(columns={\"Date\":\"time\"})  # rename to 'time' as expected by tsfresh\n",
        "combined_ts.head()\n"
      ],
      "metadata": {
        "id": "wMTxypxgHkJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have `combined_ts` DataFrame with columns: id, time, Value. It contains two groups of time series (id=1 and id=2), each with the same timestamps for simplicity but different values.\n",
        "\n",
        "Now we use `tsfresh.extract_features`. We'll specify the identifiers and time column. By default, tsfresh will compute a comprehensive list of features. For the sake of brevity, we'll limit to a smaller set of features using the `default_fc_parameters` argument. Let's extract a few basic features like mean, standard deviation, minimum, and maximum of the Value series for each id."
      ],
      "metadata": {
        "id": "9SXhS4zNpo3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tsfresh.feature_extraction import MinimalFCParameters\n",
        "\n",
        "# Use a minimal set of feature calculators for demonstration (to avoid huge output)\n",
        "fc_parameters = {\n",
        "    'mean': None,\n",
        "    'standard_deviation': None,\n",
        "    'minimum': None,\n",
        "    'maximum': None,\n",
        "    # (tsfresh has many more like median, skewness, etc., but we'll limit here)\n",
        "}\n",
        "features_df = extract_features(combined_ts, column_id='id', column_sort='time',\n",
        "                               default_fc_parameters=fc_parameters)\n",
        "features_df\n"
      ],
      "metadata": {
        "id": "fKDxNh5rIaMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting `features_df` has one row per `id` (so 2 rows, for id 1 and 2) and columns representing the features we extracted:\n",
        "\n",
        "* `Value__mean`\n",
        "* `Value__standard_deviation`\n",
        "* `Value__minimum`\n",
        "* `Value__maximum`\n",
        "\n",
        "The values in those columns are the respective statistics of the Value time series for each id.\n",
        "\n",
        "In our small example:\n",
        "\n",
        "* id 1 might have mean around ~ (the middle of the sinusoidal trend).\n",
        "* id 2 being a scaled noisy version will have a different mean, etc.\n",
        "\n",
        "If we had not limited features, by default tsfresh's Comprehensive set can generate dozens or even hundreds of features (like quantile values, Fourier transform features, time reversal symmetry, and many specialized metrics). You can then use `tsfresh.select_features` (which statistically tests which features correlate with a target, if provided) or simply feed them into a model and let it figure out importance.\n",
        "\n",
        "**When to use tsfresh:**\n",
        "\n",
        "* If you have time series and want to quickly get a broad set of characteristics.\n",
        "* Feature extraction for time-series classification problems (e.g., classify a signal as failure vs normal based on sensor readings).\n",
        "* Note that tsfresh can be computationally heavy if your series or number of features is large, so you might use the `MinimalFCParameters` or otherwise restrict features to manage performance.\n",
        "\n",
        "## 9.2 Featuretools (Deep Feature Synthesis)\n",
        "\n",
        "Featuretools is a library for automated feature engineering, especially useful when you have **relational data** (multiple tables) or hierarchical data (like customers, each with multiple transactions, etc.). It can automatically generate features by aggregating and combining information from these tables.\n",
        "\n",
        "For example, imagine a retail dataset:\n",
        "\n",
        "* A customers table (one row per customer).\n",
        "* An transactions table (multiple transactions per customer).\n",
        "\n",
        "Featuretools can automatically create features like \"total spend per customer\", \"number of transactions in last 30 days\", \"average transaction amount for customer\", etc., by aggregating the transactions table, as well as handle temporal relationships (like at time of prediction, only use past transactions).\n",
        "\n",
        "Let's use Featuretools' built-in demo dataset to illustrate. The mock dataset includes customers, their sessions, and transactions."
      ],
      "metadata": {
        "id": "DQbV6D-gpyNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import featuretools as ft\n",
        "\n",
        "# Load a demo dataset of customers\n",
        "data = ft.demo.load_mock_customer()\n",
        "customers_df = data[\"customers\"]\n",
        "sessions_df = data[\"sessions\"]\n",
        "transactions_df = data[\"transactions\"]\n",
        "\n",
        "print(\"Customers table columns:\", customers_df.columns.tolist())\n",
        "print(\"Sessions table columns:\", sessions_df.columns.tolist())\n",
        "print(\"Transactions table columns:\", transactions_df.columns.tolist())\n",
        "print(\"Number of customers:\", len(customers_df))\n",
        "print(\"Sample transactions:\\n\", transactions_df.head(5))\n"
      ],
      "metadata": {
        "id": "aT_AoBnCHmbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have:\n",
        "- customers_df: with customer info (customer_id, zip_code, join_date, birthday).\n",
        "- sessions_df: with session info (session_id, customer_id, device, session_start).\n",
        "- transactions_df: (transaction_id, session_id, transaction_time, product_id, amount).\n",
        "\n",
        "\n",
        "We need to tell Featuretools how these tables are related (customer -> sessions -> transactions) and then let it generate features for a target table (say we want features at the customer level)."
      ],
      "metadata": {
        "id": "Pgv1nuEWp1GD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA1ia8bLHgdw"
      },
      "outputs": [],
      "source": [
        "# Create an EntitySet and add the dataframes\n",
        "es = ft.EntitySet(id=\"customer_data\")\n",
        "es = es.add_dataframe(dataframe_name=\"customers\", dataframe=customers_df, index=\"customer_id\")\n",
        "es = es.add_dataframe(dataframe_name=\"sessions\", dataframe=sessions_df, index=\"session_id\", time_index=\"session_start\")\n",
        "es = es.add_dataframe(dataframe_name=\"transactions\", dataframe=transactions_df, index=\"transaction_id\", time_index=\"transaction_time\")\n",
        "\n",
        "# Define relationships\n",
        "es = es.add_relationship(parent_dataframe_name=\"customers\", parent_column_name=\"customer_id\",\n",
        "                         child_dataframe_name=\"sessions\", child_column_name=\"customer_id\")\n",
        "es = es.add_relationship(parent_dataframe_name=\"sessions\", parent_column_name=\"session_id\",\n",
        "                         child_dataframe_name=\"transactions\", child_column_name=\"session_id\")\n",
        "es\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set up an EntitySet (a structure in Featuretools that holds all tables and their relationships). We specified which column is the index for each table, and which are time indices for temporal data. Then we defined relationships:\n",
        "- customers -> sessions via customer_id\n",
        "- sessions -> transactions via session_id\n",
        "\n",
        "Now we ask Featuretools to do Deep Feature Synthesis (DFS): generate features for the customers table by aggregating or transforming data from related tables."
      ],
      "metadata": {
        "id": "9nKE4lJPp7p8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_matrix, feature_defs = ft.dfs(entityset=es, target_dataframe_name=\"customers\", max_depth=2)\n",
        "print(\"Generated feature matrix shape:\", feature_matrix.shape)\n",
        "feature_matrix.head(5)\n"
      ],
      "metadata": {
        "id": "dfAK_ohXp71r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The feature_matrix is a new dataframe where each row is a customer (since target_dataframe_name was \"customers\") and columns are newly generated features. The max_depth=2 parameter means it can stack at most 2 transformations/aggregations. If you inspect feature_matrix.head(), you'll see columns like:\n",
        "\n",
        "* AGE(customer) or something derived from birthday perhaps (maybe it calculates age from birthday and a reference).\n",
        "* Aggregations from sessions and transactions, like sessions.Count (number of sessions per customer), transactions.Sum(amount) grouped by customer (total spending), transactions.MEAN(amount) (average spend), maybe transactions.MAX(product_id) or other combinations.\n",
        "* Possibly features like \"number of unique product_id per customer\", etc.\n",
        "\n",
        "The shape printed tells how many features were created. Featuretools might create quite a few features depending on the data. For example, typical features could be:\n",
        "\n",
        "* transactions.NUM_UNIQUE(product_id) by customer\n",
        "* sessions.COUNT by customer (how many sessions)\n",
        "* sessions.MONTH(join_date) (maybe extracting month of join date)\n",
        "* transactions.SUM(amount) by sessions and then MAX by customer (like largest session spend)\n",
        "* etc.\n",
        "\n",
        "We won't list them all here, but you get the idea. Featuretools systematically enumerates combinations: it looks at each relationship and applies aggregations (sum, mean, count, etc.) and also applies transformations to columns (like extracting date parts or doing arithmetic).\n",
        "\n",
        "**Using Featuretools output:**\n",
        "\n",
        "The generated features can be used to train a model. You might want to drop some that make no sense or have high cardinality. Featuretools tries to handle a lot automatically, but not every generated feature will be useful. It's often followed by manual review or feature selection.\n",
        "\n",
        "**When to use Featuretools:**\n",
        "\n",
        "* You have complex data with multiple tables (like users → logs → transactions) and want to automatically generate features capturing relations.\n",
        "* You want to try a broad set of derived features (especially aggregations over one-to-many relationships) quickly.\n",
        "* It excels in scenarios like predicting customer churn or credit risk from transactional history, etc., where you have to aggregate over a user's history.\n",
        "\n",
        "Keep in mind that Featuretools (and automated feature generation in general) can produce a huge number of features. It's powerful, but be prepared to use feature selection or regularization to handle the output.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "In this notebook, we've covered a comprehensive range of feature engineering techniques for both tabular and time series data:\n",
        "\n",
        "* How to handle missing values by dropping or imputing, and the importance of doing so.\n",
        "* Encoding categorical features with appropriate techniques (one-hot for nominal, ordinal encoding for ordered categories).\n",
        "* Binning continuous features into categories to capture non-linear effects or reduce noise.\n",
        "* Scaling and normalizing features to ensure comparability and to meet model assumptions.\n",
        "* Extracting new features from existing ones, such as breaking down dates or forming polynomial and interaction features to enrich the feature set.\n",
        "* Selecting the most important features using filter, wrapper, and embedded methods, to simplify models and avoid overfitting.\n",
        "* Engineering time series features like lags and rolling statistics to incorporate temporal patterns into models.\n",
        "* Utilizing automated feature engineering libraries (tsfresh and Featuretools) to let algorithms discover potentially useful features across time series and relational data.\n",
        "\n",
        "Feature engineering is as much an art as it is science. It requires understanding both the data and the modeling algorithms. Always keep in mind:\n",
        "\n",
        "* **Garbage in, garbage out:** No model can make up for completely irrelevant or bad features. Invest time in understanding the data.\n",
        "* **Domain knowledge:** Use it to create meaningful features that a generic algorithm might not think of.\n",
        "* **Experimentation:** Try different transformations and combinations, and evaluate via model performance (using validation sets or cross-validation) to see what helps.\n",
        "* **Simplicity:** More features isn't always better. The right features are better. Use feature selection or judgement to keep the feature set concise and interpretable when possible."
      ],
      "metadata": {
        "id": "Z9Ugu1Mtp7-i"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K1Rpd63Sp8HO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}