{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO7g/fF6AL+QwvNyPmNdFdj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IyadSultan/AI_pediatric_oncology/blob/main/8-%20DSPy_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's your tutorial introduction rewritten and beautifully formatted in **Markdown**:\n",
        "\n",
        "---\n",
        "\n",
        "# 📘 Tutorial: Using DSPy in Google Colab for Declarative Self-Improving Language Models\n",
        "\n",
        "### 🚀 Introduction\n",
        "\n",
        "**DSPy** (Declarative Self-Improving Python) is an open-source framework developed by **Stanford** that enables you to build **modular AI pipelines** and **automatically optimize them**—eliminating the need for brittle, hand-crafted prompts.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### ❗ The Problem with Traditional Prompt Engineering\n",
        "\n",
        "In traditional prompt engineering:\n",
        "\n",
        "* Small changes to the **input data** or **underlying model** can break your carefully tuned prompt.\n",
        "* This leads to **repeated trial-and-error**, consuming time and reducing reliability.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ The DSPy Solution\n",
        "\n",
        "DSPy introduces a **shift from prompting to programming**:\n",
        "\n",
        "* You define **what** the model should do using **declarative code**—including its inputs, expected outputs, and the objective.\n",
        "* DSPy’s **compiler** automatically:\n",
        "\n",
        "  * Refines the **prompt wording**\n",
        "  * Tunes **model weights** (if needed)\n",
        "  * Adjusts the pipeline for better performance\n",
        "\n",
        "This results in AI applications that are:\n",
        "\n",
        "* 🔧 More **robust**\n",
        "* 🧠 Able to **self-improve** using feedback and data\n",
        "* 🧹 Easier to **maintain** over time\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 Who Is This For?\n",
        "\n",
        "Whether you are:\n",
        "\n",
        "* 👩‍🔬 An **academic** experimenting with complex reasoning chains\n",
        "* 👨‍💻 A **developer** deploying a production NLP system\n",
        "* 📚 A **learner** exploring prompt engineering and LLMs\n",
        "\n",
        "...DSPy offers a **systematic and scalable** approach to designing and improving language model solutions.\n",
        "\n",
        "---\n",
        "\n",
        "### 📚 What You’ll Learn in This Tutorial\n",
        "\n",
        "In this Google Colab-based tutorial, we’ll guide you through:\n",
        "\n",
        "* 🔧 Installing and setting up **DSPy**\n",
        "* 🧠 Understanding its **core concepts** (signatures, modules, optimizers)\n",
        "* 🧪 Building and testing **prompt pipelines**\n",
        "* 📈 Applying **optimizers** for automatic performance tuning\n",
        "* 🎯 Performing **fine-tuning**\n",
        "* 🔄 Integrating with different **LLM backends** (OpenAI, Hugging Face, Anthropic, etc.)\n",
        "\n",
        "---\n",
        "\n",
        "By the end, you'll be equipped to build **declarative, self-improving AI programs** using DSPy—and ready to apply them to your own projects.\n",
        "\n",
        "Let’s dive in! 🧑‍💻✨\n"
      ],
      "metadata": {
        "id": "b-J1kXaDXZn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Installation and Setup in Google Colab\n",
        "Using DSPy in Colab is straightforward. First, install the dspy-ai package from PyPI. In a Colab notebook, you can run a shell command with ! to install it:"
      ],
      "metadata": {
        "id": "x1H6uZlzXTYx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-b5Y5_RXD_q",
        "outputId": "facfe0d4-c7d9-4363-b2f7-f040661b5361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dspy-ai\n",
            "  Downloading dspy_ai-2.6.24-py3-none-any.whl.metadata (286 bytes)\n",
            "Collecting dspy>=2.6.5 (from dspy-ai)\n",
            "  Downloading dspy-2.6.24-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting backoff>=2.2 (from dspy>=2.6.5->dspy-ai)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: joblib~=1.3 in /usr/local/lib/python3.11/dist-packages (from dspy>=2.6.5->dspy-ai) (1.5.0)\n",
            "Requirement already satisfied: openai>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from dspy>=2.6.5->dspy-ai) (1.78.1)\n",
            "Requirement already satisfied: pandas>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from dspy>=2.6.5->dspy-ai) (2.2.2)\n",
            "Requirement already satisfied: regex>=2023.10.3 in /usr/local/lib/python3.11/dist-packages (from dspy>=2.6.5->dspy-ai) (2024.11.6)\n",
            "Collecting ujson>=5.8.0 (from dspy>=2.6.5->dspy-ai)\n",
            "  Downloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from dspy>=2.6.5->dspy-ai) (4.67.1)\n",
            "Collecting datasets>=2.14.6 (from dspy>=2.6.5->dspy-ai)\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from dspy>=2.6.5->dspy-ai) (2.32.3)\n",
            "Collecting optuna>=3.4.0 (from dspy>=2.6.5->dspy-ai)\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from dspy>=2.6.5->dspy-ai) (2.11.4)\n",
            "Collecting magicattr>=0.1.6 (from dspy>=2.6.5->dspy-ai)\n",
            "  Downloading magicattr-0.1.6-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting litellm>=1.60.3 (from dspy>=2.6.5->dspy-ai)\n",
            "  Downloading litellm-1.70.0-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting diskcache>=5.6.0 (from dspy>=2.6.5->dspy-ai)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting json-repair>=0.30.0 (from dspy>=2.6.5->dspy-ai)\n",
            "  Downloading json_repair-0.44.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from dspy>=2.6.5->dspy-ai) (9.1.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from dspy>=2.6.5->dspy-ai) (4.9.0)\n",
            "Collecting asyncer==0.0.8 (from dspy>=2.6.5->dspy-ai)\n",
            "  Downloading asyncer-0.0.8-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: cachetools>=5.5.0 in /usr/local/lib/python3.11/dist-packages (from dspy>=2.6.5->dspy-ai) (5.5.2)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dspy>=2.6.5->dspy-ai) (3.1.1)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from dspy>=2.6.5->dspy-ai) (13.9.4)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from dspy>=2.6.5->dspy-ai) (2.0.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio->dspy>=2.6.5->dspy-ai) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->dspy>=2.6.5->dspy-ai) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->dspy>=2.6.5->dspy-ai) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->dspy>=2.6.5->dspy-ai) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->dspy>=2.6.5->dspy-ai) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->dspy>=2.6.5->dspy-ai) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->dspy>=2.6.5->dspy-ai) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->dspy>=2.6.5->dspy-ai) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.14.6->dspy>=2.6.5->dspy-ai)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->dspy>=2.6.5->dspy-ai) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->dspy>=2.6.5->dspy-ai) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->dspy>=2.6.5->dspy-ai) (6.0.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from litellm>=1.60.3->dspy>=2.6.5->dspy-ai) (3.11.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from litellm>=1.60.3->dspy>=2.6.5->dspy-ai) (8.2.0)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.60.3->dspy>=2.6.5->dspy-ai) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.60.3->dspy>=2.6.5->dspy-ai) (8.7.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.60.3->dspy>=2.6.5->dspy-ai) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.60.3->dspy>=2.6.5->dspy-ai) (4.23.0)\n",
            "Collecting openai>=0.28.1 (from dspy>=2.6.5->dspy-ai)\n",
            "  Downloading openai-1.75.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting python-dotenv>=0.2.0 (from litellm>=1.60.3->dspy>=2.6.5->dspy-ai)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.60.3->dspy>=2.6.5->dspy-ai) (0.9.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from litellm>=1.60.3->dspy>=2.6.5->dspy-ai) (0.21.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=0.28.1->dspy>=2.6.5->dspy-ai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=0.28.1->dspy>=2.6.5->dspy-ai) (0.9.0)\n",
            "Collecting alembic>=1.5.0 (from optuna>=3.4.0->dspy>=2.6.5->dspy-ai)\n",
            "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna>=3.4.0->dspy>=2.6.5->dspy-ai)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna>=3.4.0->dspy>=2.6.5->dspy-ai) (2.0.40)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1.1->dspy>=2.6.5->dspy-ai) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1.1->dspy>=2.6.5->dspy-ai) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1.1->dspy>=2.6.5->dspy-ai) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->dspy>=2.6.5->dspy-ai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->dspy>=2.6.5->dspy-ai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->dspy>=2.6.5->dspy-ai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->dspy>=2.6.5->dspy-ai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->dspy>=2.6.5->dspy-ai) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->dspy>=2.6.5->dspy-ai) (2025.4.26)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->dspy>=2.6.5->dspy-ai) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->dspy>=2.6.5->dspy-ai) (2.19.1)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna>=3.4.0->dspy>=2.6.5->dspy-ai) (1.1.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.60.3->dspy>=2.6.5->dspy-ai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.60.3->dspy>=2.6.5->dspy-ai) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.60.3->dspy>=2.6.5->dspy-ai) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.60.3->dspy>=2.6.5->dspy-ai) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.60.3->dspy>=2.6.5->dspy-ai) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.60.3->dspy>=2.6.5->dspy-ai) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.60.3->dspy>=2.6.5->dspy-ai) (1.20.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm>=1.60.3->dspy>=2.6.5->dspy-ai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm>=1.60.3->dspy>=2.6.5->dspy-ai) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.60.3->dspy>=2.6.5->dspy-ai) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.60.3->dspy>=2.6.5->dspy-ai) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.60.3->dspy>=2.6.5->dspy-ai) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.60.3->dspy>=2.6.5->dspy-ai) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.60.3->dspy>=2.6.5->dspy-ai) (0.24.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->dspy>=2.6.5->dspy-ai) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1.1->dspy>=2.6.5->dspy-ai) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.4.0->dspy>=2.6.5->dspy-ai) (3.2.2)\n",
            "Downloading dspy_ai-2.6.24-py3-none-any.whl (1.1 kB)\n",
            "Downloading dspy-2.6.24-py3-none-any.whl (285 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.9/285.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asyncer-0.0.8-py3-none-any.whl (9.2 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json_repair-0.44.1-py3-none-any.whl (22 kB)\n",
            "Downloading litellm-1.70.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading magicattr-0.1.6-py2.py3-none-any.whl (4.7 kB)\n",
            "Downloading openai-1.75.0-py3-none-any.whl (646 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.0/647.0 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: magicattr, ujson, python-dotenv, json-repair, fsspec, diskcache, colorlog, backoff, asyncer, alembic, optuna, openai, litellm, datasets, dspy, dspy-ai\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.78.1\n",
            "    Uninstalling openai-1.78.1:\n",
            "      Successfully uninstalled openai-1.78.1\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed alembic-1.15.2 asyncer-0.0.8 backoff-2.2.1 colorlog-6.9.0 datasets-3.6.0 diskcache-5.6.3 dspy-2.6.24 dspy-ai-2.6.24 fsspec-2025.3.0 json-repair-0.44.1 litellm-1.70.0 magicattr-0.1.6 openai-1.75.0 optuna-4.3.0 python-dotenv-1.1.0 ujson-5.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install dspy-ai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command will install the latest stable version of DSPy\n",
        "datacamp.com\n",
        ". After installation, import the library in Python and (optionally) verify the version:"
      ],
      "metadata": {
        "id": "Wpn17zpxXaWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dspy\n",
        "print(\"DSPy version:\", dspy.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nneRGkpNXab9",
        "outputId": "e68427c6-df03-4e72-cf26-6f0486729d7e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DSPy version: 2.6.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting up LLM credentials**: DSPy can interface with various cloud LLM APIs (OpenAI, Anthropic, etc.) as well as local models. If you plan to use an API like OpenAI, you'll need to provide your API key. In Colab, you might store this securely (for example, using getpass to avoid hard-coding it). For instance:"
      ],
      "metadata": {
        "id": "pO9sRsMSXag9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vRLNLe-XamS",
        "outputId": "a0388ebb-5690-4158-fb9f-0466220d2c22"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔐 API Keys and Runtime Setup for DSPy\n",
        "\n",
        "DSPy handles authentication and runtime configuration with flexibility to support both cloud APIs and local models. Here’s what you need to know:\n",
        "\n",
        "---\n",
        "\n",
        "#### 🔑 API Key Management\n",
        "\n",
        "By default, **DSPy reads API keys from environment variables** when connecting to supported providers:\n",
        "\n",
        "* **OpenAI**: `OPENAI_API_KEY`\n",
        "* **Anthropic**: `ANTHROPIC_API_KEY`\n",
        "* **Databricks** and others follow a similar pattern\n",
        "\n",
        "Alternatively, you can **pass the key directly** when configuring your language model in DSPy.\n",
        "\n",
        "---\n",
        "\n",
        "#### ⚙️ Colab Runtime Considerations\n",
        "\n",
        "If you're working in **Google Colab**, keep in mind:\n",
        "\n",
        "* **Local models (via Hugging Face Transformers)**:\n",
        "\n",
        "  * Enable a **GPU** for hardware acceleration\n",
        "  * Go to:\n",
        "    `Runtime` → `Change runtime type` → Set **Hardware accelerator** to **GPU**\n",
        "\n",
        "* **Cloud APIs (e.g., OpenAI, Anthropic)**:\n",
        "\n",
        "  * No GPU is needed — computation is performed server-side\n",
        "\n",
        "---\n",
        "\n",
        "#### ✅ Ready to Initialize\n",
        "\n",
        "Once you've:\n",
        "\n",
        "* Installed DSPy\n",
        "* Set your API key(s) via environment variable or direct config\n",
        "* Enabled GPU (if using local models)\n",
        "\n",
        "...you’re ready to **initialize a language model** and begin building your DSPy pipelines!\n"
      ],
      "metadata": {
        "id": "OGbl9zpLXas4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's your content beautifully rewritten and formatted in **Markdown**:\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Overview of DSPy's Core Concepts and Architecture\n",
        "\n",
        "**DSPy** introduces a high-level programming model for building applications with large language models (LLMs). Instead of writing static prompt strings, DSPy encourages developers to structure applications into **modular components** with **declarative signatures**. These modules are then optimized automatically—through prompt tuning or model fine-tuning—to achieve better performance.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 🧩 Core Concepts in DSPy\n",
        "\n",
        "#### 🔖 Signatures\n",
        "\n",
        "A **Signature** defines the **input and output schema** of a module. It describes:\n",
        "\n",
        "* What the module is supposed to do\n",
        "* What kinds of inputs it expects\n",
        "* What outputs it should generate\n",
        "\n",
        "For example, a signature might specify:\n",
        "\n",
        "* **Input**: a question *(string)*\n",
        "* **Output**: an answer *(string)*\n",
        "\n",
        "This is a **declarative specification**—you tell the model *what* to do, not *how* to phrase the prompt. Signatures typically include:\n",
        "\n",
        "* A short task description\n",
        "* `InputField` and `OutputField` declarations for each parameter\n",
        "\n",
        "> Think of signatures as the \"contracts\" for your language model components.\n",
        "\n",
        "---\n",
        "\n",
        "#### 🧠 Modules (Predictors)\n",
        "\n",
        "A **module** in DSPy wraps a prompt strategy or pipeline step into a reusable component. Each module is built on a **Signature** and represents:\n",
        "\n",
        "* A single task or sub-task (e.g., retrieval, answering, reasoning)\n",
        "* A reusable logic unit, separate from specific prompts\n",
        "\n",
        "Modules are implemented as Python classes or functions. DSPy includes **standard modules** such as:\n",
        "\n",
        "* `ChainOfThought` – for step-by-step reasoning\n",
        "* `ReAct` – for tool-augmented agents\n",
        "\n",
        "Or, you can define your **custom modules** to suit your specific needs.\n",
        "\n",
        "> The separation between **logic** and **prompt content** makes DSPy modules highly reusable and adaptable across different LLMs.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#### 🛠️ Optimizers (Teleprompters)\n",
        "\n",
        "**Optimizers** are at the heart of DSPy’s **self-improving pipelines**. Nicknamed **teleprompters**, these algorithms:\n",
        "\n",
        "* Automatically optimize prompts\n",
        "* Tune model weights (optionally)\n",
        "* Use feedback from a defined **metric** to guide improvements\n",
        "\n",
        "Given a performance metric (e.g., accuracy, F1, BLEU), an optimizer can:\n",
        "\n",
        "* Add few-shot examples\n",
        "* Adjust prompt wording\n",
        "* Generate synthetic training data\n",
        "* Fine-tune a model\n",
        "\n",
        "> This automatic \"compilation\" process allows your pipeline to evolve and improve without manual tweaking.\n",
        "\n",
        "Examples of DSPy optimizers include:\n",
        "\n",
        "* `BootstrapFewShot` – for optimizing prompt content\n",
        "* `BootstrapFinetune` – for full model fine-tuning\n",
        "\n",
        "---\n",
        "\n",
        "#### 📏 Metrics and Evaluation\n",
        "\n",
        "In DSPy, you define a **metric function** that evaluates model outputs. This metric guides the optimization process.\n",
        "\n",
        "Metrics can be:\n",
        "\n",
        "* Simple (e.g., accuracy, F1 score)\n",
        "* Custom (e.g., string similarity, scoring logic)\n",
        "* LLM-based (e.g., using another model to assess answer quality)\n",
        "\n",
        "DSPy allows these metrics to be modular and extensible, including those that use LLMs for subjective or qualitative evaluation.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 How DSPy Works Under the Hood\n",
        "\n",
        "DSPy represents your application as a **text transformation graph**—a pipeline where each **node** is a **module** that performs a sub-task via an LLM.\n",
        "\n",
        "A **DSPy compiler** converts this graph into a fully optimized pipeline, learning:\n",
        "\n",
        "* Optimal prompts\n",
        "* (Optionally) optimal model weights\n",
        "\n",
        "This approach means you don’t need to rewrite prompts whenever:\n",
        "\n",
        "* The model changes\n",
        "* The task evolves\n",
        "* You switch to a different backend\n",
        "\n",
        "Instead, DSPy will **recompile** the program for you—adapting automatically.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Key Takeaway\n",
        "\n",
        "Whether you're building:\n",
        "\n",
        "* Few-shot classifiers\n",
        "* Chain-of-thought reasoners\n",
        "* Retrieval-augmented generators\n",
        "* Fine-tuned models\n",
        "\n",
        "...they all fit into DSPy’s **unified pipeline abstraction**. This makes DSPy not just a framework for prompt engineering, but a **compiler for language model programs**.\n",
        "\n",
        "> Figure 1 in the DSPy paper illustrates how DSPy cleanly separates program logic from prompt configuration, allowing automatic optimization and reuse.\n",
        "\n",
        "---\n",
        "\n",
        "*Now that DSPy is installed and we've covered the foundational concepts, let's move on to hands-on examples and see how it all comes together in practice.*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0qXd6onZYC7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Prompt Engineering with DSPy's Declarative Syntax\n",
        "One of the biggest advantages of DSPy is that you can perform prompt engineering declaratively in code. Instead of writing a long prompt template with placeholders and instructions, you define a Python class or use a shorthand syntax to describe the task. DSPy then handles constructing an effective prompt under the hood. This leads to clearer, more maintainable code and abstracts away the prompt quirks. Let’s walk through a simple example: suppose we want to build a question-answering module that gives a brief factual answer to any question. We’ll define a Signature for this task and then use DSPy’s Predict to create a module. Defining a Signature: We create a subclass of dspy.Signature with documentation and fields:"
      ],
      "metadata": {
        "id": "jU2wqfsEYNWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dspy\n",
        "\n",
        "# Configure a language model (for example, a free Hugging Face model)\n",
        "model_name = \"huggingface/google/flan-t5-base\"  # a small open-source model\n",
        "lm = dspy.LM(model=model_name)\n",
        "dspy.configure(lm=lm)  # set this as the default LM for DSPy\n",
        "\n",
        "# Define a signature for a basic QA task\n",
        "class BasicQA(dspy.Signature):\n",
        "    \"\"\"Answer questions with a brief factual answer.\"\"\"\n",
        "    question = dspy.InputField()\n",
        "    answer   = dspy.OutputField(desc=\"a short factoid answer\")\n"
      ],
      "metadata": {
        "id": "fmqFDvX7YCMs"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, BasicQA is a declarative specification of our task. The docstring “Answer questions with a brief factual answer.” acts as a high-level instruction to the model about this task (this will be woven into the prompt). We then declare an input field question and an output field answer. We even give the output field a description, which helps guide the model that we expect a short factoid answer\n",
        "medium.com\n",
        "medium.com\n",
        ". We did not write any prompt text like \"Please answer the question...\" – we simply described the task and I/O format. Creating a Predictor module: Now we use dspy.Predict to turn the signature into an actual callable module:"
      ],
      "metadata": {
        "id": "QxnbPbKrYXFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Lg73iJFHXa3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import dspy\n",
        "\n",
        "\n",
        "# Configure DSPy with OpenAI\n",
        "lm = dspy.LM(\"openai/gpt-4o-mini\")\n",
        "dspy.configure(lm=lm)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M0MOQbH7Xa7b"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you call generate_answer(...), DSPy will behind the scenes construct a prompt that includes the task description and fields, invoke the language model, and return a structured result (in this case, an object with an .answer attribute). For example, the output might look like:\n",
        "\n",
        "Question: What is a fungus?  \n",
        "Predicted Answer: Fungus is a type of organism.\n"
      ],
      "metadata": {
        "id": "xjCywujHXbAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though we never explicitly wrote a prompt, DSPy knew from the BasicQA signature how to ask the question to the LM and extract the answer\n",
        "medium.com\n",
        "medium.com\n",
        ". The model likely saw a prompt like: \"Answer questions with a brief factual answer. Question: What is a fungus? Answer:\" – and produced the answer text. Alternate shorthand: DSPy also allows a quick declarative syntax via strings. We could skip defining the class and instead do:"
      ],
      "metadata": {
        "id": "mdzMm_WPXbJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa = dspy.Predict(\"question: str -> response: str\")\n",
        "result = qa(question=\"What are high memory and low memory on Linux?\")\n",
        "print(result.response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74N8L4-kXbPQ",
        "outputId": "387309e8-ec70-4618-ba86-4a2561c9a939"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In Linux, \"high memory\" and \"low memory\" refer to different regions of the system's memory address space, particularly in the context of how the kernel manages memory for processes.\n",
            "\n",
            "- **Low Memory**: This typically refers to the memory that is directly accessible by the kernel and is used for kernel data structures and user processes. In a 32-bit architecture, low memory is usually limited to the first 896 MB (or 1 GB in some configurations) of the addressable memory space. This memory is easier for the kernel to manage and is used for tasks that require fast access.\n",
            "\n",
            "- **High Memory**: This refers to memory that is above the low memory limit and is not directly accessible by the kernel in a 32-bit system. Instead, it is accessible to user processes but requires special handling by the kernel to manage it. The kernel must use a mechanism called \"paging\" to access this memory, which can introduce some overhead. High memory is more relevant in systems with large amounts of RAM, where the total memory exceeds the limits of low memory.\n",
            "\n",
            "In summary, low memory is directly accessible by the kernel and is used for critical operations, while high memory is used for user processes and requires additional management by the kernel in 32-bit systems.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a signature for a basic QA task\n",
        "class BasicQA(dspy.Signature):\n",
        "    \"\"\"Answer questions with a brief factual answer.\"\"\"\n",
        "    question = dspy.InputField(desc=\"The question that needs to be answered\")\n",
        "    answer = dspy.OutputField(desc=\"a short factoid answer\")\n",
        "\n",
        "# Create a predictor module using the signature\n",
        "generate_answer = dspy.Predict(BasicQA)\n",
        "\n",
        "# Test it with a sample question\n",
        "result = generate_answer(question=\"What is a fungus?\")\n",
        "print(f\"Question: What is a fungus?\")\n",
        "print(f\"Predicted Answer: {result.answer}\")\n",
        "\n",
        "# Shorthand declarative syntax (introduced in newer versions)\n",
        "qa_shorthand = dspy.Predict(\"question: str -> response: str\")\n",
        "result_shorthand = qa_shorthand(question=\"What are high memory and low memory on Linux?\")\n",
        "print(\"\\nShorthand Result:\")\n",
        "print(f\"Question: What are high memory and low memory on Linux?\")\n",
        "print(f\"Response: {result_shorthand.response}\")\n",
        "\n",
        "# Chain of Thought example (common pattern in DSPy)\n",
        "class CoTQA(dspy.Signature):\n",
        "    \"\"\"Answer questions with step by step reasoning.\"\"\"\n",
        "    question = dspy.InputField()\n",
        "    reasoning = dspy.OutputField(desc=\"reasoning step by step\")\n",
        "    answer = dspy.OutputField(desc=\"the final answer\")\n",
        "\n",
        "# Create a chain of thought module\n",
        "cot_answer = dspy.ChainOfThought(CoTQA)\n",
        "\n",
        "# Test with a math question\n",
        "cot_result = cot_answer(question=\"If I have 5 apples and give 2 to my friend, then buy 3 more, how many do I have?\")\n",
        "print(\"\\nChain of Thought Example:\")\n",
        "print(f\"Question: If I have 5 apples and give 2 to my friend, then buy 3 more, how many do I have?\")\n",
        "print(f\"Reasoning: {cot_result.reasoning}\")\n",
        "print(f\"Answer: {cot_result.answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u51QJvjkDAs",
        "outputId": "fa5d85fa-2cb1-4fd8-f60c-381dd99005c4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is a fungus?\n",
            "Predicted Answer: A fungus is a type of organism that belongs to the kingdom Fungi, which includes yeasts, molds, and mushrooms, and is characterized by its ability to decompose organic material and absorb nutrients through its cell walls.\n",
            "\n",
            "Shorthand Result:\n",
            "Question: What are high memory and low memory on Linux?\n",
            "Response: In Linux, \"high memory\" and \"low memory\" refer to different regions of the system's memory address space, particularly in the context of how the kernel manages memory for processes.\n",
            "\n",
            "- **Low Memory**: This typically refers to the memory that is directly accessible by the kernel and is used for kernel data structures and user processes. In a 32-bit architecture, low memory is usually limited to the first 896 MB (or 1 GB in some configurations) of the addressable memory space. This memory is easier for the kernel to manage and is used for tasks that require fast access.\n",
            "\n",
            "- **High Memory**: This refers to memory that is above the low memory limit and is not directly accessible by the kernel in a 32-bit system. Instead, it is accessible to user processes but requires special handling by the kernel to manage it. The kernel must use a mechanism called \"paging\" to access this memory, which can introduce some overhead. High memory is more relevant in systems with large amounts of RAM, where the total memory exceeds the limits of low memory.\n",
            "\n",
            "In summary, low memory is directly accessible by the kernel and is used for critical operations, while high memory is used for user processes and requires additional management by the kernel in 32-bit systems.\n",
            "\n",
            "Chain of Thought Example:\n",
            "Question: If I have 5 apples and give 2 to my friend, then buy 3 more, how many do I have?\n",
            "Reasoning: 1. Start with the initial number of apples: 5 apples.\n",
            "2. Give 2 apples to a friend: 5 - 2 = 3 apples remaining.\n",
            "3. Buy 3 more apples: 3 + 3 = 6 apples in total.\n",
            "4. Therefore, after giving away 2 apples and buying 3 more, the total number of apples is 6.\n",
            "Answer: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✍️ Declarative Prompting with DSPy: A Function-Like Approach\n",
        "\n",
        "DSPy allows you to create a **signature from a simple string format**, such as:\n",
        "\n",
        "```\n",
        "\"input_field: type -> output_field: type\"\n",
        "```\n",
        "\n",
        "This one-liner defines the **input-output schema** and immediately calls the model. For example, a signature like:\n",
        "\n",
        "```\n",
        "\"question: str -> response: str\"\n",
        "```\n",
        "\n",
        "...might return a brief explanation such as the difference between **high memory and low memory in Linux**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 How It Works\n",
        "\n",
        "Under the hood, DSPy:\n",
        "\n",
        "* Parses the signature format\n",
        "* Automatically builds a prompt using the field names and any provided types/descriptions\n",
        "* Sends the prompt to the model and returns structured output\n",
        "\n",
        "You **don’t need to manually craft prompt text** like “Please answer the following question…” Instead, you simply declare the structure and intent of the task.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧩 Prompting = Programming\n",
        "\n",
        "Through these examples, you can see that **prompt engineering in DSPy is more like writing a function signature** than crafting a static prompt string.\n",
        "\n",
        "Instead of hardcoding how the model should be instructed, you **declare what you want**, and DSPy handles the *how*—letting the model interpret your signature and docstrings intelligently.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Benefits of This Declarative Approach\n",
        "\n",
        "* ✨ **Cleaner Code**\n",
        "  Looks and feels like standard function declarations—less clutter and easier to read.\n",
        "\n",
        "* 🔁 **Easier to Modify**\n",
        "  To change the model’s behavior, simply update the **docstring** or **field descriptions**.\n",
        "\n",
        "* 📦 **More Reusable**\n",
        "  No need to maintain complex prompt templates. You can use the same module logic with different models or use cases.\n",
        "\n",
        "---\n",
        "\n",
        "### 🛠️ Flexibility in Output Design\n",
        "\n",
        "If you want the answer to be:\n",
        "\n",
        "* **More detailed**\n",
        "* **Formatted in a certain way**\n",
        "* **Tailored to a specific tone or output style**\n",
        "\n",
        "...you can simply **adjust the signature’s docstring** or the description on the `OutputField`. There’s no need to rewrite an entire prompt block.\n",
        "\n",
        "---\n",
        "\n",
        "> 🔁 This makes your code **easier to change**, **more maintainable**, and **robust to model or task updates**—making DSPy an ideal tool for both prototyping and production.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "t02IKUMhXbUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 4. ⚙️ Techniques for Programmatic Optimization and Model Improvement\n",
        "\n",
        "Defining **modules and signatures** in DSPy is just the beginning—the real power lies in DSPy’s ability to **programmatically optimize** those modules. DSPy includes several built-in **optimizers** (also known as **teleprompters**) that automatically improve your pipeline’s performance based on a metric you define.\n",
        "\n",
        "These optimizers support techniques like:\n",
        "\n",
        "* Few-shot prompt bootstrapping\n",
        "* Prompt refinement\n",
        "* Full model fine-tuning\n",
        "\n",
        "---\n",
        "\n",
        "### 🛠️ Built-in DSPy Optimization Techniques\n",
        "\n",
        "#### 🔹 Few-Shot Prompt Bootstrapping: `BootstrapFewShot`\n",
        "\n",
        "A simple yet powerful optimizer that:\n",
        "\n",
        "* Adds **example Q\\&A pairs** (a.k.a. demonstrations) to the prompt\n",
        "* Simulates usage of your module to generate new examples\n",
        "* Incorporates helpful examples **in-context**\n",
        "\n",
        "Best used when:\n",
        "\n",
        "* You have **very little training data** (\\~10 examples)\n",
        "* You want a fast way to improve a module’s effectiveness\n",
        "\n",
        "> It \"learns better prompts by example\"—a go-to for minimal-data scenarios.\n",
        "\n",
        "---\n",
        "\n",
        "#### 🔹 Random Prompt Search: `BootstrapFewShotWithRandomSearch`\n",
        "\n",
        "An extension of `BootstrapFewShot` that:\n",
        "\n",
        "* Generates **variations** of examples or prompt wording\n",
        "* Applies **random permutations** (e.g., instruction order, phrasing)\n",
        "* Evaluates which version works best using your metric\n",
        "\n",
        "Best used when:\n",
        "\n",
        "* You have a **moderate number of examples** (\\~50)\n",
        "* You suspect **prompt structure** matters\n",
        "\n",
        "> This is an **automated prompt tuning via random search**.\n",
        "\n",
        "---\n",
        "\n",
        "#### 🔹 Iterative Multi-Step Optimization: `MIPRO`\n",
        "\n",
        "For **large datasets** (hundreds of examples), consider `MIPRO` (Mixed-Init Prompt Optimization). This advanced optimizer:\n",
        "\n",
        "* Bootstraps with few-shot examples\n",
        "* Refines with systematic exploration or gradient-style optimization\n",
        "* Iteratively improves **multi-step reasoning pipelines**\n",
        "\n",
        "Best used when:\n",
        "\n",
        "* Your task involves **complex chains of logic**\n",
        "* You want **state-of-the-art performance**\n",
        "\n",
        "> These optimizers are **compute-intensive**, but highly effective.\n",
        "\n",
        "---\n",
        "\n",
        "#### 🔹 Feedback & AI-Critic Optimization\n",
        "\n",
        "While not a standalone DSPy class, you can implement **AI feedback loops** by:\n",
        "\n",
        "* Using a **powerful model** (e.g., GPT-4) to **evaluate** outputs from a smaller model\n",
        "* Feeding this evaluation as a **metric** into an optimizer\n",
        "\n",
        "This mimics **RLHF (Reinforcement Learning from Human Feedback)**—but uses an **AI-critic** instead of humans.\n",
        "\n",
        "> DSPy's flexibility allows for meta-optimization strategies like this.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔄 Combining Optimization Strategies\n",
        "\n",
        "DSPy optimizers can **chain techniques together**. For example:\n",
        "\n",
        "* `SIMBA` might simulate interactions, apply **bandit algorithms**, and adapt prompts based on feedback.\n",
        "\n",
        "🧠 The **default workflow** is usually:\n",
        "\n",
        "1. Start with a **bootstrap optimizer**\n",
        "2. If needed, apply **random search**, **MIPRO**, or **fine-tuning**\n",
        "\n",
        "---\n",
        "\n",
        "## 🧪 How to Use an Optimizer in DSPy\n",
        "\n",
        "Using an optimizer involves **4 key steps**:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Define a Metric\n",
        "\n",
        "A **metric** evaluates your module’s output. For example:\n",
        "\n",
        "* Accuracy: does `predicted_output == gold_output`?\n",
        "* Custom functions (string similarity, BLEU, F1, etc.)\n",
        "* Use DSPy's `Evaluate` utility for datasets\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Instantiate the Optimizer\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Compile the Program\n",
        "\n",
        "Treat your module as a program and optimize it:\n",
        "\n",
        "\n",
        "\n",
        "* `training_examples` is a list of `dspy.Example` objects or a dataset\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Use or Evaluate the Optimized Module\n",
        "\n",
        "The result is a new module with:\n",
        "\n",
        "* Improved prompts\n",
        "* Possibly adjusted weights (if applicable)\n",
        "\n",
        "You can now use the optimized module **just like the original**:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "phczMB1MYrf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dspy import Example, Evaluate, BootstrapFewShot\n",
        "\n",
        "# Suppose we have some labeled examples for QA\n",
        "dev_set = [\n",
        "    Example(question=\"Who wrote 'To Kill a Mockingbird'?\", answer=\"Harper Lee\"),\n",
        "    Example(question=\"What is the capital of France?\", answer=\"Paris\"),\n",
        "    # ... more examples\n",
        "]\n",
        "\n",
        "# Define a simple accuracy metric\n",
        "def exact_match_metric(example, pred):\n",
        "    return 1.0 if pred.answer.strip().lower() == example.answer.strip().lower() else 0.0\n",
        "\n",
        "# Evaluate current performance (zero-shot, no optimization)\n",
        "evaluate = Evaluate(devset=dev_set, metric=exact_match_metric)\n",
        "baseline_score = evaluate(generate_answer)  # our unoptimized predictor\n",
        "print(\"Baseline accuracy:\", baseline_score)\n",
        "\n",
        "# Optimize the module using BootstrapFewShot\n",
        "optimizer = BootstrapFewShot(metric=exact_match_metric)\n",
        "optimized_generate_answer = optimizer.compile(generate_answer, trainset=dev_set)\n",
        "\n",
        "# Evaluate optimized module\n",
        "optimized_score = evaluate(optimized_generate_answer)\n",
        "print(\"Optimized accuracy:\", optimized_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wgxhj9N_XbZ9",
        "outputId": "86f12c67-84a1-4f4b-fdac-4465ce361643"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/05/17 14:05:01 ERROR dspy.utils.parallelizer: Error for Example({'question': \"Who wrote 'To Kill a Mockingbird'?\", 'answer': 'Harper Lee'}) (input_keys=None): Inputs have not been set for this example. Use `example.with_inputs()` to set them.. Set `provide_traceback=True` for traceback.\n",
            "2025/05/17 14:05:01 ERROR dspy.utils.parallelizer: Error for Example({'question': 'What is the capital of France?', 'answer': 'Paris'}) (input_keys=None): Inputs have not been set for this example. Use `example.with_inputs()` to set them.. Set `provide_traceback=True` for traceback.\n",
            "2025/05/17 14:05:01 INFO dspy.evaluate.evaluate: Average Metric: 0.0 / 2 (0.0%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline accuracy: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]2025/05/17 14:05:01 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'question': \"Who wrote 'To Kill a Mockingbird'?\", 'answer': 'Harper Lee'}) (input_keys=None) with <function exact_match_metric at 0x79c1eaa47a60> due to Inputs have not been set for this example. Use `example.with_inputs()` to set them..\n",
            "2025/05/17 14:05:01 ERROR dspy.teleprompt.bootstrap: Failed to run or to evaluate example Example({'question': 'What is the capital of France?', 'answer': 'Paris'}) (input_keys=None) with <function exact_match_metric at 0x79c1eaa47a60> due to Inputs have not been set for this example. Use `example.with_inputs()` to set them..\n",
            "100%|██████████| 2/2 [00:00<00:00, 1209.78it/s]\n",
            "2025/05/17 14:05:01 ERROR dspy.utils.parallelizer: Error for Example({'question': \"Who wrote 'To Kill a Mockingbird'?\", 'answer': 'Harper Lee'}) (input_keys=None): Inputs have not been set for this example. Use `example.with_inputs()` to set them.. Set `provide_traceback=True` for traceback.\n",
            "2025/05/17 14:05:01 ERROR dspy.utils.parallelizer: Error for Example({'question': 'What is the capital of France?', 'answer': 'Paris'}) (input_keys=None): Inputs have not been set for this example. Use `example.with_inputs()` to set them.. Set `provide_traceback=True` for traceback.\n",
            "2025/05/17 14:05:01 INFO dspy.evaluate.evaluate: Average Metric: 0.0 / 2 (0.0%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 0 full traces after 1 examples for up to 1 rounds, amounting to 2 attempts.\n",
            "Optimized accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dspy\n",
        "import os\n",
        "\n",
        "# Set up DSPy with a language model\n",
        "lm = dspy.LM(\"openai/gpt-4o-mini\")\n",
        "dspy.settings.configure(lm=lm)\n",
        "\n",
        "# Define a simple RAG signature for question answering\n",
        "class RAG(dspy.Signature):\n",
        "    \"\"\"Answer questions based on the retrieved context.\"\"\"\n",
        "    question = dspy.InputField()\n",
        "    context = dspy.InputField(desc=\"retrieved passages from a knowledge base\")\n",
        "    answer = dspy.OutputField(desc=\"a detailed answer based on the context\")\n",
        "\n",
        "# Define a retrieval module signature\n",
        "class Retrieve(dspy.Signature):\n",
        "    \"\"\"Retrieve relevant passages for a question.\"\"\"\n",
        "    question = dspy.InputField()\n",
        "    passages = dspy.OutputField(desc=\"retrieved passages relevant to the question\")\n",
        "\n",
        "# Create a simple RAG pipeline\n",
        "class SimplifiedRAG(dspy.Module):\n",
        "    def __init__(self, num_passages=3):\n",
        "        super().__init__()\n",
        "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
        "        self.generate_answer = dspy.Predict(RAG)\n",
        "\n",
        "    def forward(self, question):\n",
        "        # Step 1: Retrieve relevant passages\n",
        "        retrieved = self.retrieve(question)\n",
        "\n",
        "        # Step 2: Generate answer based on retrieved context\n",
        "        context = \"\\n\\n\".join(retrieved.passages)\n",
        "        prediction = self.generate_answer(question=question, context=context)\n",
        "\n",
        "        return dspy.Prediction(answer=prediction.answer)\n",
        "\n",
        "# Set up a retrieval model (for example purposes)\n",
        "# In actual implementation, you'd use a real retrieval model\n",
        "print(\"Setting up a mock retrieval model for example purposes\")\n",
        "print(\"In practice, you would configure with:\")\n",
        "print(\"retriever = dspy.ColBERTv2(url='http://example.com/colbert')\")\n",
        "print(\"dspy.settings.configure(rm=retriever)\")\n",
        "\n",
        "# For this example, we're using a mock retriever\n",
        "class MockRetriever:\n",
        "    def __init__(self):\n",
        "        self.db = {\n",
        "            \"machine learning\": [\n",
        "                \"Machine learning is a branch of artificial intelligence that focuses on building systems that learn from data.\",\n",
        "                \"In machine learning, algorithms are trained on data to make predictions or decisions without being explicitly programmed.\",\n",
        "                \"Common machine learning approaches include supervised learning, unsupervised learning, and reinforcement learning.\"\n",
        "            ],\n",
        "            \"python programming\": [\n",
        "                \"Python is a high-level, interpreted programming language known for its readability and simplicity.\",\n",
        "                \"Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming.\",\n",
        "                \"Python has a large standard library and ecosystem of third-party packages for various applications.\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def search(self, query, k=3):\n",
        "        # Very simplified retrieval logic\n",
        "        for key, passages in self.db.items():\n",
        "            if key in query.lower():\n",
        "                return passages[:k]\n",
        "        return [\"No relevant information found.\"]\n",
        "\n",
        "# Register our mock retriever\n",
        "mock_rm = MockRetriever()\n",
        "dspy.settings.configure(rm=mock_rm)\n",
        "\n",
        "# Override the retrieve module to use our mock retriever\n",
        "def my_retrieve(self, question):\n",
        "    passages = mock_rm.search(question)\n",
        "    return dspy.Prediction(passages=passages)\n",
        "\n",
        "dspy.Retrieve.forward = my_retrieve\n",
        "\n",
        "# Create an instance of our RAG pipeline\n",
        "rag_pipeline = SimplifiedRAG(num_passages=2)\n",
        "\n",
        "# Test the RAG pipeline\n",
        "question = \"Explain the basics of machine learning\"\n",
        "result = rag_pipeline(question)\n",
        "print(f\"\\nQuestion: {question}\")\n",
        "print(f\"Answer: {result.answer}\")\n",
        "\n",
        "# Test with another question\n",
        "question = \"What is Python programming good for?\"\n",
        "result = rag_pipeline(question)\n",
        "print(f\"\\nQuestion: {question}\")\n",
        "print(f\"Answer: {result.answer}\")\n",
        "\n",
        "# Optimizing the RAG pipeline\n",
        "print(\"\\nIn a real application, you would optimize the RAG pipeline with:\")\n",
        "print(\"\"\"\n",
        "# Define a suitable metric\n",
        "def rag_metric(example, pred):\n",
        "    # Measure correctness and relevance of the answer\n",
        "    correctness = ... # Logic to evaluate correctness\n",
        "    relevance = ... # Logic to evaluate relevance\n",
        "    return (correctness + relevance) / 2\n",
        "\n",
        "# Create evaluation dataset with explicit input specification\n",
        "dev_set = [\n",
        "    Example(question=\"What is machine learning?\", answer=\"Machine learning is...\").with_inputs(\"question\"),\n",
        "    ...\n",
        "]\n",
        "\n",
        "# Optimize with MIPROv2 or another suitable optimizer\n",
        "optimizer = dspy.MIPROv2(metric=rag_metric)\n",
        "optimized_rag = optimizer.compile(rag_pipeline, trainset=dev_set)\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nFor a complete RAG implementation with DSPy, refer to the documentation:\")\n",
        "print(\"https://dspy.ai/tutorials/rag/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itFlbo-fkpqn",
        "outputId": "3f973955-7755-437f-d253-7b39d3e53f93"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up a mock retrieval model for example purposes\n",
            "In practice, you would configure with:\n",
            "retriever = dspy.ColBERTv2(url='http://example.com/colbert')\n",
            "dspy.settings.configure(rm=retriever)\n",
            "\n",
            "Question: Explain the basics of machine learning\n",
            "Answer: Machine learning is a subset of artificial intelligence that emphasizes the development of algorithms and models that enable computers to learn from and make predictions based on data. The fundamental idea is to allow systems to improve their performance on a specific task over time without being explicitly programmed for each scenario.\n",
            "\n",
            "There are three primary types of machine learning approaches:\n",
            "\n",
            "1. **Supervised Learning**: In this approach, the algorithm is trained on a labeled dataset, meaning that each training example is paired with an output label. The model learns to map inputs to the correct outputs, which allows it to make predictions on new, unseen data.\n",
            "\n",
            "2. **Unsupervised Learning**: Unlike supervised learning, unsupervised learning deals with unlabeled data. The algorithm tries to identify patterns or groupings within the data without any prior knowledge of the outcomes. Common techniques include clustering and dimensionality reduction.\n",
            "\n",
            "3. **Reinforcement Learning**: This approach involves training algorithms to make a sequence of decisions by rewarding them for good actions and penalizing them for bad ones. The model learns to optimize its strategy over time based on the feedback it receives from the environment.\n",
            "\n",
            "Overall, machine learning enables systems to adapt and improve autonomously, making it a powerful tool in various applications, from image recognition to natural language processing and beyond.\n",
            "\n",
            "Question: What is Python programming good for?\n",
            "Answer: Python programming is good for a wide range of applications due to its versatility and ease of use. It is particularly well-suited for web development, data analysis, artificial intelligence, scientific computing, automation, and scripting. The language's readability and simplicity make it an excellent choice for beginners, while its support for multiple programming paradigms allows experienced developers to implement complex solutions efficiently. Additionally, Python's extensive standard library and a rich ecosystem of third-party packages enable developers to quickly build and deploy applications across various domains.\n",
            "\n",
            "In a real application, you would optimize the RAG pipeline with:\n",
            "\n",
            "# Define a suitable metric\n",
            "def rag_metric(example, pred):\n",
            "    # Measure correctness and relevance of the answer\n",
            "    correctness = ... # Logic to evaluate correctness\n",
            "    relevance = ... # Logic to evaluate relevance\n",
            "    return (correctness + relevance) / 2\n",
            "\n",
            "# Create evaluation dataset with explicit input specification\n",
            "dev_set = [\n",
            "    Example(question=\"What is machine learning?\", answer=\"Machine learning is...\").with_inputs(\"question\"),\n",
            "    ...\n",
            "]\n",
            "\n",
            "# Optimize with MIPROv2 or another suitable optimizer\n",
            "optimizer = dspy.MIPROv2(metric=rag_metric)\n",
            "optimized_rag = optimizer.compile(rag_pipeline, trainset=dev_set)\n",
            "\n",
            "\n",
            "For a complete RAG implementation with DSPy, refer to the documentation:\n",
            "https://dspy.ai/tutorials/rag/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 🔧 Step-by-Step Guidance on Fine-Tuning Workflows\n",
        "\n",
        "While **prompt optimization** alone can bring significant improvements, there are cases where you may want to go further and **fine-tune a model’s weights** for:\n",
        "\n",
        "* Better task-specific performance\n",
        "* Lower inference cost\n",
        "* Deployment in resource-constrained environments\n",
        "\n",
        "DSPy supports **fine-tuning workflows** by bridging **prompt-based improvements** with **model training**, enabling you to distill the capabilities of large models into smaller ones for practical use.\n",
        "\n",
        "> 🔗 [Source: Medium](https://medium.com)\n",
        "\n",
        "---\n",
        "\n",
        "### 🧪 The Typical Fine-Tuning Scenario\n",
        "\n",
        "1. Start with a **powerful model** (e.g., GPT-4 or Claude)\n",
        "2. Use DSPy to **optimize the prompts**\n",
        "3. Then **distill** or **fine-tune** a smaller, cost-efficient model to replicate the behavior\n",
        "4. Deploy the smaller model in production\n",
        "\n",
        "This lets you **retain performance** while reducing dependence on high-cost APIs.\n",
        "\n",
        "---\n",
        "\n",
        "### 📝 How to Fine-Tune with DSPy\n",
        "\n",
        "#### ✅ 1. Prepare Your Data as `dspy.Example` Objects\n",
        "\n",
        "* Collect a dataset of **input-output pairs**\n",
        "* These can be:\n",
        "\n",
        "  * An existing **labeled dataset**\n",
        "  * A synthetic dataset **generated by a large teacher model** (e.g., GPT-4)\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "from dspy import Example\n",
        "\n",
        "dev_set = [\n",
        "    Example(question=\"What is photosynthesis?\", answer=\"The process by which plants convert light into energy.\"),\n",
        "    Example(question=\"Who wrote 'Hamlet'?\", answer=\"William Shakespeare\"),\n",
        "]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### ✅ 2. Optimize Prompts First (Optional but Recommended)\n",
        "\n",
        "* Use an optimizer like `BootstrapFewShot` with a large LLM (e.g., GPT-4 or LLaMA2-70B)\n",
        "* Achieve a strong **prompt-based solution** that serves as a **teacher model**\n",
        "\n",
        "> This serves as the foundation for distillation into a smaller model.\n",
        "\n",
        "---\n",
        "\n",
        "#### ✅ 3. Use `BootstrapFinetune` to Train a Smaller Model\n",
        "\n",
        "DSPy provides the `BootstrapFinetune` optimizer, which:\n",
        "\n",
        "* Takes an optimized DSPy program\n",
        "* Generates synthetic training data (if needed)\n",
        "* Fine-tunes a smaller model’s weights to **mimic** the original performance\n",
        "\n",
        "You specify:\n",
        "\n",
        "* The original DSPy module (e.g., `generate_answer`)\n",
        "* The **target model** (e.g., `google/flan-t5-base` or `meta-llama/Llama-2-7b`)\n",
        "* The evaluation **metric** used to guide improvement\n",
        "\n",
        "---\n",
        "\n",
        "#### 🧩 Example Usage\n",
        "\n",
        "```python\n",
        "from dspy import BootstrapFinetune\n",
        "\n",
        "# Define your metric (e.g., exact match or F1)\n",
        "def exact_match_metric(example, pred):\n",
        "    return 1.0 if pred.answer.strip().lower() == example.answer.strip().lower() else 0.0\n",
        "\n",
        "# Create the optimizer\n",
        "student_opt = BootstrapFinetune(metric=exact_match_metric)\n",
        "\n",
        "# Fine-tune a smaller model to replicate a prompt-based program\n",
        "distilled_program = student_opt.compile(generate_answer, trainset=dev_set)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 Why This Workflow Matters\n",
        "\n",
        "* ⚡ **Performance**: Smaller models can approach the accuracy of large ones\n",
        "* 💰 **Cost-Efficiency**: Great for reducing API usage and speeding up inference\n",
        "* 🚀 **Deployability**: Easier to serve models locally or on limited hardware\n",
        "\n",
        "DSPy enables a **data-efficient distillation process**—by reusing optimized prompts, you bootstrap model training with very little manual work.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TNk_6C7dXbeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dspy import BootstrapFinetune, Example\n",
        "import os\n",
        "\n",
        "# Preparing a larger dataset for fine-tuning\n",
        "# In practice, you would load this from a file or API\n",
        "fine_tuning_examples = [\n",
        "    Example(question=\"What is photosynthesis?\",\n",
        "            answer=\"The process by which plants convert light energy into chemical energy that can be used to fuel the organisms' activities.\").with_inputs(\"question\"),\n",
        "    Example(question=\"Who wrote 'Hamlet'?\",\n",
        "            answer=\"William Shakespeare\").with_inputs(\"question\"),\n",
        "    Example(question=\"What is the capital of Japan?\",\n",
        "            answer=\"Tokyo\").with_inputs(\"question\"),\n",
        "    Example(question=\"What is the largest planet in our solar system?\",\n",
        "            answer=\"Jupiter\").with_inputs(\"question\"),\n",
        "    Example(question=\"What is the chemical formula for water?\",\n",
        "            answer=\"H2O\").with_inputs(\"question\"),\n",
        "    # Add more examples as needed\n",
        "]\n",
        "\n",
        "# Define a metric for fine-tuning\n",
        "def semantic_match_metric(example, pred):\n",
        "    \"\"\"Measure semantic similarity between prediction and reference.\"\"\"\n",
        "    # In practice, you might use sentence embeddings or an LLM-based evaluator\n",
        "    # This is a simplified version\n",
        "    reference = example.answer.lower()\n",
        "    prediction = pred.answer.lower()\n",
        "\n",
        "    # Check if key terms are present\n",
        "    key_terms_present = all(term in prediction for term in reference.split()[:3])\n",
        "    return 1.0 if key_terms_present else 0.0\n",
        "\n",
        "# The simplest version of BootstrapFinetune - just with a metric\n",
        "print(\"Setting up BootstrapFinetune optimizer...\")\n",
        "student_opt = BootstrapFinetune(metric=semantic_match_metric)\n",
        "\n",
        "# Example of how you would use it\n",
        "print(\"This would fine-tune the model (not actually running due to compute requirements)\")\n",
        "print(\"Example usage code (specific parameters may vary based on DSPy version):\")\n",
        "print(\"\"\"\n",
        "# Set up the target model\n",
        "target_lm = dspy.LM(\"huggingface/google/flan-t5-base\")\n",
        "\n",
        "# For older DSPy versions\n",
        "distilled_program = student_opt.compile(\n",
        "    generate_answer,\n",
        "    trainset=fine_tuning_examples,\n",
        "    target_lm=target_lm  # In some versions\n",
        ")\n",
        "\n",
        "# For newer DSPy versions (may need additional parameters)\n",
        "# Check the current documentation for the exact API\n",
        "# Parameters like finetune_args might be used to pass max_epochs, batch_size, etc.\n",
        "\"\"\")\n",
        "\n",
        "# How to save and load fine-tuned models\n",
        "print(\"\\nAfter fine-tuning, you can save the model:\")\n",
        "print(\"distilled_program.lm.save_pretrained('path/to/save/model')\")\n",
        "\n",
        "print(\"\\nTo load and use a fine-tuned model:\")\n",
        "print(\"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\")\n",
        "print(\"model = AutoModelForSeq2SeqLM.from_pretrained('path/to/save/model')\")\n",
        "print(\"tokenizer = AutoTokenizer.from_pretrained('path/to/save/model')\")\n",
        "print(\"finetuned_lm = dspy.LM(model=model, tokenizer=tokenizer)\")\n",
        "print(\"dspy.settings.configure(lm=finetuned_lm)\")\n",
        "\n",
        "# Integration with MLflow for tracking experiments\n",
        "print(\"\\nTo track fine-tuning with MLflow:\")\n",
        "print(\"\"\"\n",
        "import mlflow\n",
        "\n",
        "with mlflow.start_run(run_name=\"dspy_finetuning\"):\n",
        "    # Set up and run fine-tuning\n",
        "    target_lm = dspy.LM(\"huggingface/google/flan-t5-base\")\n",
        "    distilled_program = student_opt.compile(generate_answer, trainset=fine_tuning_examples)\n",
        "\n",
        "    # Log metrics\n",
        "    mlflow.log_metric(\"final_accuracy\", evaluate(distilled_program))\n",
        "\n",
        "    # Log the model\n",
        "    mlflow.dspy.log_model(distilled_program, \"fine_tuned_model\")\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nNote: The BootstrapFinetune API has been evolving in DSPy.\")\n",
        "print(\"For the most up-to-date usage, please check the official documentation:\")\n",
        "print(\"https://dspy.ai\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sodxfbRXbjh",
        "outputId": "56d43acc-bc74-4f6a-aa4b-e5cb2e0cc519"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up BootstrapFinetune optimizer...\n",
            "This would fine-tune the model (not actually running due to compute requirements)\n",
            "Example usage code (specific parameters may vary based on DSPy version):\n",
            "\n",
            "# Set up the target model\n",
            "target_lm = dspy.LM(\"huggingface/google/flan-t5-base\") \n",
            "\n",
            "# For older DSPy versions\n",
            "distilled_program = student_opt.compile(\n",
            "    generate_answer, \n",
            "    trainset=fine_tuning_examples,\n",
            "    target_lm=target_lm  # In some versions\n",
            ")\n",
            "\n",
            "# For newer DSPy versions (may need additional parameters)\n",
            "# Check the current documentation for the exact API\n",
            "# Parameters like finetune_args might be used to pass max_epochs, batch_size, etc.\n",
            "\n",
            "\n",
            "After fine-tuning, you can save the model:\n",
            "distilled_program.lm.save_pretrained('path/to/save/model')\n",
            "\n",
            "To load and use a fine-tuned model:\n",
            "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
            "model = AutoModelForSeq2SeqLM.from_pretrained('path/to/save/model')\n",
            "tokenizer = AutoTokenizer.from_pretrained('path/to/save/model')\n",
            "finetuned_lm = dspy.LM(model=model, tokenizer=tokenizer)\n",
            "dspy.settings.configure(lm=finetuned_lm)\n",
            "\n",
            "To track fine-tuning with MLflow:\n",
            "\n",
            "import mlflow\n",
            "\n",
            "with mlflow.start_run(run_name=\"dspy_finetuning\"):\n",
            "    # Set up and run fine-tuning\n",
            "    target_lm = dspy.LM(\"huggingface/google/flan-t5-base\")\n",
            "    distilled_program = student_opt.compile(generate_answer, trainset=fine_tuning_examples)\n",
            "    \n",
            "    # Log metrics\n",
            "    mlflow.log_metric(\"final_accuracy\", evaluate(distilled_program))\n",
            "    \n",
            "    # Log the model\n",
            "    mlflow.dspy.log_model(distilled_program, \"fine_tuned_model\")\n",
            "\n",
            "\n",
            "Note: The BootstrapFinetune API has been evolving in DSPy.\n",
            "For the most up-to-date usage, please check the official documentation:\n",
            "https://dspy.ai\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Here's your content beautifully rewritten in **Markdown** format for readability and clarity:\n",
        "\n",
        "---\n",
        "\n",
        "## 🎓 Fine-Tuning with DSPy: From Large Models to Efficient Deployments\n",
        "\n",
        "### 🧪 Model Distillation via `BootstrapFinetune`\n",
        "\n",
        "DSPy enables a powerful workflow for fine-tuning smaller models using the outputs of larger, high-performance models. This process is initiated with `BootstrapFinetune`, which triggers the following sequence:\n",
        "\n",
        "1. **Student Program Creation**:\n",
        "   DSPy creates a **\"student\"** version of your program.\n",
        "\n",
        "2. **Synthetic Data Generation**:\n",
        "   Under the hood, it uses the **teacher model** (e.g., GPT-4 or Claude) to generate a wide range of **input-output examples**—potentially beyond your development set.\n",
        "\n",
        "3. **Model Fine-Tuning**:\n",
        "   The **smaller model** (e.g., FLAN-T5 or LLaMA 7B) is fine-tuned on this synthetic dataset, learning to mimic the behavior of the larger model.\n",
        "\n",
        "> 🔗 Source: [OpenReview – DSPy Paper](https://openreview.net/forum?id=sY5N0zY5Od)\n",
        "\n",
        "---\n",
        "\n",
        "### 📦 Saving and Deploying the Fine-Tuned Model\n",
        "\n",
        "Once `BootstrapFinetune` completes:\n",
        "\n",
        "* The resulting `distilled_program` is a new DSPy module backed by your **fine-tuned smaller LM** instead of the original large LM.\n",
        "* You can **save the fine-tuned model**:\n",
        "\n",
        "  * To **disk** (in Colab or local)\n",
        "  * To the **Hugging Face Hub** for reuse and deployment\n",
        "\n",
        "To make it your default model:\n",
        "\n",
        "```python\n",
        "dspy.configure(lm=distilled_program.lm)\n",
        "```\n",
        "\n",
        "Or, you can call it directly:\n",
        "\n",
        "```python\n",
        "distilled_program(input)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Evaluating the Fine-Tuned Model\n",
        "\n",
        "After fine-tuning, it’s important to **evaluate** the new model on a test set to confirm its performance.\n",
        "\n",
        "* In many cases, **medium-sized models** (like T5 or LLaMA 7B), when fine-tuned via DSPy, can **rival much larger models** in task accuracy.\n",
        "* This delivers **near state-of-the-art performance** at a **fraction of the runtime cost**.\n",
        "\n",
        "> 🎯 For production, this means high-quality results **without expensive API calls or large infrastructure** requirements.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔁 Workflow Summary\n",
        "\n",
        "Here’s a simplified view of DSPy’s fine-tuning flow:\n",
        "\n",
        "1. **Optimize prompts**\n",
        "2. **Generate synthetic data (from a teacher model)**\n",
        "3. **Fine-tune a smaller model**\n",
        "4. **Swap in the smaller model**\n",
        "5. *(Optional)* **Repeat / Iterate for further gains**\n",
        "\n",
        "This workflow is valuable for both:\n",
        "\n",
        "* 🧑‍🔬 **Academic research** – to explore how compact models can match larger models\n",
        "* 🏭 **Production systems** – to reduce cost and improve deployment efficiency\n",
        "\n",
        "> 🧪 According to the [DSPy paper](https://openreview.net/forum?id=sY5N0zY5Od), a **770M T5** model and a **13B LLaMA** model, when compiled and fine-tuned using DSPy, **matched or outperformed** GPT-3.5 on complex tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### 🚀 Why This Matters\n",
        "\n",
        "By **distilling knowledge from large models** into smaller, task-specific ones:\n",
        "\n",
        "* You preserve performance\n",
        "* Lower costs dramatically\n",
        "* Maintain flexibility and control\n",
        "\n",
        "DSPy makes this process structured, automated, and reproducible—making advanced model deployment more accessible than ever.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2VmVLVTUXbn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Integration with Top-Performing LLM Backends (OpenAI, Hugging Face, etc.)\n",
        "DSPy is designed to be backend-agnostic, meaning you can plug in different language model providers or libraries, and your DSPy program code stays the same. This is great for experimenting with which LLM works best for your needs, or switching from a proprietary API to an open-source model for cost reasons. Let’s discuss how to integrate DSPy with some popular LLM backends and the considerations for each:\n",
        "\n",
        "OpenAI (GPT-3.5, GPT-4): These models are currently among the top-performing LLMs for many tasks. In DSPy, using an OpenAI model is as simple as naming it in dspy.LM. For example:"
      ],
      "metadata": {
        "id": "MCTnONg0Zren"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lm = dspy.LM(\"huggingface/google/flan-t5-base\")\n",
        "dspy.configure(lm=lm)\n"
      ],
      "metadata": {
        "id": "R6DyvxZGXbsm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Here's your content beautifully rewritten in **Markdown** format for clarity and elegance:\n",
        "\n",
        "---\n",
        "\n",
        "## 🌐 Using Hugging Face and Other Model Backends with DSPy\n",
        "\n",
        "### 🤗 Hugging Face Transformers: Local Inference in Colab\n",
        "\n",
        "You can use the **Hugging Face Transformers** library to load models like `google/flan-t5-base`—a smaller, instruction-tuned model—for local inference in Google Colab.\n",
        "\n",
        "You’re not limited to this model: simply swap the model name with any available model on Hugging Face, such as:\n",
        "\n",
        "* `huggingface/meta-llama/Llama-2-7b-chat-hf` *(if your hardware supports it)*\n",
        "\n",
        "**Benefits of using open models in Colab:**\n",
        "\n",
        "* 💸 **Free** to use\n",
        "* 🔒 **Private** – your data stays in the Colab instance\n",
        "* 🛠️ **Fully controllable** – fine-tune and customize as needed\n",
        "\n",
        "**Things to keep in mind:**\n",
        "\n",
        "* Open models may be **less powerful** than commercial models like GPT-4\n",
        "* You may need **prompt engineering** or **fine-tuning** to reach your desired quality\n",
        "* DSPy’s **optimizers** can help teach smaller models using outputs from larger ones or a modest dataset\n",
        "\n",
        "**Hardware Note:**\n",
        "Colab’s T4 GPUs can typically handle models up to **\\~13B parameters in 16-bit mode**. For larger models, consider **8-bit quantization** or choose a smaller model for better results.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔐 Other Providers: Anthropic Claude, Google PaLM, and More\n",
        "\n",
        "Providers like **Anthropic** and **Google** offer high-quality models via API. DSPy supports these through the same `dspy.LM` interface by specifying models like:\n",
        "\n",
        "* `anthropic/claude-2`\n",
        "* `gemini/<model>`\n",
        "\n",
        "**Claude**, in particular, is known for its **long-context understanding** and **thoughtful responses**. These models perform similarly to OpenAI’s offerings in terms of quality and considerations like cost and privacy.\n",
        "\n",
        "---\n",
        "\n",
        "### 🏠 Local Model Servers and Community LLMs\n",
        "\n",
        "DSPy also integrates with **local serving solutions**, such as:\n",
        "\n",
        "* **Ollama server**\n",
        "* **Hugging Face’s Text Generation Inference**\n",
        "* **Databricks models**\n",
        "* Other community-hosted LLMs\n",
        "\n",
        "If the model exposes an endpoint following a known protocol (OpenAI-style or Hugging Face-style), DSPy can connect using just the `api_base` and `model_type` parameters.\n",
        "\n",
        "> **Flexibility in action:** You can switch from GPT-4 to a local LLaMA2 by changing a **single line of code**, while keeping your DSPy pipeline and optimizations intact.\n",
        "\n",
        "This is a **huge advantage for production**: prototype with a premium model, then migrate to a more affordable, self-hosted one—all without reworking your architecture.\n",
        "\n",
        "---\n",
        "\n",
        "### 🤔 Choosing the Right Backend: Quality vs. Control\n",
        "\n",
        "Your choice of backend depends on your priorities:\n",
        "\n",
        "#### ✅ Use commercial models like GPT-4 or Claude if:\n",
        "\n",
        "* You need **highest quality**\n",
        "* The task is **complex or nuanced**\n",
        "* You want fast results **out-of-the-box**\n",
        "\n",
        "#### 💡 Use open-source models via Hugging Face if:\n",
        "\n",
        "* You're **cost-conscious**\n",
        "* You're working with **sensitive data**\n",
        "* You prefer **no external dependencies**\n",
        "\n",
        "With DSPy’s optimizers and a small set of examples—or outputs from a teacher model—you can bring smaller open models close to big model performance for **your specific use case**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔁 Hybrid Strategies: Best of Both Worlds\n",
        "\n",
        "A **hybrid approach** often works best:\n",
        "\n",
        "1. Use **GPT-4 or Claude** to generate labeled datasets or reference outputs.\n",
        "2. Fine-tune a **local Hugging Face model** using DSPy’s optimizers.\n",
        "3. Deploy with **cost efficiency and full control**.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚙️ Practical Considerations\n",
        "\n",
        "* **Latency**: Self-hosted models may be slower.\n",
        "* **Context length**: Open models may support shorter input windows.\n",
        "* **Scalability**: Hosted models are better suited for fast, high-volume responses.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔄 The DSPy Advantage: Backend Agnostic by Design\n",
        "\n",
        "DSPy doesn’t tie you to any single model. You can:\n",
        "\n",
        "* Swap backends with **minimal code changes**\n",
        "* Use **multiple models in one pipeline** (e.g., local model for retrieval, GPT-4 for answering)\n",
        "* Reuse your **logic and optimization work** regardless of backend\n",
        "\n",
        "In Colab, you’re free to test and explore different configurations. Just remember to manage:\n",
        "\n",
        "* Your **API keys**\n",
        "* Your **session state** – once you configure a new language model, it becomes the default for all DSPy operations unless changed again.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e2_5hS1BXbxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's your text formatted as **Markdown**:\n",
        "\n",
        "---\n",
        "\n",
        "## Example Use Cases and Practical Applications\n",
        "\n",
        "To ground everything we’ve covered, let’s explore some example use cases for **DSPy** and how it can be applied in practice. DSPy is a general framework, so the possibilities are broad – here are a few scenarios across academic, production, and learning contexts:\n",
        "\n",
        "---\n",
        "\n",
        "### 📘 Complex Question Answering (Academic/Research)\n",
        "\n",
        "Imagine you're researching multi-hop question answering (where a system must retrieve information from multiple sources to answer a question, like the [HotpotQA](https://hotpotqa.github.io) dataset). With DSPy, you can define a pipeline with:\n",
        "\n",
        "* A **retrieval module** (to fetch relevant passages)\n",
        "* A **reasoning module** (to synthesize an answer)\n",
        "\n",
        "Each module has a clear signature (e.g., retrieval: `question -> supporting paragraph`; reasoning: `question + context -> answer`). DSPy optimizers can then be used to:\n",
        "\n",
        "* Add few-shot examples to improve retrieval\n",
        "* Fine-tune smaller models for better synthesis\n",
        "\n",
        "> The [DSPy paper](https://openreview.net/forum?id=sY5N0zY5Od) demonstrated strong results on complex QA tasks.\n",
        "\n",
        "✅ **Benefit**: Faster iteration for researchers—focus on pipeline design while DSPy handles prompt tuning and model training.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Reliable AI Agents (Production)\n",
        "\n",
        "Consider building an AI assistant that uses tools (like search engines or calculators), commonly implemented with the **ReAct** (Reason+Act) prompt strategy. Normally, you'd hand-craft prompts, but with DSPy you can:\n",
        "\n",
        "* Break the agent into modules:\n",
        "\n",
        "  * Tool decision\n",
        "  * Tool output processing\n",
        "  * Final answer generation\n",
        "* Use DSPy templates like `ChainOfThought` or `ReAct`\n",
        "* Apply an optimizer with a custom metric (e.g., tool success rate)\n",
        "\n",
        "> This results in a robust agent loop that learns from mistakes—improving reliability and reducing failure cases.\n",
        "\n",
        "✅ **Benefit**: Higher success rates and fewer edge-case errors in production systems.\n",
        "\n",
        "> Reference: [Medium](https://medium.com), [DataCamp](https://datacamp.com)\n",
        "\n",
        "---\n",
        "\n",
        "### 🏢 Retrieval-Augmented Generation (Enterprise)\n",
        "\n",
        "A company wants to build a chatbot that answers questions based on proprietary documents. With DSPy, they can:\n",
        "\n",
        "* Create a pipeline:\n",
        "\n",
        "  * **Retrieval module** (e.g., using vector DBs)\n",
        "  * **Generation module** (to form answers)\n",
        "* Use optimizers to:\n",
        "\n",
        "  * Tune number of documents retrieved\n",
        "  * Improve how answers are generated and cited\n",
        "\n",
        "Over time, user interaction generates training data that fine-tunes the model, improving accuracy.\n",
        "\n",
        "✅ **Benefit**: Adaptable, self-improving pipelines with minimal human upkeep.\n",
        "\n",
        "> Ideal for LLMOps and enterprise-scale Q\\&A systems.\n",
        "\n",
        "---\n",
        "\n",
        "### 🎓 Educational Prompt Tuning (Learning/Teaching)\n",
        "\n",
        "Teaching a course on LLMs or learning prompt engineering? DSPy provides a great hands-on approach:\n",
        "\n",
        "* Start with a basic solution (e.g., summarization)\n",
        "* Define evaluation metrics (e.g., length, coverage)\n",
        "* Use an optimizer to iteratively improve the result\n",
        "\n",
        "This shows how prompt engineering can be **systematic and reproducible**.\n",
        "\n",
        "✅ **Benefit**: Teaches evaluation, prompt tuning, and fine-tuning in a controlled, scalable way.\n",
        "\n",
        "> Try it in Colab with open models for free.\n",
        "> Source: [OpenReview](https://openreview.net/forum?id=sY5N0zY5Od)\n",
        "\n",
        "---\n",
        "\n",
        "### 🛠️ Custom NLP Task Prototyping\n",
        "\n",
        "DSPy isn’t just for Q\\&A or chat. Examples of other tasks:\n",
        "\n",
        "* **Entity extraction**: `Text -> [Entities]`\n",
        "* **Content moderation**: `Text -> Label`\n",
        "\n",
        "You can use small prompts or models and refine them using DSPy optimizers. Great for:\n",
        "\n",
        "* Rapid prototyping\n",
        "* Improving performance without full fine-tuning\n",
        "\n",
        "✅ **Benefit**: Middle ground between prompt engineering and full-scale model training.\n",
        "\n",
        "> Reference: [Medium](https://medium.com)\n",
        "\n",
        "---\n",
        "\n",
        "## Common Theme\n",
        "\n",
        "Across all cases:\n",
        "\n",
        "* **Break down tasks into modules**\n",
        "* **Declare module signatures**\n",
        "* **Use DSPy optimizers to self-improve**\n",
        "\n",
        "✅ **Result**: Faster development, better accuracy, more robust behavior, and reduced manual work.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion and Next Steps\n",
        "\n",
        "In this tutorial, we covered:\n",
        "\n",
        "* Getting started with DSPy in Colab\n",
        "* Declarative programming with signatures and modules\n",
        "* Using optimizers for prompt tuning and model fine-tuning\n",
        "* Integrating with OpenAI, Hugging Face, and other LLMs\n",
        "* Practical applications: academic, production, education, prototyping\n",
        "\n",
        "---\n",
        "\n",
        "### 🚀 Next Steps\n",
        "\n",
        "**Explore the Official Documentation**\n",
        "\n",
        "* Tutorials, API references, RAG examples, and more\n",
        "  🔗 [DSPy Documentation](https://dspy.ai)\n",
        "\n",
        "**Join the Community**\n",
        "\n",
        "* GitHub repo and Discord available\n",
        "  🔗 [dspy.ai](https://dspy.ai)\n",
        "\n",
        "**Try Advanced Optimizers**\n",
        "\n",
        "* Explore: `BootstrapFewShotWithRandomSearch`, `MIPROv2`, `BootstrapFinetune`\n",
        "  🔗 [Pondhouse Data](https://pondhouse-data.com)\n",
        "\n",
        "**Combine with Other Tools**\n",
        "\n",
        "* Hugging Face Datasets\n",
        "* LanceDB for vector search\n",
        "* Evaluation libraries like 🦙 `lm-evaluation-harness` or 🤗 `Evaluate`\n",
        "\n",
        "---\n",
        "\n",
        "By adopting DSPy, you get a **maintainable, self-improving** approach to building LLM applications—future-proofing your work as models and use cases evolve.\n",
        "\n",
        "> **Happy coding – may your language model programs ever improve themselves!** 🎉\n",
        "\n",
        "---\n",
        "\n",
        "## References\n",
        "\n",
        "* Omar Khattab et al., *“DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines.”* ICLR 2024. [OpenReview](https://openreview.net/forum?id=sY5N0zY5Od)\n",
        "* Plaban Nayak, *“Declarative Self-improving Language Programs Pythonically.”* The AI Forum on Medium, Jun 2024. [Medium](https://medium.com)\n",
        "* DataCamp Team, *“What Is DSPy? How It Works, Use Cases, and Resources.”* [DataCamp](https://datacamp.com)\n",
        "* Nimrita Koul, *“DSPy — Programming with Language Models.”* [Medium](https://medium.com)\n",
        "* Pondhouse Data, *“Build Better AI Systems with Automated Prompt Optimization.”* [Pondhouse Blog](https://pondhouse-data.com)\n",
        "* [DSPy Documentation](https://dspy.ai)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fW3bAewiZ-cH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dspy\n",
        "import os\n",
        "\n",
        "# ===== Different LLM Backend Configurations =====\n",
        "\n",
        "# 1. OpenAI Configuration\n",
        "def setup_openai():\n",
        "    \"\"\"Configure DSPy to use OpenAI's models.\"\"\"\n",
        "    api_key = os.environ.get(\"OPENAI_API_KEY\", \"your-api-key-here\")\n",
        "\n",
        "    # Modern configuration with named models\n",
        "    openai_lm = dspy.LM(\"openai/gpt-4o-mini\", api_key=api_key)\n",
        "    dspy.settings.configure(lm=openai_lm)\n",
        "\n",
        "    print(\"Configured for OpenAI GPT-4o-mini\")\n",
        "    return openai_lm\n",
        "\n",
        "# 2. Hugging Face Configuration\n",
        "def setup_huggingface(model_name=\"google/flan-t5-base\"):\n",
        "    \"\"\"Configure DSPy to use Hugging Face models.\"\"\"\n",
        "    # For local inference (requires appropriate hardware)\n",
        "    hf_lm = dspy.LM(f\"huggingface/{model_name}\")\n",
        "    dspy.settings.configure(lm=hf_lm)\n",
        "\n",
        "    print(f\"Configured for Hugging Face model: {model_name}\")\n",
        "    return hf_lm\n",
        "\n",
        "# 3. Anthropic Configuration\n",
        "def setup_anthropic():\n",
        "    \"\"\"Configure DSPy to use Anthropic Claude models.\"\"\"\n",
        "    api_key = os.environ.get(\"ANTHROPIC_API_KEY\", \"your-api-key-here\")\n",
        "\n",
        "    # Modern configuration with named models\n",
        "    claude_lm = dspy.LM(\"anthropic/claude-3-5-sonnet\", api_key=api_key)\n",
        "    dspy.settings.configure(lm=claude_lm)\n",
        "\n",
        "    print(\"Configured for Anthropic Claude-3.5-Sonnet\")\n",
        "    return claude_lm\n",
        "\n",
        "# 4. Ollama Configuration (for local deployment)\n",
        "def setup_ollama(model_name=\"llama3.2\"):\n",
        "    \"\"\"Configure DSPy to use Ollama for local inference.\"\"\"\n",
        "    # Requires Ollama to be installed and running\n",
        "    # curl -fsSL https://ollama.ai/install.sh | sh\n",
        "    # ollama run llama3.2:1b\n",
        "    ollama_lm = dspy.LM(\"ollama/llama3.2\")\n",
        "    dspy.settings.configure(lm=ollama_lm)\n",
        "\n",
        "    print(f\"Configured for Ollama model: {model_name}\")\n",
        "    return ollama_lm\n",
        "\n",
        "# 5. Gemini Configuration\n",
        "def setup_gemini():\n",
        "    \"\"\"Configure DSPy to use Google's Gemini models.\"\"\"\n",
        "    api_key = os.environ.get(\"GOOGLE_API_KEY\", \"your-api-key-here\")\n",
        "\n",
        "    gemini_lm = dspy.LM(\"gemini/gemini-pro\", api_key=api_key)\n",
        "    dspy.settings.configure(lm=gemini_lm)\n",
        "\n",
        "    print(\"Configured for Google Gemini-Pro\")\n",
        "    return gemini_lm\n",
        "\n",
        "# 6. Databricks Configuration\n",
        "def setup_databricks():\n",
        "    \"\"\"Configure DSPy to use Databricks models.\"\"\"\n",
        "    # Automatic authentication on Databricks platform\n",
        "    # Or set DATABRICKS_API_KEY and DATABRICKS_API_BASE\n",
        "    databricks_lm = dspy.LM(\"databricks/dbrx-instruct\")\n",
        "    dspy.settings.configure(lm=databricks_lm)\n",
        "\n",
        "    print(\"Configured for Databricks DBRX-Instruct\")\n",
        "    return databricks_lm\n",
        "\n",
        "# Example of testing each configuration (uncomment to try)\n",
        "# setup_openai()\n",
        "# test_qa = dspy.Predict(\"question: str -> answer: str\")\n",
        "# print(test_qa(question=\"What is machine learning?\").answer)\n",
        "\n",
        "# Switch to a different provider\n",
        "# setup_anthropic()\n",
        "# print(test_qa(question=\"What is deep learning?\").answer)\n",
        "\n",
        "print(\"LLM Backend integration code is ready to use\")\n",
        "print(\"Uncomment the examples to test different providers\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0YHFAu0Xb3I",
        "outputId": "dcc1ff60-ec16-4c90-825b-5c86d4ef12e6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Backend integration code is ready to use\n",
            "Uncomment the examples to test different providers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dspy\n",
        "import os\n",
        "\n",
        "# Set up DSPy with a language model\n",
        "lm = dspy.LM(\"openai/gpt-4o-mini\")\n",
        "dspy.settings.configure(lm=lm)\n",
        "\n",
        "# Define a simple RAG signature for question answering\n",
        "class RAG(dspy.Signature):\n",
        "    \"\"\"Answer questions based on the retrieved context.\"\"\"\n",
        "    question = dspy.InputField()\n",
        "    context = dspy.InputField(desc=\"retrieved passages from a knowledge base\")\n",
        "    answer = dspy.OutputField(desc=\"a detailed answer based on the context\")\n",
        "\n",
        "# Define a retrieval module signature\n",
        "class Retrieve(dspy.Signature):\n",
        "    \"\"\"Retrieve relevant passages for a question.\"\"\"\n",
        "    question = dspy.InputField()\n",
        "    passages = dspy.OutputField(desc=\"retrieved passages relevant to the question\")\n",
        "\n",
        "# Create a simple RAG pipeline\n",
        "class SimplifiedRAG(dspy.Module):\n",
        "    def __init__(self, num_passages=3):\n",
        "        super().__init__()\n",
        "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
        "        self.generate_answer = dspy.Predict(RAG)\n",
        "\n",
        "    def forward(self, question):\n",
        "        # Step 1: Retrieve relevant passages\n",
        "        retrieved = self.retrieve(question)\n",
        "\n",
        "        # Step 2: Generate answer based on retrieved context\n",
        "        context = \"\\n\\n\".join(retrieved.passages)\n",
        "        prediction = self.generate_answer(question=question, context=context)\n",
        "\n",
        "        return dspy.Prediction(answer=prediction.answer)\n",
        "\n",
        "# Set up a retrieval model (for example purposes)\n",
        "# In actual implementation, you'd use a real retrieval model\n",
        "print(\"Setting up a mock retrieval model for example purposes\")\n",
        "print(\"In practice, you would configure with:\")\n",
        "print(\"retriever = dspy.ColBERTv2(url='http://example.com/colbert')\")\n",
        "print(\"dspy.settings.configure(rm=retriever)\")\n",
        "\n",
        "# For this example, we're using a mock retriever\n",
        "class MockRetriever:\n",
        "    def __init__(self):\n",
        "        self.db = {\n",
        "            \"machine learning\": [\n",
        "                \"Machine learning is a branch of artificial intelligence that focuses on building systems that learn from data.\",\n",
        "                \"In machine learning, algorithms are trained on data to make predictions or decisions without being explicitly programmed.\",\n",
        "                \"Common machine learning approaches include supervised learning, unsupervised learning, and reinforcement learning.\"\n",
        "            ],\n",
        "            \"python programming\": [\n",
        "                \"Python is a high-level, interpreted programming language known for its readability and simplicity.\",\n",
        "                \"Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming.\",\n",
        "                \"Python has a large standard library and ecosystem of third-party packages for various applications.\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def search(self, query, k=3):\n",
        "        # Very simplified retrieval logic\n",
        "        for key, passages in self.db.items():\n",
        "            if key in query.lower():\n",
        "                return passages[:k]\n",
        "        return [\"No relevant information found.\"]\n",
        "\n",
        "# Register our mock retriever\n",
        "mock_rm = MockRetriever()\n",
        "dspy.settings.configure(rm=mock_rm)\n",
        "\n",
        "# Override the retrieve module to use our mock retriever\n",
        "def my_retrieve(self, question):\n",
        "    passages = mock_rm.search(question)\n",
        "    return dspy.Prediction(passages=passages)\n",
        "\n",
        "dspy.Retrieve.forward = my_retrieve\n",
        "\n",
        "# Create an instance of our RAG pipeline\n",
        "rag_pipeline = SimplifiedRAG(num_passages=2)\n",
        "\n",
        "# Test the RAG pipeline\n",
        "question = \"Explain the basics of machine learning\"\n",
        "result = rag_pipeline(question)\n",
        "print(f\"\\nQuestion: {question}\")\n",
        "print(f\"Answer: {result.answer}\")\n",
        "\n",
        "# Test with another question\n",
        "question = \"What is Python programming good for?\"\n",
        "result = rag_pipeline(question)\n",
        "print(f\"\\nQuestion: {question}\")\n",
        "print(f\"Answer: {result.answer}\")\n",
        "\n",
        "# Optimizing the RAG pipeline\n",
        "print(\"\\nIn a real application, you would optimize the RAG pipeline with:\")\n",
        "print(\"\"\"\n",
        "# Define a suitable metric\n",
        "def rag_metric(example, pred):\n",
        "    # Measure correctness and relevance of the answer\n",
        "    correctness = ... # Logic to evaluate correctness\n",
        "    relevance = ... # Logic to evaluate relevance\n",
        "    return (correctness + relevance) / 2\n",
        "\n",
        "# Create evaluation dataset\n",
        "dev_set = [\n",
        "    Example(question=\"What is machine learning?\", answer=\"Machine learning is...\"),\n",
        "    ...\n",
        "]\n",
        "\n",
        "# Optimize with MIPROv2 or another suitable optimizer\n",
        "optimizer = dspy.MIPROv2(metric=rag_metric)\n",
        "optimized_rag = optimizer.compile(rag_pipeline, trainset=dev_set)\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nFor a complete RAG implementation with DSPy, refer to the documentation:\")\n",
        "print(\"https://dspy.ai/tutorials/rag/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joQcH10ymOjz",
        "outputId": "cfbf1e94-b5b4-4800-fe7c-0beb3bfe379e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up a mock retrieval model for example purposes\n",
            "In practice, you would configure with:\n",
            "retriever = dspy.ColBERTv2(url='http://example.com/colbert')\n",
            "dspy.settings.configure(rm=retriever)\n",
            "\n",
            "Question: Explain the basics of machine learning\n",
            "Answer: Machine learning is a subset of artificial intelligence that emphasizes the development of algorithms and models that enable computers to learn from and make predictions based on data. The fundamental idea is to allow systems to improve their performance on a specific task over time without being explicitly programmed for each scenario.\n",
            "\n",
            "There are three primary types of machine learning approaches:\n",
            "\n",
            "1. **Supervised Learning**: In this approach, the algorithm is trained on a labeled dataset, meaning that each training example is paired with an output label. The model learns to map inputs to the correct outputs, which allows it to make predictions on new, unseen data.\n",
            "\n",
            "2. **Unsupervised Learning**: Unlike supervised learning, unsupervised learning deals with unlabeled data. The algorithm tries to identify patterns or groupings within the data without any prior knowledge of the outcomes. Common techniques include clustering and dimensionality reduction.\n",
            "\n",
            "3. **Reinforcement Learning**: This approach involves training algorithms to make a sequence of decisions by rewarding them for good actions and penalizing them for bad ones. The model learns to optimize its strategy over time based on the feedback it receives from the environment.\n",
            "\n",
            "Overall, machine learning enables systems to adapt and improve autonomously, making it a powerful tool in various applications, from image recognition to natural language processing and beyond.\n",
            "\n",
            "Question: What is Python programming good for?\n",
            "Answer: Python programming is good for a wide range of applications due to its versatility and ease of use. It is particularly well-suited for web development, data analysis, artificial intelligence, scientific computing, automation, and scripting. The language's readability and simplicity make it an excellent choice for beginners, while its support for multiple programming paradigms allows experienced developers to implement complex solutions efficiently. Additionally, Python's extensive standard library and a rich ecosystem of third-party packages enable developers to quickly build and deploy applications across various domains.\n",
            "\n",
            "In a real application, you would optimize the RAG pipeline with:\n",
            "\n",
            "# Define a suitable metric\n",
            "def rag_metric(example, pred):\n",
            "    # Measure correctness and relevance of the answer\n",
            "    correctness = ... # Logic to evaluate correctness\n",
            "    relevance = ... # Logic to evaluate relevance\n",
            "    return (correctness + relevance) / 2\n",
            "\n",
            "# Create evaluation dataset\n",
            "dev_set = [\n",
            "    Example(question=\"What is machine learning?\", answer=\"Machine learning is...\"),\n",
            "    ...\n",
            "]\n",
            "\n",
            "# Optimize with MIPROv2 or another suitable optimizer\n",
            "optimizer = dspy.MIPROv2(metric=rag_metric)\n",
            "optimized_rag = optimizer.compile(rag_pipeline, trainset=dev_set)\n",
            "\n",
            "\n",
            "For a complete RAG implementation with DSPy, refer to the documentation:\n",
            "https://dspy.ai/tutorials/rag/\n"
          ]
        }
      ]
    }
  ]
}