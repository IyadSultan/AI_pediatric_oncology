{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMH/w01eaGMu5kfcgc04hWa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IyadSultan/educational/blob/main/Hugging_Face_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face Transformers"
      ],
      "metadata": {
        "id": "-GUpdQJyiQZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "In this session, we will explore how to use Hugging Face Transformers to perform Named Entity Recognition (NER) on medical text. This tutorial is designed for pediatric oncologists and healthcare professionals with minimal coding experience, and it builds on a previous workshop where we covered Python basics and the OpenAI API. Here, we'll shift focus to open-source transformer models that run in our environment (Google Colab) without requiring an external API.\n",
        "\n",
        "What is NER?\n",
        "\n",
        "It's a technique to automatically identify and classify key terms in text (entities) into categories like diseases, medications, symptoms, etc. For example, in a clinical note, an NER model might highlight a disease name or a drug.\n",
        "\n",
        "By the end of this tutorial, you'll know how to:\n",
        "\n",
        "- Set up and install the Hugging Face Transformers library in Colab.\n",
        "- Load a pre-trained medical NER model from Hugging Face (no training required).\n",
        "- Tokenize and run the model on synthetic pediatric oncology notes to extract entities.\n",
        "- Build a simple interactive UI with Streamlit to input text and see highlighted entities.\n",
        "- Understand real-world applications of such tools (classification, NER, embeddings) in clinical settings, and how clinicians can collaborate with data scientists to prototype AI solutions.\n",
        "\n",
        "\n",
        "Let's get started!"
      ],
      "metadata": {
        "id": "cDEEV17xibbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setting Up the Colab Environment\n",
        "First, we'll ensure the required libraries are installed. We need the Transformers library (from Hugging Face) and Streamlit for the UI part. In a Colab notebook, you can install packages using pip. Run the following cell to install Transformers and Streamlit:"
      ],
      "metadata": {
        "id": "CdBqsKmfi1EF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRfr52HrTN9d",
        "outputId": "ba21a1a2-b2c2-46fe-b884-c993670c7261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.44.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.35.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.44.1-py3-none-any.whl (9.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.44.1 watchdog-6.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running pip install transformers will fetch the Hugging Face Transformers library, which provides access to state-of-the-art Transformer models for NLP tasks (including NER). Similarly, streamlit will be installed for building our web interface. After this installation, we can import the libraries in Python."
      ],
      "metadata": {
        "id": "-7MqdCtIi8Zd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Importing the Transformers Library"
      ],
      "metadata": {
        "id": "e1Ja9fh6jBb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the libraries installed, we import the necessary classes and functions from Transformers. We'll use Hugging Face's high-level pipeline API as well as some specific classes for tokenization and modeling:"
      ],
      "metadata": {
        "id": "oro7VPuEiRIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification"
      ],
      "metadata": {
        "id": "JiEeVQYDiRnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**pipeline** is an easy-to-use function that abstracts away a lot of the complexity behind running models​. It gives a simple interface for various tasks like NER, text generation, etc. (We'll use it for NER in this tutorial).\n",
        "\n",
        "As the Hugging Face docs note: “The pipelines are a great and easy way to use models for inference... offering a simple API dedicated to several tasks, including Named Entity Recognition.”​. In other words, pipelines let us apply a model in one line of code without deep knowledge of the model’s internals​.\n",
        "\n",
        "**AutoTokenizer** and **AutoModelForTokenClassification** are classes that automatically load the appropriate tokenizer and model architecture for a given model name. We use these to get the components for our NER model."
      ],
      "metadata": {
        "id": "MJkIQ-HJiRtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Loading a Pretrained Medical NER Model\n",
        "\n",
        "Hugging Face hosts thousands of pretrained models on their Hub. We will use a general biomedical NER model called d4data/biomedical-ner-all as an example. This model was trained to recognize a wide range of biomedical entities (it can identify 107 different types of entities in medical text​!). These include categories like medical conditions, medications, procedures, demographic info, etc., making it suitable for clinical notes​. The great thing is we don’t need to train anything ourselves – we can load this pretrained model directly.\n",
        "\n",
        "Let's load the model and tokenizer, then wrap them in a pipeline for NER:"
      ],
      "metadata": {
        "id": "thIpv8zhjcnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"d4data/biomedical-ner-all\"\n",
        "\n",
        "# Load tokenizer and model from Hugging Face Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "# Create an NER pipeline with the model, using an aggregation strategy to group tokens into entities\n",
        "ner_pipeline = pipeline(\n",
        "    \"ner\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    aggregation_strategy=\"first\"  # 'simple' will group contiguous tokens into single entities\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AQPuu1aiRzw",
        "outputId": "05ef5c7c-72a9-4706-e706-3e3d515117d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When this code runs, it will download the model weights and tokenizer for **\"d4data/biomedical-ner-all\"** from Hugging Face.\n",
        "\n",
        "Under the hood:\n",
        "The **tokenizer** is responsible for preprocessing text (splitting text into tokens and converting them to numeric IDs the model understands).\n",
        "\n",
        "The model is a transformer (DistilBERT base, in this case) fine-tuned for token classification (NER task).\n",
        "\n",
        "The pipeline with **aggregation_strategy=\"simple\"** means the pipeline will merge tokens that belong to the same entity, giving us whole entity spans instead of raw token-by-token output."
      ],
      "metadata": {
        "id": "63MDh-SPiR5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner_pipeline = pipeline(\"ner\", model=model_name, aggregation_strategy=\"first\")  # try \"simple\", \"average\", \"max\" or \"first\" as an aggregation_strategy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdkqvLymiR-W",
        "outputId": "6138bc48-21bc-4060-97a3-808bece61580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This : would load the model and tokenizer internally by name. We showed the longer form above for clarity."
      ],
      "metadata": {
        "id": "g58EGDUTkRz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Tokenizing and Running NER on Synthetic Text\n",
        "Now that we have our **ner_pipeline**, let's test it on some example medical text. We'll create a synthetic clinical note inspired by pediatric oncology. It's important to use fake but realistic examples – so we ensure no real patient data is used, but the content resembles what a doctor might write. Example Clinical Note:\n",
        "\n",
        "\"*Patient is a 7-year-old boy with acute lymphoblastic leukemia (ALL) who presents with a two-week history of fever, bone pain, and fatigue. On exam, noted pallor and bruising. Plan is to start induction chemotherapy with vincristine, prednisone, and L-asparaginase.*\"\n",
        "\n",
        "This text contains various medical entities (age, disease name, symptoms, physical findings, treatment plan, drug names). Let's see if the model can identify these. We will run the pipeline on this text and examine the output:"
      ],
      "metadata": {
        "id": "LX0RdYqBiSNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Synthetic pediatric oncology note\n",
        "text = (\"Patient is a 7-year-old boy with acute lymphoblastic leukemia (ALL) who presents with \"\n",
        "        \"a two-week history of fever, bone pain, and fatigue. On exam, noted pallor and bruising. \"\n",
        "        \"Plan is to start induction chemotherapy with vincristine, prednisone, and L-asparaginase.\")\n",
        "\n",
        "# Use the NER pipeline on the text\n",
        "entities = ner_pipeline(text)\n",
        "\n",
        "print(\"Entities found:\", len(entities))\n",
        "for ent in entities:\n",
        "    print(ent)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTk7Ycd6iSS-",
        "outputId": "62843958-2b69-409d-e79b-5b2f1afa9975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities found: 11\n",
            "{'entity_group': 'Age', 'score': np.float32(0.99662447), 'word': '7 - year - old', 'start': 13, 'end': 23}\n",
            "{'entity_group': 'Sex', 'score': np.float32(0.9985592), 'word': 'boy', 'start': 24, 'end': 27}\n",
            "{'entity_group': 'Detailed_description', 'score': np.float32(0.99693465), 'word': 'acute', 'start': 33, 'end': 38}\n",
            "{'entity_group': 'Disease_disorder', 'score': np.float32(0.9993123), 'word': 'lymphoblastic leukemia', 'start': 39, 'end': 61}\n",
            "{'entity_group': 'Duration', 'score': np.float32(0.9950433), 'word': 'two - week', 'start': 88, 'end': 96}\n",
            "{'entity_group': 'Biological_structure', 'score': np.float32(0.90964204), 'word': 'bone', 'start': 115, 'end': 119}\n",
            "{'entity_group': 'Sign_symptom', 'score': np.float32(0.99826247), 'word': 'pallor', 'start': 154, 'end': 160}\n",
            "{'entity_group': 'Medication', 'score': np.float32(0.9997482), 'word': 'chemotherapy', 'start': 202, 'end': 214}\n",
            "{'entity_group': 'Medication', 'score': np.float32(0.56305647), 'word': 'vincristine', 'start': 220, 'end': 231}\n",
            "{'entity_group': 'Medication', 'score': np.float32(0.9966965), 'word': 'prednisone', 'start': 233, 'end': 243}\n",
            "{'entity_group': 'Medication', 'score': np.float32(0.9012441), 'word': 'l', 'start': 249, 'end': 250}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the model found entities like \"7-year-old\" (categorized as Age), \"acute lymphoblastic leukemia\" (Disease), \"fever\", \"bone pain\", \"fatigue\" (all Symptoms), \"induction chemotherapy\" (Treatment), and the medications \"vincristine\", \"prednisone\", \"L-asparaginase\". This is great – without any manual coding of rules, the model identified key medical terms in the text and classified them.\n",
        "\n",
        "**How does this work? **\n",
        "The pipeline handled everything:\n",
        "- It tokenized the input (for example, splitting \"acute lymphoblastic leukemia\" into tokens like \"acute\", \"lymphoblastic\", \"leukemia\"). You can actually see the tokens by running tokenizer.tokenize(text) if curious.\n",
        "- The tokens were fed into the model, which is a neural network that output a predicted label for each token (like B-Disease, I-Disease for beginning/inside of a disease name, etc.).\n",
        "- The pipeline then aggregated those token-level labels into whole entity spans (that's why we got a single dictionary covering \"acute lymphoblastic leukemia\" as one entity, instead of three separate tokens). This makes the output easier to interpret.\n",
        "Hugging Face provides the model’s predictions in that convenient list-of-entities format, which we can now use for downstream purposes – such as highlighting these entities in the original text.\n",
        "\n"
      ],
      "metadata": {
        "id": "nksbDAeaiSXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Building a Simple Streamlit UI for NER\n",
        "To make our demo interactive, let's build a small web interface using Streamlit. Streamlit allows us to create a user interface (with text boxes, buttons, etc.) for our Python code easily. Participants can paste in a clinical note and click a button to see the NER results with highlights, which is more engaging than just printing raw output.\n",
        "\n",
        "We'll create a Streamlit app that does the following:\n",
        "- Provides a text area for inputting (or editing) a clinical note.\n",
        "- When a button is clicked, runs the ner_pipeline on the input text.\n",
        "- Displays the input text with the recognized entities highlighted (e.g., with a background color).\n",
        "\n",
        "In Colab, we will write the Streamlit app to a Python file (app.py) and then run it. Use the %%writefile magic to create the file:"
      ],
      "metadata": {
        "id": "HMEc6ILuoXD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the same NER pipeline inside the app (this will use the model we downloaded)\n",
        "ner_pipeline = pipeline(\"ner\", model=\"d4data/biomedical-ner-all\", aggregation_strategy=\"simple\")\n",
        "\n",
        "# Streamlit UI layout\n",
        "st.title(\"Clinical NER Demo\")\n",
        "st.markdown(\"Enter a synthetic clinical note and **extract entities** using a pretrained Transformer model:\")\n",
        "\n",
        "# A text area for input\n",
        "default_text = (\"Patient is a 7-year-old boy with acute lymphoblastic leukemia (ALL) who presents with \"\n",
        "                \"a two-week history of fever, bone pain, and fatigue. On exam, noted pallor and bruising. \"\n",
        "                \"Plan is to start induction chemotherapy with vincristine, prednisone, and L-asparaginase.\")\n",
        "user_input = st.text_area(\"Clinical Note\", default_text, height=150)\n",
        "\n",
        "# Button to run NER\n",
        "if st.button(\"Extract Entities\"):\n",
        "    # Run the NER pipeline on the input text\n",
        "    entities = ner_pipeline(user_input)\n",
        "    # Highlight the entities in the text by wrapping them with HTML <mark> tag\n",
        "    highlighted_text = user_input\n",
        "    # Insert the highlight tags in reverse order of indices (to not mess up positions as we insert)\n",
        "    for ent in sorted(entities, key=lambda x: x['start'], reverse=True):\n",
        "        start, end = ent['start'], ent['end']\n",
        "        highlighted_text = (highlighted_text[:start]\n",
        "                             + f\"<mark>{highlighted_text[start:end]}</mark>\"\n",
        "                             + highlighted_text[end:])\n",
        "    # Display the highlighted text. 'unsafe_allow_html=True' lets us render the <mark> tags.\n",
        "    st.write(\"**Extracted Entities Highlighted:**\")\n",
        "    st.markdown(highlighted_text, unsafe_allow_html=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-uIQHRgiSbt",
        "outputId": "7af43ae0-ae05-4bf3-94aa-2de62af18470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s break down the code above:\n",
        "- We import Streamlit and our Transformers pipeline. Inside the Streamlit script, we instantiate ner_pipeline again. (When running inside the app, it needs its own copy of the model. This will use the same model name, downloading it if not already cached.)\n",
        "- We define the app title and a description using st.title and st.markdown for some instructions.\n",
        "- We use st.text_area to provide a multi-line text input. We even pre-fill it with our example note so users can see an example. They can edit this or replace with their own example. (The height is set to 150 pixels just to make it a bit larger.)\n",
        "- We have an st.button(\"Extract Entities\"). When the button is clicked, the code under the if block runs:\n",
        "  - It calls the NER pipeline on whatever text is in user_input.\n",
        "  - It then goes through the list of entities and inserts HTML tags around each entity span in the text. The tag by default highlights text with a yellow background.\n",
        "  - We sort the entities by their start index in reverse order because if we insert tags from the end of the string towards the beginning, we don't disturb the character positions of entities that come earlier in the text.\n",
        "  - Finally, we display the modified highlighted_text using st.markdown with unsafe_allow_html=True (this flag is needed to render raw HTML in Streamlit, in this case to apply the highlight). We also label it with a subheader \"Extracted Entities Highlighted:\" for clarity.\n",
        "\n",
        "That’s it! This simple app will take the input text and show you the same text with identified entities highlighted in yellow. For instance, \"acute lymphoblastic leukemia\" would be highlighted as soon as you hit \"Extract Entities,\" confirming the model spotted it as a disease.\n",
        "\n",
        "\n",
        "**Running the Streamlit App in Colab**\n",
        "\n",
        "Now that we have **app.py**, we need to run the Streamlit server and make it accessible. Colab doesn’t show Streamlit apps by default (since it’s basically a separate web server). However, we can use a tool called LocalTunnel to get a public URL for our app.\n",
        "\n",
        "We will ask colab to generate the code to:\n",
        "- Install LocalTunnel (a utility to expose localhost ports to the web),\n",
        "- Launch the Streamlit app in the background, and\n",
        "- Create a tunnel to port 8501 (Streamlit’s default port) so we can access it."
      ],
      "metadata": {
        "id": "fNnq44-DiSgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install -q localtunnel\n",
        "!streamlit run app.py & npx localtunnel --port 8501\n",
        "# you need to run the next cell to get the tunnel password\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7zKkzMiiSlH",
        "outputId": "70412c18-1e3d-4354-a295-c403a8152322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\n",
            "up to date, audited 23 packages in 701ms\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\n",
            "2 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.48.156.56:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://grumpy-bags-behave.loca.lt\n",
            "2025-04-25 11:36:12.341357: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745580972.379478   13318 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745580972.388404   13318 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Device set to use cpu\n",
            "2025-04-25 11:36:17.541 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "Device set to use cpu\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://loca.lt/mytunnelpassword\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmYd_w0gqb1k",
        "outputId": "189b5515-7309-419e-bd80-ecf70870459c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.48.156.56"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will do a few things:\n",
        "\n",
        "The first line installs localtunnel via Node’s package manager (npm). The -q flag just quiets the output.\n",
        "\n",
        "The second line runs the Streamlit app (app.py). The & symbol makes it run in the background, and then npx localtunnel --port 8501 starts the tunneling. After a moment, you should see an output in the cell with a URL (usually ending with *.loca.lt). This URL is an external address that tunnels into the Colab environment on port 8501​.\n",
        "\n",
        "👉 Click the URL that appears (it will look similar to https://warm-mouse-1234.loca.lt or some random words). This will open a new page with your Streamlit app. You should see the title \"Clinical NER Demo\" and the example text pre-filled. Hit the \"Extract Entities\" button, and after a second, the text will display with highlighted entities!\n",
        "\n",
        "Try editing the text to other scenarios (e.g., change the symptoms or disease, or add a new sentence) and click the button again to see how the model performs.\n",
        "\n",
        "*If the localtunnel URL doesn’t show or connect on the first try, you might need to rerun the cell. Occasionally the tunnel fails to establish on the first attempt.*\n",
        "\n",
        "Streamlit makes it easy to build such prototype apps. In a real setting, you or your data science collaborators could deploy similar apps to allow clinicians to test out models on their own examples."
      ],
      "metadata": {
        "id": "MyuZqyQDiSpu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion: Real-World Applications and Next Steps"
      ],
      "metadata": {
        "id": "nguB-TyPr_Ml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we demonstrated how even those with minimal coding experience can leverage powerful NLP models for clinical text. We installed a Transformers model and built a mini application that identifies medical entities in text – all within a short session. This kind of non-generative AI model (one that extracts or classifies information rather than generating free-form text) can be very useful in healthcare. Here are some real-world clinical challenges where these tools can help:\n",
        "- Information Extraction (NER in practice): Just as we saw, NER can pull out critical details from unstructured text. For example, it can automatically scan pathology reports or clinical notes to find mentions of diseases, medications, allergies, or symptoms. This could help populate fields in an electronic health record or alert a physician to key findings in a long narrative. NER algorithms have been used to extract patient names, medical conditions, prescription names, and more from clinical text​, saving time on manual data entry.\n",
        "- Clinical Text Classification: Another common task is classification. For instance, categorizing patient notes or messages by urgency or by category (e.g., classifying radiology reports as showing cancer vs. no cancer, or triaging emails into appointment requests, medication queries, etc.). In pediatrics oncology, you might classify notes based on disease type or complication versus routine check-up. Machine learning models (like fine-tuned transformers or simpler algorithms with embeddings) can learn to assign labels to texts. For example, entire documents like discharge summaries can be classified into categories (diagnoses, specialties, outcomes) to help organize and triage information​. This helps healthcare professionals make decisions faster by organizing data.\n",
        "- Embeddings and Semantic Search: Beyond direct classification, transformer models can convert clinical text into embeddings (numerical vector representations) that capture the meaning. These embeddings enable semantic similarity comparisons. What’s the use? Imagine you have a complex patient case – you could find similar cases from the literature or past records by searching via embeddings. For example, you could encode a patient’s symptoms and lab findings, and quickly retrieve past cases or relevant research papers that are similar in semantic content (even if they don’t share exact keywords). This is a powerful way to do case-based reasoning or literature search. Researchers have shown that embedding models and vector databases can successfully be used to encode and classify medical text without training new task-specific models​. In practice, this could mean quicker access to relevant information for decision support.\n",
        "- Other Applications: There are many other non-generative NLP applications in healthcare. Prediction models can be built on text (e.g., predicting 30-day readmission risk from a discharge summary – which is essentially a classification/regression task on text). Clustering of notes via unsupervised learning could reveal patterns (maybe grouping patients by similar symptom profiles). Entity linking could be used to map extracted terms to standardized codes (like linking \"ALL\" to a specific ICD code or ontology term). All these tasks do not generate new text, but rather analyze or organize existing text, providing decision support while keeping the human in the loop.\n",
        "\n",
        "Finally, it's worth emphasizing the importance of collaboration. As a clinician, you bring the domain knowledge (you know what problems need solving, and you can interpret whether model output is useful or clinically valid). By partnering with data scientists or informaticians, you can build solutions that address those problems. Tools like the ones we explored enable rapid prototyping – in a single notebook, we went from an idea to a working demo. This is incredibly powerful in a clinical setting where iterating quickly on ideas can lead to impactful tools. In fact, many hospitals and clinics are now forming interdisciplinary teams to develop AI-driven prototypes for tasks like clinical documentation assistance and decision support, allowing clinicians to test and give feedback early in development.\n",
        "\n",
        "*Next Steps: If this session piqued your interest, you might explore further:*\n",
        "- Try fine-tuning a model like this on your own dataset (with a data scientist’s help) to better adapt to your hospital’s jargon.\n",
        "- Explore other Hugging Face Transformers tasks: e.g., question answering (on medical FAQs), or text summarization (summarizing a long report).\n",
        "- Learn about how to evaluate these models’ performance on clinical data (accuracy, errors, etc.) and ensure they meet healthcare standards for reliability.\n",
        "\n",
        "We hope this tutorial showed that modern NLP tools are within reach even if you’re new to coding. With a bit of practice, you can begin to prototype AI solutions that address everyday challenges in pediatric oncology and beyond. Happy experimenting!"
      ],
      "metadata": {
        "id": "PozyXY4Nscgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us try to modify the code above:  not only highlight the text identified by NER but color code it according to its category."
      ],
      "metadata": {
        "id": "ClDOVqC6iS0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Let us try to modify the code : not only highlight the text identified by NER but color code it according to its category.\n",
        "# Cover all entities and provide color coding legend on the bottom\n",
        "# We can see the model found entities like \"7-year-old\" (categorized as Age), \"acute lymphoblastic leukemia\" (Dis\n",
        "# %%writefile app.py\n",
        "# import streamlit as st\n",
        "# from transformers import pipeline\n",
        "# # Load the same NER pipeline inside the app (this will use the model we downloaded)\n",
        "# ner_pipeline = pipeline(\"ner\", model=\"d4data/biomedical-ner-all\", aggregation_strategy=\"simple\")\n",
        "# # Streamlit UI layout\n",
        "# st.title(\"Clinical NER Demo\")\n",
        "# st.markdown(\"Enter a synthetic clinical note and **extract entities** using a pretrained Transformer model:\")\n",
        "# # A text area for input\n",
        "# default_text = (\"Patient is a 7-year-old boy with acute lymphoblastic leukemia (ALL) who presents with \"\n",
        "#                 \"a two-week history of fever, bone pain, and fatigue. On exam, noted pallor and bruising. \"\n",
        "#                 \"Plan is to start induction chemotherapy with vincristine, prednisone, and L-asparaginase.\")\n",
        "# user_input = st.text_area(\"Clinical Note\", default_text, height=150)\n",
        "# # Button to run NER\n",
        "# if st.button(\"Extract Entities\"):\n",
        "#     # Run the NER pipeline on the input text\n",
        "#     entities = ner_pipeline(user_input)\n",
        "#     # Highlight the entities in the text by wrapping them with HTML <mark> tag\n",
        "#     highlighted_text = user_input\n",
        "#     # Insert the highlight tags in reverse order of indices (to not mess up positions as we insert)\n",
        "#     for ent in sorted(entities, key=lambda x: x['start'], reverse=True):\n",
        "#         start, end = ent['start'], ent['end']\n",
        "#         highlighted_text = (highlighted_text[:start]\n",
        "#                              + f\"<mark>{highlighted_text[start:end]}</mark>\"\n",
        "#                              + highlighted_text[end:])\n",
        "#     # Display the highlighted text. 'unsafe_allow_html=True' lets us render the <mark> tags.\n",
        "#     st.write(\"**Extracted Entities Highlighted:**\")\n",
        "#     st.markdown(highlighted_text, unsafe_allow_html=True)\n",
        "\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the same NER pipeline inside the app\n",
        "ner_pipeline = pipeline(\"ner\", model=\"d4data/biomedical-ner-all\", aggregation_strategy=\"simple\")\n",
        "\n",
        "# Streamlit UI layout\n",
        "st.title(\"Clinical NER Demo with Color Coding\")\n",
        "st.markdown(\"Enter a synthetic clinical note and **extract entities** using a pretrained Transformer model:\")\n",
        "\n",
        "# A text area for input\n",
        "default_text = (\"Patient is a 7-year-old boy with acute lymphoblastic leukemia (ALL) who presents with \"\n",
        "                \"a two-week history of fever, bone pain, and fatigue. On exam, noted pallor and bruising. \"\n",
        "                \"Plan is to start induction chemotherapy with vincristine, prednisone, and L-asparaginase.\")\n",
        "user_input = st.text_area(\"Clinical Note\", default_text, height=150)\n",
        "\n",
        "# Button to run NER\n",
        "if st.button(\"Extract Entities\"):\n",
        "    # Run the NER pipeline on the input text\n",
        "    entities = ner_pipeline(user_input)\n",
        "\n",
        "    # Color mapping for entity categories\n",
        "    color_map = {\n",
        "        \"Age\": \"lightblue\",\n",
        "        \"Disease\": \"lightcoral\",\n",
        "        \"Symptom\": \"lightgreen\",\n",
        "        \"Treatment\": \"lightsalmon\",\n",
        "        \"Medication\": \"lightgoldenrodyellow\",\n",
        "        \"Finding\": \"lightcyan\", # Example for \"pallor\" and \"bruising\"\n",
        "        # Add more categories and colors as needed\n",
        "    }\n",
        "\n",
        "    highlighted_text = user_input\n",
        "    for ent in sorted(entities, key=lambda x: x['start'], reverse=True):\n",
        "        start, end = ent['start'], ent['end']\n",
        "        entity_type = ent['entity_group']\n",
        "        color = color_map.get(entity_type, \"yellow\")  # Default to yellow if category not in map\n",
        "        highlighted_text = (highlighted_text[:start] +\n",
        "                             f\"<mark style='background-color:{color}'>{highlighted_text[start:end]}</mark>\" +\n",
        "                             highlighted_text[end:])\n",
        "\n",
        "    st.write(\"**Extracted Entities Highlighted:**\")\n",
        "    st.markdown(highlighted_text, unsafe_allow_html=True)\n",
        "\n",
        "    # Display color coding legend\n",
        "    st.write(\"**Color Coding Legend:**\")\n",
        "    for entity_type, color in color_map.items():\n",
        "        st.markdown(f\"<span style='background-color:{color}; padding: 0.3em 0.5em; border-radius: 0.2em;'>{entity_type}</span>\", unsafe_allow_html=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQOBDaUbiS5P",
        "outputId": "f7d804e8-7d1a-4600-d51b-498c46f9af6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://loca.lt/mytunnelpassword"
      ],
      "metadata": {
        "id": "jX-jJChQvLTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "J-4sLS6YiTCI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}