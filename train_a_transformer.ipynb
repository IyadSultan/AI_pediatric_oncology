{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOy0JvoCvMqe0FVjyxQMx10",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "65d74f6012254e908a7415852bd547d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c94acb7807cd4f84ab7b8055fcb990c8",
              "IPY_MODEL_bb336c931c4a4230b3e528b3a5510062",
              "IPY_MODEL_ecaa68db66b543d5b32e29d859c8c6f1"
            ],
            "layout": "IPY_MODEL_b2dcfa4e50df425da7c71ad85f5e6d46"
          }
        },
        "c94acb7807cd4f84ab7b8055fcb990c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f43bc68458aa4ac9810891520102acdd",
            "placeholder": "​",
            "style": "IPY_MODEL_9668a70f96404ceebe0681ac8a089aaf",
            "value": "Map: 100%"
          }
        },
        "bb336c931c4a4230b3e528b3a5510062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76bdf6e6ecc140cfadb952a9479a7dac",
            "max": 924,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a86c772f2d0346e6bf23a638bade15ac",
            "value": 924
          }
        },
        "ecaa68db66b543d5b32e29d859c8c6f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0ab4cf915134fa3b96cb2fa5460b752",
            "placeholder": "​",
            "style": "IPY_MODEL_4e0b1c290a274297957e74eaed5ca1f4",
            "value": " 924/924 [00:00&lt;00:00, 3990.98 examples/s]"
          }
        },
        "b2dcfa4e50df425da7c71ad85f5e6d46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f43bc68458aa4ac9810891520102acdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9668a70f96404ceebe0681ac8a089aaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76bdf6e6ecc140cfadb952a9479a7dac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a86c772f2d0346e6bf23a638bade15ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0ab4cf915134fa3b96cb2fa5460b752": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e0b1c290a274297957e74eaed5ca1f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IyadSultan/educational/blob/main/train_a_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a Transformer for Medical Problem NER in Clinical Text\n",
        "\n",
        "Named Entity Recognition (NER) in healthcare can automatically highlight medical problems (like diseases or symptoms) in clinical notes, helping clinicians quickly identify key conditions. In this hands-on tutorial, we will fine-tune a Transformer model to recognize medical problems in text using Hugging Face 🤗 Transformers. We’ll use a domain-specific BERT model (BioClinicalBERT) and an open medical NER dataset, and walk through the entire process from data loading to evaluation. What you will learn:\n",
        "1. Setting up the Colab environment and installing necessary libraries.\n",
        "2. Loading an open-access medical NER dataset from the Hugging Face Hub.\n",
        "3. Preprocessing clinical text data and aligning entity labels with subword tokens.\n",
        "4. Choosing a suitable pre-trained Transformer model (e.g. BioClinicalBERT) for clinical text​.\n",
        "5. Fine-tuning the model for NER using the Hugging Face Trainer API.\n",
        "6. Evaluating model performance (precision, recall, F1-score) before and after fine-tuning.\n",
        "7. Testing the model on example clinical text to see qualitative improvements in NER.\n",
        "\n",
        "Prerequisites: Basic Python coding and familiarity with general concepts of machine learning. We will explain NLP and Transformer concepts in simple terms, so healthcare professionals with beginner coding experience should be able to follow along."
      ],
      "metadata": {
        "id": "zN23ix2lCjYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Environment Setup\n",
        "First, we need to set up our environment. This tutorial is designed for Google Colab – make sure you've selected a GPU runtime for faster training (go to Runtime > Change runtime type > Hardware accelerator > GPU). We’ll install Hugging Face’s Transformers, Datasets, and evaluation libraries."
      ],
      "metadata": {
        "id": "2W-fA_LhCjKh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUTIyna1Ca77",
        "outputId": "ead618fb-7d59-4df0-b808-8980e77e3106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.6.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets evaluate seqeval"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation* : The transformers library provides the model and training tools, datasets will let us easily load the dataset, and seqeval (via evaluate) is a library for computing NER metrics like precision and F1. After running this, you should see the libraries installing."
      ],
      "metadata": {
        "id": "AwL6x3-_CkLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let’s also verify that a GPU is available:\n",
        "import torch\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEkY9pHCCkSR",
        "outputId": "868671a3-3e12-498f-e639-14c83ba36a18"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the output says GPU available: True, we’re ready to go (Colab Pro’s GPU will help speed up training). If it’s False, double-check the runtime settings."
      ],
      "metadata": {
        "id": "eWNpboIVCkXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Loading the Dataset\n",
        "We will use an open-access dataset from the Hugging Face Hub. One good option is the NCBI Disease corpus, which contains biomedical text (PubMed abstracts) annotated with disease names​. This **English-language dataset** is open-access and suitable for our task of extracting medical problems. Let's load the dataset using 🤗 Datasets:"
      ],
      "metadata": {
        "id": "qW0b9XexEEhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the NCBI Disease dataset (with train/val/test splits)\n",
        "raw_datasets = load_dataset(\"ncbi_disease\")\n",
        "print(raw_datasets)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIhbGXY8CkcE",
        "outputId": "b045ebdf-1085-4bcb-91dc-68200d69b9f6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'tokens', 'ner_tags'],\n",
            "        num_rows: 5433\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'tokens', 'ner_tags'],\n",
            "        num_rows: 924\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'tokens', 'ner_tags'],\n",
            "        num_rows: 941\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This should download the dataset and show the splits (train/validation/test). The NCBI Disease corpus has 793 abstracts with disease annotations​, already split into about 5,433 training sentences, 924 validation sentences, and 941 test sentences. Each data sample in this dataset is a sentence with two fields:\n",
        "\"tokens\": the list of word tokens in the sentence.\n",
        "\"ner_tags\": the list of numeric labels for each token (0 = outside any entity, 1 = beginning of a disease entity, 2 = inside a disease entity)​.\n",
        "\n",
        "Let’s inspect an example from the training set:"
      ],
      "metadata": {
        "id": "Qj8s_jSeCkjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Peek at one training example\n",
        "example = raw_datasets[\"train\"][0]\n",
        "print(example[\"tokens\"])\n",
        "print(example[\"ner_tags\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUxYCnRbCkpV",
        "outputId": "5bc39030-2a62-4728-b85f-5d63a0d83314"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Identification', 'of', 'APC2', ',', 'a', 'homologue', 'of', 'the', 'adenomatous', 'polyposis', 'coli', 'tumour', 'suppressor', '.']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the tokens ['adenomatous', 'polyposis', 'coli', 'tumour'] have tags [1, 2, 2, 2], meaning \"adenomatous\" is tagged as B-disease (beginning of a disease name) and the next three tokens are I-disease (continuation of the disease name), while all other tokens are 0 (not an entity). The disease mentioned here is “adenomatous polyposis coli tumour.”"
      ],
      "metadata": {
        "id": "eI_XVzZJCkvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding the task**: Our goal is to train a model that takes a sequence of tokens (a clinical sentence) and outputs the correct tag (O, B-problem, I-problem) for each token, thereby identifying spans of text that are medical problems (diseases, in this dataset)."
      ],
      "metadata": {
        "id": "HjaQgmyyE50_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data Preprocessing and Tokenization\n",
        "Before training, we need to preprocess the data for our Transformer model. Transformers like BERT cannot take raw text strings directly for NER; we must:\n",
        "Tokenize the text with the model’s tokenizer, and Align the labels to the tokenizer’s output tokens.\n",
        "\n",
        "*Why align labels?*\n",
        "\n",
        "Models like BERT use subword tokenization. A single word (e.g., \"diabetes\") might be broken into multiple subtokens (e.g., \"dia\", \"##betes\"). Our dataset labels are at the word level, so we need to create a label for each subtoken as well. Typically, we give the same label to all subtokens of a word, or label only the first subtoken and mark the rest as “ignore” in the loss calculation. We will use the Hugging Face AutoTokenizer to handle tokenization. Let’s choose our model’s tokenizer (we will use BioClinicalBERT in the next section, which has a fast tokenizer available):"
      ],
      "metadata": {
        "id": "cfZzAmHmE-dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
      ],
      "metadata": {
        "id": "DKDOUmHGCk03"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define a function to tokenize the inputs and align the NER tags. We'll use is_split_into_words=True because our dataset already provides a list of tokens for each example (so we don't want the tokenizer to re-split the sentence into words, but rather tokenize each given token).\n",
        "\n",
        "**For each tokenized example:**\n",
        "- We get a mapping of token indices to original word indices using tokenized_inputs.word_ids(batch_index).\n",
        "= We assign each subtoken the label of its originating word. We will mark only the first subtoken of each word with the original label and use -100 for the remaining subtokens. The value -100 is a special label that tells the loss function to ignore those positions (so we don't double-count a multi-subtoken word)."
      ],
      "metadata": {
        "id": "3UQHH7GxCk8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], is_split_into_words=True, truncation=True)\n",
        "    all_labels = examples[\"ner_tags\"]\n",
        "    aligned_labels = []\n",
        "    for i, labels in enumerate(all_labels):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their word index in the original example\n",
        "        label_ids = []\n",
        "        previous_word_idx = None\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                # Special token like [CLS] or [SEP]\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                # Start of a new word, use that word's label\n",
        "                label_ids.append(labels[word_idx])\n",
        "            else:\n",
        "                # Continuation of the same word, mark for ignoring\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        aligned_labels.append(label_ids)\n",
        "    # Include the aligned labels in the tokenized input\n",
        "    tokenized_inputs[\"labels\"] = aligned_labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Apply the tokenization and alignment to the entire dataset\n",
        "tokenized_datasets = raw_datasets.map(tokenize_and_align_labels, batched=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "65d74f6012254e908a7415852bd547d7",
            "c94acb7807cd4f84ab7b8055fcb990c8",
            "bb336c931c4a4230b3e528b3a5510062",
            "ecaa68db66b543d5b32e29d859c8c6f1",
            "b2dcfa4e50df425da7c71ad85f5e6d46",
            "f43bc68458aa4ac9810891520102acdd",
            "9668a70f96404ceebe0681ac8a089aaf",
            "76bdf6e6ecc140cfadb952a9479a7dac",
            "a86c772f2d0346e6bf23a638bade15ac",
            "f0ab4cf915134fa3b96cb2fa5460b752",
            "4e0b1c290a274297957e74eaed5ca1f4"
          ]
        },
        "id": "d88VzGphClCL",
        "outputId": "b1602c3e-d8ae-4ea9-d9d8-bd5822ae3791"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/924 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65d74f6012254e908a7415852bd547d7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation:* We iterate through each example’s word indices from the tokenizer. When word_idx is None, it corresponds to special tokens (like [CLS] start or [SEP] end tokens) – we assign -100 so that they are ignored in training. When we see a new word (word index not equal to the previous one), we take that word’s original label (labels[word_idx]). If the tokenizer output is still on the same word as the previous token (meaning the word was split into multiple subtokens), we also assign -100 to those subtokens. This way, only the first subtoken of a word carries the label, and the loss will be calculated only once per original word.\n",
        "\n",
        "**Note: An alternative would be to label subsequent subtokens as \"I-\" (inside) tags of the same entity. In our simple approach, we ignore them for loss calculation to avoid overweighting long words. This is a common technique to handle subword tokenization in NER​**.\n",
        "\n",
        "Now that we have tokenized_datasets, each entry has not only \"input_ids\" and \"attention_mask\" (from tokenization) but also an aligned \"labels\" sequence for training. We can verify the tokenization on our earlier example to see how the labels align:"
      ],
      "metadata": {
        "id": "VKdO04cdClH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check tokenization and label alignment on the example\n",
        "tokens = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "print(\"Original tokens:\", example[\"tokens\"])\n",
        "print(\"Original labels:\", example[\"ner_tags\"])\n",
        "print(\"Subword tokens:\", tokens.tokens())\n",
        "print(\"Aligned labels:\", tokenize_and_align_labels({\"tokens\": [example[\"tokens\"]], \"ner_tags\": [example[\"ner_tags\"]]})[\"labels\"][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GrMovTtClSF",
        "outputId": "c013f2cf-f5ee-42c9-a8ce-5ba8ccbe7e98"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tokens: ['Identification', 'of', 'APC2', ',', 'a', 'homologue', 'of', 'the', 'adenomatous', 'polyposis', 'coli', 'tumour', 'suppressor', '.']\n",
            "Original labels: [0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0]\n",
            "Subword tokens: ['[CLS]', 'identification', 'of', 'a', '##p', '##c', '##2', ',', 'a', 'ho', '##mo', '##logue', 'of', 'the', 'ad', '##eno', '##mat', '##ous', 'p', '##oly', '##po', '##sis', 'co', '##li', 't', '##umour', 'suppress', '##or', '.', '[SEP]']\n",
            "Aligned labels: [-100, 0, 0, 0, -100, -100, -100, 0, 0, 0, -100, -100, 0, 0, 1, -100, -100, -100, 2, -100, -100, -100, 2, -100, 2, -100, 0, -100, 0, -100]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will show the original tokens and their labels, the subword tokens (including special [CLS] and [SEP]), and the aligned labels for each subword token. You should see -100 for the special tokens and second part of any split word. For instance, if \"polyposis\" was split into [\"poly\", \"##posis\"], the first subtoken might get label 2 (I-disease) and the second subtoken -100."
      ],
      "metadata": {
        "id": "gbwGa2jTClYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Choosing a Pretrained Model (BioClinicalBERT)\n",
        "Selecting a good pretrained model as a starting point is crucial. We need a model that understands clinical language. BioClinicalBERT is an excellent choice: it’s a BERT-based model that was pretrained on biomedical literature and clinical notes (MIMIC-III ICU records), giving it a strong grasp of medical terminology and context. This domain-specific pretraining should help it recognize disease names and other medical entities better than a generic BERT.\n",
        "\n",
        "Other options could be BioBERT (pretrained on biomedical articles) or even a smaller model like DistilBERT fine-tuned on a medical NER dataset. However, BioClinicalBERT is well-suited for clinical text, and since we have a GPU, its size (base BERT, ~110M parameters) is manageable.\n",
        "\n",
        "Let's load the model with a classification head for token classification (NER). We will use AutoModelForTokenClassification which adds a token-level classification layer on top of the transformer. We need to specify the number of labels our model will predict:\n",
        "\n",
        "- Label 0: \"O\" (no entity)\n",
        "- Label 1: \"B-Disease\" (beginning of a disease mention)\n",
        "- Label 2: \"I-Disease\" (inside a disease mention)\n",
        "\n",
        "We'll also pass **id2label and label2id** mappings for better clarity (so the model knows which label index corresponds to \"B-Disease\", etc., which will be useful for inference)."
      ],
      "metadata": {
        "id": "_0zPu9isGBad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "num_labels = 3  # O, B-Disease, I-Disease\n",
        "id2label = {0: \"O\", 1: \"B-Disease\", 2: \"I-Disease\"}\n",
        "label2id = {\"O\": 0, \"B-Disease\": 1, \"I-Disease\": 2}\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n"
      ],
      "metadata": {
        "id": "JZHmzPMLClep"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When this runs, it will download the BioClinicalBERT weights. You might see a warning that some weights are not used (those are for the original masked language modeling head) and some new weights are initialized (the classification head). That’s expected because we are repurposing the base model for NER.\n",
        "\n",
        "**Note: BioClinicalBERT was initialized from BioBERT and trained on. It’s a domain-specific model ideal for clinical text. If you were dealing with general text, you might use a generic BERT base model, but for our medical task, leveraging domain knowledge should give us a boost.**"
      ],
      "metadata": {
        "id": "Htvc9vceCljk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Fine-Tuning the Model with Trainer API\n",
        "Now we have our prepared dataset (tokenized_datasets) and our model. The next step is to fine-tune the model on the training data. We'll use Hugging Face’s Trainer API, which simplifies the training loop and handles things like gradient accumulation, evaluation, and more. Setup Training Arguments: We need to specify how we train, e.g., number of epochs, batch size, learning rate, etc. For NER with a base BERT model:\n",
        "- A few epochs (2-4) are usually sufficient for a dataset of this size.\n",
        "- A learning rate around 2e-5 to 5e-5 works well for fine-tuning BERT.\n",
        "- We’ll use the validation set to evaluate after each epoch.\n",
        "\n",
        "We also define a compute_metrics function to compute precision, recall, and F1 using the seqeval metric. We will focus on the “Disease” entity class performance and overall metrics."
      ],
      "metadata": {
        "id": "wFqxnBKkGvxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "# Load the seqeval metric\n",
        "seqeval = evaluate.load(\"seqeval\")\n",
        "\n",
        "# Define compute_metrics to use during evaluation\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    true_labels = []\n",
        "    true_predictions = []\n",
        "    for pred, lab in zip(predictions, labels):\n",
        "        # Remove ignored index (subword pieces)\n",
        "        lab = [l for l in lab if l != -100]\n",
        "        pred = pred[:len(lab)]  # truncate prediction to same length as lab\n",
        "        true_labels.append([id2label[l] for l in lab])\n",
        "        true_predictions.append([id2label[p] for p in pred])\n",
        "    # Compute metrics using seqeval\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "    # We will return overall precision, recall, F1, and accuracy\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n"
      ],
      "metadata": {
        "id": "jkzGYP9oClsd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*A quick explanation:* We take the logits (model outputs before softmax) and labels from the evaluation batch. We argmax the logits to get the predicted label indices. We then filter out any -100 in the labels (those correspond to subword pieces we chose to ignore), and make sure to truncate the predictions to the same length (since we don't want to count predictions for subword tokens that were ignored in labels). We map the numeric labels back to their string names (using id2label) and feed the lists to seqeval.compute(). Seqeval will calculate precision, recall, F1, etc., treating the labels as sequences of entity tags. We return the overall metrics. (Seqeval also provides class-specific metrics, but “overall” here mostly reflects the single entity type we care about, Disease.)"
      ],
      "metadata": {
        "id": "chhBg2OBCl19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we set up the TrainingArguments and the Trainer:\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "\n",
        "batch_size = 16\n",
        "logging_steps = len(tokenized_datasets[\"train\"]) // batch_size  # log once per epoch (approx)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"med_ner_model\",\n",
        "    save_strategy=\"no\",\n",
        "    # Remove evaluation_strategy and use eval_steps instead\n",
        "    eval_steps=logging_steps,  # Evaluate once per epoch\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=logging_steps,\n",
        "    log_level=\"error\",\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAF6kztkCmBs",
        "outputId": "a974ba1b-e879-470b-cc2a-69e9b3b49ed2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-539bf30b8263>:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key points in these settings:\n",
        "We set evaluation_strategy=\"epoch\" to evaluate on the validation set at the end of each epoch.\n",
        "We chose 3 epochs. You can adjust this; often 2-4 is enough for convergence on a small dataset, and more can cause overfitting.\n",
        "per_device_train_batch_size=16 is reasonable for a base model on a GPU with enough memory (Colab Pro GPUs can handle this). If you get OOM errors, you might reduce the batch size.\n",
        "We disable saving checkpoints (save_strategy=\"no\") and pushing to hub for simplicity, but in a real scenario, you might want to save the fine-tuned model.\n",
        "Before training, let's optionally evaluate the untrained model on the validation set to get a baseline. (Since the classification head is random initially, we expect it to perform very poorly – likely predicting no entities correctly.)"
      ],
      "metadata": {
        "id": "RpD-3e55HsXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model *before* training (baseline performance)\n",
        "baseline_metrics = trainer.evaluate()\n",
        "print(\"Baseline performance:\", baseline_metrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "BkxnSi49HtzF",
        "outputId": "98ad3591-14c4-47a7-ee5d-b25cc6d87d1b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    776\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m                     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mas_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    738\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: expected sequence of length 17 at dim 1 (got 25)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-4a3c619f7aa8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate the model *before* training (baseline performance)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbaseline_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Baseline performance:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4153\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4154\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   4155\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4156\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4337\u001b[0m         \u001b[0;31m# Main evaluation loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4338\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4339\u001b[0m             \u001b[0;31m# Update the observed num examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4340\u001b[0m             \u001b[0mobserved_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# We iterate one batch ahead to check when we are at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         batch = pad_without_fast_tokenizer_warning(\n\u001b[0m\u001b[1;32m    273\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpad_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpad_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Restore the state of the warning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3405\u001b[0m                 \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3407\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3409\u001b[0m     def create_token_type_ids_from_sequences(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    791\u001b[0m                         \u001b[0;34m\"Please see if a fast version of this tokenizer is available to have this feature available.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m                     ) from e\n\u001b[0;32m--> 793\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    794\u001b[0m                     \u001b[0;34m\"Unable to create tensor, you should probably activate truncation and/or padding with\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m                     \u001b[0;34m\" 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "r7pN2obfCmI0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2_ZcOSgXCmPt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}